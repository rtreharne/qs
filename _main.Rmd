---
title: "Quantitative Skills in Biosciences I"
author: "R. E. Treharne"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    css: styles.css
    config:
      search: yes # Enable search bar
      sharing: null
  #bookdown::pdf_book: default
---


```{r setup, echo=FALSE, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

```

# Cover{.unnumbered .ignore}


```{r, echo=FALSE}
knitr::include_graphics("img/front/coverart.png")
```

# Introduction {.unnumbered}

This book accompanies the Quantitative Skills (QS) component of the **BIOS103 - Introductory Practical Skills in Biosciences I** course.

If you are participating in the weekly timetabled QS workshops associated with this course then be sure to submit your weekly summative QS assignments via Canvas by the deadline specified for each workshop.

All QS workshops will be delivered online via Teams. Access the meeting link from the BIOS103 Canvas course.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/2Igdytf8zW4?si=jg2-CgvI-EK2ejWJ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

<!--chapter:end:index.Rmd-->

---
output:
  pdf_document: default
  html_document: default
  

  
---
\newpage

# Introduction to Excel and R

If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover **all of this section's content** and complete this week's [**formative and summative assessments**](#complete-your-weekly-assignments-01) in the BIOS103 Canvas Course.

## Estimating the Volume of a Snail

In this section, you will learn how to import data from a CSV file into Excel, perform basic calculations, create a scatterplot including a linear trendline to make predictions. The primary objective is to estimate the volume of a snail based on its mass using a provided dataset.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/0ShVZvl1sG0?si=q1Ve8aptT5m-1H21" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
>**Google Sheets Alternative**
>
>If you like, you can do everything in the above video using Google Sheets instead! Here's my [alternate video](https://youtu.be/D-URR3a7mmQ?si=mgHlelXssWuSO0SU) just for Google Sheets. 

### Download and Import the CSV File

1. **Download the CSV File:** 
   - Here is an [example dataset](https://canvaswizards.org.uk/dataspell/snails/301). Download it to your local machine.
   - If that link doesn't you can get the dataset in your browser [here](https://raw.githubusercontent.com/rtreharne/qs/refs/heads/main/data/01/snails_301.csv). Right click and Save as.

2. **Import into Excel:**
   - Open Excel (Use the desktop version - you won't be able to do this using the online version!).
   - Go to **Data > From Text/CSV** and select the downloaded CSV file.
   - When the import wizard appears, click **Load**.

### Calculate Volume \( V \) in Excel

We will estimate the volume \( V \) of our snails using the formula for the volume of a sphere:
\[ V = \frac{4}{3} \pi r^3 \]

where \( r \) is the radius, which we assume is equal to half the `Height L (mm)` column.

1. **Add a New Column for Volume \( V \):**
   - In the third column, label it as **Volume V (mm^3)**.
   - In the first cell of this column, use the formula:
     ```excel
     = (4/3) * PI() * ((B2/2)^3)
     ```
   - Drag the formula down to apply it to all rows.

### Add a Linear Trendline and Equation

1. **Create a Scatter Plot:**
   - Select the **Mass M (g)** and **Volume V (mm^3)** columns.
   - Go to the **Insert** tab and select **Scatter Plot**.

2. **Add Trendline:**
   - Click on the plot
   - Click the green **+** icon that appears at the top right of the plot.
   - Click the **>** symbol on the **Trendline** option and click **More options...**
   - From the trendline options menu that appears on the right, select the **Linear** trendline and check the box **isplay Equation on chart**.
   - Right-click on a data point in the scatter plot.

3. **Interpret the Equation:**
   - The trendline equation will appear on the chart in the form of \( y = ax + b \), where:
     - \( y \) is the volume.
     - \( x \) is the mass.
     - \( a \) and \( b \) are coefficients.
   - For the Example dataset:
     - a = 1341.7 \( mm^3.g^{-1} \)
     - b = 1140.2 \( mm^3 \)

### Estimation of Volume for a Snail with Mass 10g


1. **Use the Trendline Equation:**
   - Substitute \( x = 10 \) into the trendline equation to calculate the estimated volume \( V \).
   - Express the volume in \( cm^3 \) to one decimal place (Note: \( 1 cm^3 = 1000 mm^3 \)).
2. **For the example dataset:**
   - The estimated volume of a snail that is \( 10 g \) is **\( 14.6 cm^3 \)**.
   
## Getting Ready for R {#getting-ready-for-R}

Over the next couple of weeks you will continue to use Excel to load, manipulate, analyse and visualise data. Beyond this you will be using the coding language R exclusively. To prepare for this, you need to download, install and configure R and RStudio today. 

>**Chromebook Users**
>
>I love a chromebook. Sadly, installing R and RStudio on one involves a bit of extra work compared to Windows and Mac.
>This YouTube video seems to have all the bases covered: ["How to Install RStudio on a Chromebook"](https://youtu.be/km4rQu6SoC0?si=YOflmxVUleCNI8kJ)

### Download and Install (Windows and Mac only)

**R** and **RStudio** are actually separate things, although they are often mentioned together. 

**R** is a programming language and software environment specifically designed for statistical computing and data visualisation. Other examples of programming languages include Python, Java and Ruby. 

**RStudio** is the integrated development environment (IDE) for R. It provides a user-friendy interface that will allow you to write all your R scripts and compile them to do stuff. I'm using it to write this handbook right now!

Despite their differences, you might hear the terms **R** and **RStudio** used interchangeably, as RStudio serves as the primary interface through which users interact with the R programming language.

**You need to download and install both R and Rstudio.**

   - Install R first from the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org/)
   - Then install RStudio from the [RStudio website](https://posit.co/download/rstudio-desktop/)
   
Once both are installed, [open up RStudio](https://youtu.be/tQ227sB8YeA) and get ready to create your first **R Project**.

**All University of Liverpool MWS machines already have R and RStudio installed and ready to use.**




### Creating Your First R Project in RStudio

Follow these steps to set up and manage your first R project in RStudio:

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/mzPEVgnz3GY?si=a1cjJaMrhzNgMoiZ1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

1. **Open RStudio**

- Launch RStudio from your applications menu.

2. **Start a New Project**

- Click on the **File** menu at the top of the RStudio window.
- Select **New Project...** from the dropdown menu.

3. **Choose Project Type**

You will be prompted with three options:

- **New Directory**: Create a new project in a new directory.
- **Existing Directory**: Use an existing directory as the project's folder.
- **Version Control**: Clone a project from a version control repository (e.g., GitHub).

For your first project, select **New Directory**.

4. **Select Project Template**

- Choose **Empty Project**. 
- Click **Next**.

5. **Set Up Project Directory**

- **Directory name**: Enter a name for your project folder. This will be the name of the directory created for your project.
- **Subdirectory of**: Choose the parent directory where the new project folder will be created. You can navigate to the desired location using the file browser.
- Click **Create Project**.

6. **RStudio Project Interface**

Once the project is created, you will see a new RStudio window or tab with the following components:

- **Files pane**: Displays the files and folders in your project directory.
- **Script editor**: Where you write and edit your R scripts.
- **Console**: Where you can directly enter and execute R commands.
- **Environment/History**: Shows your workspace objects and command history.
- **Plots/Packages/Help/Viewer**: Various tabs for viewing plots, managing packages, accessing help documentation, and viewing other outputs.

7. **Create and Save an R Script**

- Click **File > New File > R Script**.
- Write some R code in the script editor. For example:

To run your print command, click on the line and click the `Run` button at the top right of your script editor window or press **Ctrl + ENTER** (Cmd + Enter on Mac).

You will see the following output in your console window:
```{r echo=FALSE}
# my first line of code. this is a comment!
print("Hello World!")

```
And there it is! You've just successfully compiled your first line of R. Congratulations!

### Something More Complicated

As you delve deeper into R programming, you'll find that your scripts become more sophisticated than the "Hello World" example above. In the following example, I'll walk you through a script to create a random number generator. Here's the script in its entirety:

```{r eval=FALSE}
# create a variable called "seed"
seed <- 999

# set the seed
set.seed(seed)

# generate a random number
random_number <- runif(1)

# print the random number
print(random_number)
```
Cut and paste these lines into the script file we were working on earlier (overwrite the "Hello World" example).

Now, you can run each line in turn as before (using **Ctrl + ENTER**) or you can run everything by clicking the **Source** button. You should see the following number appear in your console:

Let's break down each line of the script to understand its purpose and functionality.

1. **Creating a Variable Called "seed"**
```{r eval=FALSE}
# create a variable called "seed"
seed <- 123
```

   - `seed <- 123`: Here, we are using the `<-` operator to assign the value 123 to the variable named seed. In R, variables are used to store data that can be reused or manipulated later in the script. The number 123 is arbitrary in this case, but we use it to illustrate how to set a seed for random number generation.
   
2. **Setting the Seed**
```{r eval=FALSE}
# set the seed
set.seed(seed)
```
   - The `set.seed()` function initializes the R environment's build in random number generator with the value stored in seed. Setting a seed is essential for reproducibility, meaning that if someone else runs this code with the same seed, they will get the same random number output in their console. This is particularly useful in simulations and randomised experiments where consistent results are needed.
   
3. **Generating a Random Number**
```{r eval=FALSE}
# generate a random number
random_number <- runif(1)
```

   - `random_number <- runif(1)`: This line generates a single random number between 0 and 1. The function runif() generates random numbers from a uniform distribution, which means that each number within the specified range has an equal probability of being selected. The 1 inside the parentheses specifies that only one random number should be generated. The resulting number is then assigned to the variable called `random_number`.

4. **Printing the Random Number**
```{r eval=FALSE}
# print the random number
print(random_number)
```

   - `print(random_number)`: The print() function outputs the value stored in `random_number` to the console. This is useful for verifying the output of your code and ensuring that the operations have been executed correctly.

**Summary**

This example script demonstrates a more complex task than the basic "Hello World" script. It introduces key concepts like variable assignment with <-, setting a seed for reproducibility using set.seed(), generating random numbers with runif(), and printing results using print(). As you continue learning R, these foundational concepts will become increasingly important, enabling you to build more advanced and meaningful analyses. Remember, comments (#) are your friends! They help explain what each part of your code does, making it easier for you and others to understand and maintain your scripts. Be liberal with your comments. You'll thank yourself later (trust me).

**Give me feedback**

I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It's completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe src="https://canvaswizards.org.uk/likertysplit/qs/" width="400" style="max-width: 100%" height="600"></iframe>')
}
```

## Complete your Weekly Assignments {.complete-your-weekly-assignments-01}

In the BIOS103 Canvas course you will find this week's **formative** and **summative** QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section's content. The assignments are identical in all but the following details:

   + You can attempt the **formative assignment** as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you're confident that you can get the correct answer on your own.
   + You can attempt the **summative** assignment **only once**. It will be identical to the formative assignment but will use different values and datasets. This assignment **will** contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days.
   
In **ALL** cases, when you click the button to "begin" a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit.






<!--chapter:end:01.Rmd-->

---
output:
  html_document: default
  pdf_document: default

  bookdown::gitbook:
    toc: true
    number_sections: true
---
\newpage

# Summarising Data and ANOVA in Excel

If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover **all of this section's content** and complete this week's [**formative and summative assessments**](#complete-your-weekly-assignments-02) in the BIOS103 Canvas Course.

In this section we will only be using Excel. No R today! Specifically, you will be developing two important skills:

1. **Summarising Data.**
2. **Constructing and testing hypotheses.**

The ultimate aim is to gain insight and learn something new about the world from the data that we have painstaking measured in our well designed lab experiments. This is a cornerstone of what being a scientist is all about.

## Summarising Data

Raw data is beautiful, but messy. Showing another person your raw data and expecting them to immediately understand it, no matter how proud you are of the toil expended to generate the data, is an unrealistic expectation. You need to boil your data down into something that another person can grasp instantaneously. 

Let's take a look at a Zebrafish dataset from an experiment that is uncannily similar to the one you performed in your lab practical this week.

### Download and Import the CSV File

1. **Download the CSV File:** 
   - Here is an [example dataset](https://canvaswizards.org.uk/dataspell/zebrafish/999999999). Download it to your local machine.

2. **Import into Excel:**
   - Open Excel (Use the desktop version - you won't be able to do this using the online version!).
   - Go to **Data > From Text/CSV** and select the downloaded CSV file.
   - When the import wizard appears, click **Load**.

You should now see something like that in Figure \@ref(fig:zebrafish-data-snap). There are 3 columns:

   - **ID** - A unique number to identify a measurement.
   - **conc_pc** - The ethanol concentration (%) that the each embryo was treated with.
   - **length_micron** - The measured lengths, in \( \mu m \) of the embryos.
   
 We call this format, in which each row corresponds to a single measurement, a **long** format.

```{r zebrafish-data-snap, echo=FALSE, fig.cap="You should see this (or something like it) after you have imported your date into Excel."}
knitr::include_graphics("img/02/figure_1.png")
```



### Generating a Summary Table

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/wji1rnZbe08?si=U7-r_s3RjNafU71v" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
>**Excel Alternative**
>
>If you are having trouble accessing the Desktop version of Excel then here is an alternative video.
>["Summary Table with Google Sheets"](https://youtu.be/8k7xW01jxJo)


1. **Identify your Groups**
   - Click anywhere in your table.
   - Select the **Data** menu and click the **Advanced** icon in the **Sort & Filter** section. This will bring up a window called "Advanced Filter".
   - Select the **Copy to another location** action.
   - Your list range should already be set to **\$B:\$B**, but if not make it so.
   - Set the **Copy to** cell to **E1**
   - Make sure the **Unique records only** check box is selected and click OK.
   - You should now see a complete list of your alcohol concentration groups in a column with a header **conc_pc**. Make this into a new table by clicking on any of the concentration values, and then **Insert > New table > OK**.
   
Nice. Now you're ready to start building out your summary table horizontally. Let's start with calculating the mean Zebrafish length for each group.

Right now, your spreadsheet should look roughly the same as the screenshot in figure \@ref(fig:zebrafish-snap-2)

```{r zebrafish-snap-2, echo=FALSE, fig.cap="Constructing a summary table", out.width="100%"}
knitr::include_graphics("img/02/figure_2.png")
```

2. **Calculating a Mean Column**
   - Create a new column in your summary table by typing the word **Mean** in cell **F1**.
   - Calculate the mean of the Zebrafish lengths for the control group (0% alcohol concentration) by entering the following formula into cell **F2**.
   ```excel
   =AVERAGE(IF($B1:$B$161=$E2,$C1:$C$161))
   ```
   
> **A Deeper Explanation**  
> 
> The formula `=AVERAGE(IF($B$2:$B$161=$E2,$C$2:$C$161))` is an array formula that calculates the average length of Zebrafish for a specific group based on the concentration of alcohol. It's a bit of a beast isn't it? Let's break it down.
> 
> - **\$B$2:\$B$161**: The **\$** symbols before both the column letter **B** and the row numbers **2** and **161** lock the entire range. This means that when you copy the formula to other cells, this range will not change; it will always refer to cells **B2** to **B161**.
> 
> - **\$E2**: The **\$** before the column letter **E** locks the column, but since there's no **\$** before the row number **2**, the row number can change if the formula is dragged down across rows. This cell is used to compare each value in the range **\$B$2:\$B$161** to the specific concentration value in the corresponding row in column **E**.

> 
> - **\$C$2:\$C$161**: Similar to the range for column **B**, this locks the range of cells in column **C** from which the values will be averaged, conditional on the **IF** statement.
> 
> - **AVERAGE(IF(...))**: The **IF** function checks each row in the range **\$B$2:\$B$161** to see if it matches the value in the corresponding row in column **E**. If it matches, the corresponding value in column **\$C$2:\$C$161** is included in the average calculation. The **AVERAGE** function then calculates the mean of these filtered values.
> 
> This approach is particularly useful when you want to calculate conditional averages across a dataset, ensuring that the correct cells are referenced even when copying the formula to different parts of the spreadsheet.

3. **Calculating More Columns**
   - Create four more columns with headers:
     - Std. Dev.
     - Median
     - Min
     - Max
   - Drag the cell **F2** to **G2**. Change the word **AVERAGE** in the formula in **G2** to **STDEV**. This will calculate the standard deviation for the group and the remaining cells in the column should also auto complete.
    - Do the same for the median, min and max columns. Be sure to use the corresponding function.
   
> **A Deeper Explanation**  
> 
> When analysing data, it's important to understand the basic statistical measures that summarise the data's distribution. Here are some key terms:
> 
> - **Mean**: The mean, often referred to as the average, is the sum of all values in a dataset divided by the number of values. It provides a central value for the data. However, the mean can be influenced by outliers (extremely high or low values).
> 
> - **Standard Deviation**: The standard deviation measures the amount of variation or dispersion in a dataset. It is calculated as the square root of the variance, where variance is the average of the squared differences between each data point and the mean. A low standard deviation indicates that the data points tend to be close to the mean, while a high standard deviation indicates more spread out data.
> 
> - **Median**: The median is the middle value in a dataset when the values are arranged in ascending or descending order. If the dataset has an odd number of values, the median is the central value. If the dataset has an even number of values, the median is the average of the two central values. The median is less affected by outliers compared to the mean.
> 
> - **Min**: The minimum (min) value is the smallest value in the dataset. It provides a measure of the lower bound of the data.
> 
> - **Max**: The maximum (max) value is the largest value in the dataset. It provides a measure of the upper bound of the data.
> 
> These measures are fundamental for understanding the distribution of data. The mean and median give you central tendencies, while the standard deviation tells you how spread out the data is. The minimum and maximum values provide the range within which all the data points fall.

### Presenting Your Summary Table

At some point you may wish to include your Excel summary table in a Word document. There's a lot of wiggle room on how you choose to format your table but there are a few **unbreakable** rules:

+ The table **MUST** have a caption. 
+ The caption should be placed **ABOVE** the table (not below as for a figure or graph).
+ The caption should be numbered accordingly. For example, if this is the first table in your document the figure caption should start "**Figure 1: ...**".
+ The caption should be descriptive and unambiguous. The reader should be able to quickly interpret what is going on without having to read the body of the text. Any symbols or variables or units should be defined in the caption.
+ The data should be formatted to a sensible number of decimal places (i.e. if you're measurements are made to 1 decimal place, your summary values should not be quoted to more than this).
  
Follow these rules and you can't go wrong. Figure \@ref(fig:zebrafish-summary) shows how my summary table looks when copied and pasted into Word. I like to make my tables span the entire width of my document using the **Auto-fit to window** command. I also like to center my columns. These are personal preferences, but you can't deny they look great!



```{r zebrafish-summary, echo=FALSE, fig.cap="Formatting a summary table in Microsoft Word.", out.width="100%"}
knitr::include_graphics("img/02/figure_3.png")
```


   
## Analysis of Variance (ANOVA)

You're about to learn a **critical skill** that is important for becoming a scientist: formulating hypotheses and testing them. This is a cornerstone of scientific inquiry.

You've already summarised your data using descriptive statistics. Now, we'll move on to another branch of statistics called inferential statistics. This involves using an appropriate statistical test to determine whether specific hypotheses that we construct should be accepted or rejected.

Knowing which statistical test to use depends on the data and context. It takes time and lots of practice to become proficient at this, and it's completely normal to forget which test you need or how to interpret the result.

We'll stick with out Zebrafish dataset and perform an Analysis of Variance (ANOVA) to determine whether there is something we can learn from our data. We'll formulate a hypothesis and use Excel to perform the ANOVA. Then we'll interpret the results.

But before we dive into this, let's create a boxplot to visually inspect the data and see if we can generate some gut intuition as to what might be going on.

### Grouped Boxplots in Excel

You might not have seen a grouped boxplot before. That's OK. I'm confident that you'll have a good intuition for what they show. However, for more information the key components of a boxplot read the text in the [Anatomy of a Boxplot](#understanding-boxplots) section. Let's dive straight in for now though.


#### Create the Boxplot {.unnumbered}

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/c9jp3nHwutY?si=qIJMDTbjIb4xcLRf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
>**Excel Alternative**
>
>If you are having trouble accessing the Desktop version of Excel then here is an alternative video.
>["Boxplots with Excel Online"](https://youtu.be/v2-WdO4w5BA)


1. **Insert a Boxplot**: 
   - Select your data.
   - Go to the **Insert** tab, click on **Insert Statistical Chart**, and choose **Box and Whisker**.

2. **Select Data**: 
   - Your plot will look a bit weird. That's because we need to configure the groupings properly.
   - Click the **Select data** button.
   - Remove the **conc_pc** series from the left-hand list.
   - Click the **Edit** button on the (currently empty) right-hand list.
3. **Boxplots in the wrong order?**
   - Sort the **conc_pc** column from smallest to largest.
4. **Re-scale y-axis**:
   - It's best to re-scale the y-axis to maximise the space used by the boxplots. This will make any effect easier to see.
   - Double-click on the numbers in the y-axis. Set the **Bounds > Minimum**: to 1000.
5. **Add Axis Labels**:
    - Click the green **+** icon in the top-right of your graph.
    - Check the **Axes titles** box.
    - Double-click on each label in turn and update with appropriate labels:
      - X-axis: "Alcohol Conc. (%)"
      - Y-axis: "Embryo Length (\(\mu m\))
    - Note: To use the **\(\mu\)** symbol click **Insert > Symbols > Symbol**. Find the symbol in the list and click **Insert**.
6. **Get Rid of Chart Title**:
    - If you're going to be presenting this figure in a report or poster then it should not have a title above the axes.
    - Instead you should include a figure caption **below** the plot.
    - The same **unbreakable** rules for your caption are the same as those described above for table captions. Just make sure your caption is below the figure instead of above.
7. **Export Your Figure**:
    - Right click on your figure anywhere outside the plot area and you should see the option to **Save as picture**.
    - Save it somewhere sensible as a .png file and then insert it into a Word document with a sensible caption.
    
```{r zebrafish-boxplot, echo=FALSE, fig.cap="Distribution of Zebrafish embryo lengths organised by Alcohol treatments.", out.width="100%"}
knitr::include_graphics("img/02/figure_4.png")
```

#### Interpret the Boxplot {.unnumbered}

Figure \@ref(fig:zebrafish-boxplot) shows our finished boxplot. 

- **Alcohol Concentration 0%**: 
  - The median embryo length is approximately 2500 µm, with the mean slightly above the median.
  - The data is relatively spread out, as shown by the wide box and long whiskers.
  - There are no outliers in this group.

- **Alcohol Concentration 1.5%**:
  - The median length is slightly lower than the 0% group, but the mean is still fairly close.
  - The box and whiskers are narrower than in the 0% group, indicating less variability in embryo length.
  - One outlier is present above the whisker, indicating a particularly large embryo in this concentration or possibly, a random measurement error.

- **Alcohol Concentration 2%**:
  - The median and mean have both decreased, showing a reduction in embryo length as alcohol concentration increases.
  - The box is narrower, indicating less variability, but there are several outliers both above and below the whiskers, suggesting that while most embryos were of a similar size, a few were much larger or smaller.

- **Alcohol Concentration 2.5%**:
  - The median and mean lengths have further decreased, indicating a continued negative effect of alcohol concentration on embryo length.
  - The box is of similar size to the 2% group, but with longer whiskers, indicating more variability in the data.
  - There is one outlier below the whisker, indicating a particularly small embryo in this concentration.

In summary, As alcohol concentration increases, the median and mean embryo lengths decrease, indicating a negative correlation between alcohol concentration and embryo length. Variability in embryo length decreases slightly up to 2% concentration but then increases again at 2.5%, as evidenced by the longer whiskers. The presence of outliers, particularly at higher concentrations, suggests that while most embryos are affected similarly, some experience more extreme changes in size.

However, visual patterns alone cannot confirm the existence of a true relationship between alcohol concentration and embryo length. To rigorously explore whether the observed trends are **statistically significant** or merely due to chance, we must construct testable hypotheses. This process will allow us to formalise our observations and set the stage for appropriate statistical analysis.


><h4 id="understanding-boxplots">Anatomy of a Boxplot</h4>
> A boxplot is a standardised way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. The box in the boxplot represents the interquartile range (IQR), which is the range between Q1 and Q3. The central line within the box indicates the median, which is the middle value of the dataset.
> 
> Sometimes, an "X" symbol is also included within the box, representing the mean of the dataset. However, it is important to note that the mean is not always shown in a boxplot.
> 
> The "whiskers" of the boxplot extend from the box to the smallest and largest values within 1.5 times the IQR from the first and third quartiles, respectively. These whiskers help to indicate the spread of the majority of the data.
> 
> Outliers are data points that fall outside the range defined by the whiskers. These are typically plotted as individual points beyond the ends of the whiskers, highlighting data points that are unusually high or low compared to the rest of the dataset.
> 
> To determine whether a data point is an outlier, you compare it to the thresholds defined by the IQR:
> 
> - Any data point below Q1 - 1.5 * IQR is considered a lower outlier.
> - Any data point above Q3 + 1.5 * IQR is considered an upper outlier.
> 
> **Summary:**
> 
> - **Box**: Represents the interquartile range (IQR), the middle 50% of the data.
> - **Central Line**: Indicates the median value.
> - **X (if shown)**: Indicates the mean value.
> - **Whiskers**: Extend to the smallest and largest values within 1.5 times the IQR from the quartiles.
> - **Outliers**: Data points that lie outside the whiskers, typically displayed as individual points.


### Constructing Testable Hypotheses

Given the patterns observed in the boxplot, where higher alcohol concentrations appear to be associated with shorter embryo lengths, it is essential to move from visual interpretation to a more rigorous statistical approach. This involves constructing and testing hypotheses to determine whether the observed trends are statistically significant.

#### Formulating the Hypotheses {.unnumbered}

In the context of your data, the primary goal is to determine whether different alcohol concentrations have a statistically significant effect on embryo length. To do this, we formulate a null hypothesis (\(H_{0}\)) and an alternative hypothesis (\(H_{1}\)):

- **Null Hypothesis (\(H_{0}\)):** There is no statistically significant difference in the mean embryo lengths between the different alcohol concentration groups. Any observed differences are attributed to random variation rather than an effect of alcohol concentration.

  \[
  H_0: \bar{x}_0 = \bar{x}_{1.5} = \bar{x}_2 = \bar{x}_{2.5}
  \]

  Here, \(\bar{x}_0\), \(\bar{x}_{1.5}\), \(\bar{x}_2\), and \(\bar{x}_{2.5}\) represent the mean embryo lengths at 0%, 1.5%, 2%, and 2.5% alcohol concentrations, respectively.

- **Alternative Hypothesis (\(H_{1}\)):** At least one of the mean embryo lengths differs significantly from the others, suggesting that alcohol concentration has a measurable impact on embryo length.

  \[
  H_1: \text{At least one } \bar{x} \text{ is different}
  \]
  
><h4 id="origin-of-hypothesis-testing">The Origin of Hypothesis Testing</h4> 
> Hypothesis testing emerged in the early 20th century through the work of statisticians like Ronald A. Fisher, who applied these methods to agricultural experiments, leading to significant advancements in statistical methodology. However, Fisher's legacy is also marred by his support for eugenics, reflecting the darker intersections of early statistical science with discriminatory ideologies.
>
> While hypothesis testing remains central to scientific research, the use of **p-values** has faced criticism. P-values, often misinterpreted, simply measure data compatibility with the null hypothesis, not the truth of the hypothesis itself. The conventional threshold (p < 0.05) can lead to arbitrary decisions, and practices like p-hacking undermine the validity of results.
>
> Alternatives and complements to p-values include **confidence intervals** (which offer a range of likely values for parameters), **Bayesian methods** (which incorporate prior knowledge into probability assessments), and **effect sizes** (which quantify the magnitude of an effect). 
>
> Understanding these tools and their limitations enables researchers to draw more nuanced and reliable conclusions from their data.

#### Getting Ready to Test {.unnumbered}

A **One-way ANOVA** is a statistical test used to determine whether there are significant differences between the means of three or more independent (unrelated) groups. In this case, the groups are the different levels of alcohol concentration (0%, 1.5%, 2%, and 2.5%).

The term "one-way" refers to the fact that we are analysing the effect of a single factor (alcohol concentration) on the dependent variable (embryo length). If we were interested in analysing the impact of an additional dependent variables (e.g. incubation temperature), as well as any interaction effects with alcohol concentration, we would likely want to perform a "two-way" ANOVA.

We are using ANOVA in this context because:

- **Multiple Group Comparisons**: We have more than two groups (four alcohol concentration levels), and ANOVA is designed to handle comparisons across multiple groups simultaneously. This is more efficient and statistically sound than performing multiple t-tests, which would increase the likelihood of Type I errors (false positives).

- **Assessing Variability**: ANOVA compares the variability within each group (i.e., how much embryo lengths vary within each alcohol concentration) to the variability between groups (i.e., how much the group means differ from each other). This helps us determine if the observed differences in means are greater than what we would expect by chance alone.

#### Assumptions of ANOVA {.unnumbered}

For ANOVA to be valid, certain assumptions must be met:

1. **Independence of Observations**: The data points in each group are independent of each other. In other words, each data point corresponds to a unique embryo.
2. **Normality**: The distribution of the residuals (differences between observed and predicted values) should be approximately **normal**.
3. **Homogeneity of Variances**: The variances within each group should be **roughly** equal.

In this case, I have engineered the dataset so that these assumptions are met. However, in practice, when working with real data, it is crucial to check whether these assumptions hold before applying ANOVA. If the assumptions are violated, the results of the ANOVA may not be reliable, and alternative methods may be necessary. I'll show you how to check that the assumptions for a statistical test are met in Chapter 5 and the available alternative tests for cases where the assumptions are not met.

### Performing a One-Way Anova in Excel

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/tMEJjeNG998?si=zJiqgP57PZDCQMvg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
>**Excel Alternative**
>
>If you are having trouble accessing the Desktop version of Excel then here is an alternative video.
>["ANOVA with Google Sheets"](https://youtu.be/llk_wWySMTA)


Please be reminded that the guidance in the video above, and in the text below, is only relevant if you are using the desktop version of Excel on a Windows machine. It might work on a Mac - there's no guarantee, and it won't be relevant if you are using Google Sheets. **If you can't do the following with your personal device then please use one of the University machines**.

**Step 1**: Install and Activate the Data Analysis ToolPak

To perform a One-Way ANOVA in Excel, you need to install and activate the Analysis ToolPak add-on.

1. Open Excel and go to **File > Options**.
2. In the Options menu, select **Add-ins**.
3. At the bottom, next to "Manage", ensure **Excel Add-ins** is selected and click **Go**.
4. In the Add-Ins box, check the **Analysis ToolPak** option and click **OK**.
5. You should now see a **Data Analysis** button in the **Data** ribbon.

**Step 2**: Create a Pivot Table to Reshape Data

Before performing the ANOVA, you need to reshape your long data into a wide format using a pivot table. Here’s how to do it:

1. Select your dataset (including headers).
2. Go to the **Insert** tab and click on **PivotTable**.
3. In the Create PivotTable dialog box, select where you want the PivotTable to be placed (e.g., New Worksheet).
4. In the PivotTable Fields pane:
   - Drag **conc_pc** to the **Columns** area.
   - Drag **id** to the **Rows** area.
   - Drag **length_micron** to the **Values** area.
5. The resulting pivot table will have the concentration levels as columns and the measurements as rows, which is the required format for One-Way ANOVA.

**Step 3**: Perform the One-Way ANOVA

1. Click on the **Data** ribbon and select **Data Analysis**.
2. In the Data Analysis dialog, select **ANOVA: Single Factor** and click **OK**.
3. In the **ANOVA: Single Factor** dialog box:
   - **Input Range**: Select the range of your reshaped data (including the labels).
   - **Grouped By**: Choose **Columns** (since each group is in a separate column).
   - **Labels in First Row**: Check this option if you included labels.
   - **Alpha**: Set this to 0.05 (the default significance level).
   - **Output Options**: Choose where you want to display the results (e.g., New Worksheet Ply). You can name the new sheet "Results".

4. Click **OK** to run the analysis. You should see a new sheet that looks identical to that shown in \@ref(fig:anova).

```{r anova, echo=FALSE, fig.cap="Results tables of one-way ANOVA in Excel", out.width="100%"}
knitr::include_graphics("img/02/figure_5.png")
```

**Step 4**: Interpret the Results

Excel will generate a new worksheet with the ANOVA results. The output includes two tables:

1. **Summary Table**:
   - **Count**: Number of observations in each group.
   - **Sum**: Sum of all values in each group.
   - **Average**: Mean value of each group.
   - **Variance**: Variability within each group.

2. **ANOVA Table**:
   - **Source of Variation**:
     - *Between Groups*: Variability between the groups.
     - *Within Groups*: Variability within each group.
   - **SS (Sum of Squares)**: Measure of the total variation.
   - **df (Degrees of Freedom)**: Calculated as the number of groups minus 1 for between groups, and total observations minus the number of groups for within groups.
   - **MS (Mean Square)**: SS divided by df.
   - **F (F-Statistic)**: Ratio of MS between groups to MS within groups.
   - **P-Value**: Indicates if the results are statistically significant.
   - **F crit**: Critical value of F for the given alpha level.

**Step 5**: Make a Conclusion

- Compare the **P-Value** to the alpha level (0.05):
  - If **P-Value ≤ 0.05**, reject the null hypothesis and conclude that there is a significant difference between the groups.
  - If **P-Value > 0.05**, fail to reject the null hypothesis and conclude that there is no significant difference between the groups.
  
In your ANOVA result, a p-value of **2.48E-08** means that the probability of observing the differences between your group means by random chance is exceedingly low (just 0.0000000248). Since this p-value is far below the common threshold of 0.05, we can reject the null hypothesis ($H_0$), and conclude that **there are statistically significant differences between the groups**.

While the one-way ANOVA tells us that there are significant differences between the groups, it does not specify which groups differ from each other. To determine where these differences lie, additional testing, known as **post-hoc** testing, is required. Post-hoc tests, allow us to compare the group means directly and identify which specific groups are significantly different. Although post-hoc testing requires a bit more work, it can significantly enhance our understanding of the data and provide deeper insights into the relationships between groups.

#### Understanding Exponent Notation {.unnumbered #exponent-notation}
> In statistical analysis, particularly when working with software like Excel or R, you might values expressed in scientific notation, often using the letter "E" followed by a number. For example, the p-value **2.48E-08** appears in your ANOVA results.
>
> #### What Does **2.48E-08** Mean?
>
> The notation **2.48E-08** is Excel's way of representing the number 2.48 × 10⁻⁸. Here's how to break it down:
>
> - **2.48**: This is the base number.
> - **E-08**: This indicates that the base number (2.48) should be multiplied by 10 raised to the power of -8.
>
> So, **2.48E-08** is mathematically equivalent to:
>
> \[
> 2.48 \times 10^{-8} = 0.0000000248
> \]
>
> This value is extremely small, which is typical for p-values when the test results are highly significant.
>
> #### Why Use Scientific Notation?
>
> Scientific notation is used to conveniently express very large or very small numbers that would otherwise be cumbersome to write out in full. In the case of p-values, this format is particularly useful because significant results often involve very small numbers. Instead of writing 0.0000000248, which can be error-prone and hard to read, Excel uses **2.48E-08** to convey the same information succinctly.
>

### Performing Post-hoc Tests in Excel

Following a significant ANOVA, we need to perform additional tests to determine where the differences between the groups lie. These are called **Post-hoc** tests.


```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/EHwZFRx0hr0?si=x0E9QGjAmGeoBUhY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

**Step 1: Create a new table to list group comparisons**

 - In cell **A19**, underneath your ANOVA result table, Create a new column label called **Groups**.
 - List all the possible ways two groups can be compared to each other (there are six ways in total):
   - 0% v 1.5%
   - 0% v 2.0%
   - 0% v 2.5%
   - 1.5% v 2.0%
   - 1.5% v 2.5%
   - 2.0% v 2.5%
 - Create additional column headers **P-value** and **Significant?** in cells **B19** and **C19** respectively.
   
**Step 2: Perform T-tests for each group comparison**

   - Starting in cell **B20** type:
   ```excel
   =TTEST(wide!B$4:B$164, wide!C$4:C$164, 2, 2)
   ```
   - Copy the cell down for the remaining rows and update the columns in the formula to correspond with the respective groups that are being compared.
   
#### What is a T-test? {.unnumbered #ttest}

> **Unlike ANOVA**, which compares multiple groups simultaneously to see if there are any significant differences in the means of the groups, a T-test can only compare two groups at a time.
>
>The assumptions for a T-test are similar to those for ANOVA: the data should be normally distributed, the samples should be independent, and the variances of the two groups should be equal if using a two-sample T-test assuming equal variances.
>
**Like ANOVA**, the key output of a T-test is the **p-value**.
>
>**In Excel** you perform a T-test like this:
>```excel
>=T.TEST(list1, list2, tails, type)
>```
>where:
>
> - **list1**: Corresponds to the list of values in your first group.
> - **list2**: Corresponds to the list of values in your second group.
> - **tails**: Requires a value of 1 or 2 indicating whether the test is one-tailed or two-tailed respectively.
>   - A one-tailed T-test tests for a difference in a specific direction (greater or less), while a two-tailed T-test tests for any difference regardless of direction.
> - **type**: Requires a value of 1, 2, or 3 indicating:
>   - **1**: A paired t-test (i.e., groups are not independent).
>   - **2**: An independent t-test with equal variances between groups.
>   - **3**: An independent t-test with unequal variances between groups (also known as a Welch test).
>
> **T-tests are powerful for pairwise comparisons** but need to be used in conjunction with corrections like Bonferroni when multiple T-tests are conducted, as performing multiple tests increases the risk of Type I errors (false positives).

Let's create a new table underneath my ANOVA table in Excel, say starting in cell **A19**

**Step 3: Calculate Bonferroni Corrected Alpha Level**
   - To calculate the Bonferroni corrected alpha level divide the initial threshold value of 0.05 by the number of t-tests you are performing, i.e. 6.
   - Use the corrected value by comparing it to the p-value of each of the t-tests to determine if the difference in the means of groups is significant.
   
**Step 4: Extend Your Conclusions**

The post-hoc tests indicate that there are significant differences in embryo lengths were found between the following pairs of alcohol concentrations:

- **0% vs. 2%**
- **0% vs. 2.5%**
- **1.5% vs. 2%**
- **1.5% vs. 2.5%**

These findings suggest that increases in alcohol concentration from 0% to 2%, and from 1.5% to 2.5%, lead to significant changes in embryo length. 

<h3>Feedback Please.</h3>

I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It's completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe src="https://canvaswizards.org.uk/likertysplit/qs/" width="400" style="max-width: 100%" height="600"></iframe>')
}
```

## Complete your Weekly Assignments

In the BIOS103 Canvas course you will find this week's **formative** and **summative** QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section's content. The assignments are identical in all but the following details:

   + You can attempt the **formative assignment** as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you're confident that you can get the correct answer on your own.
   + You can attempt the **summative** assignment **only once**. It will be identical to the formative assignment but will use different values and datasets. This assignment **will** contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days.
   
In **ALL** cases, when you click the button to "begin" a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit.




<!--chapter:end:02.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
\newpage

# Calibration Curves and Linear Regression in Excel 

If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover **all of this section's content** and complete this week's [**formative and summative assessments**](#chapter-3-assignments) in the BIOS103 Canvas Course.

In this section we'll be re-visiting linear trendlines in Excel in the context of calibration curves and extending our knowledge by going deeper into the world of linear regression.

We will be working on two important skills:

- Manipulating data (transposing)
- Making predictions from linear models.

## Calibration Curves {.calibration-curves}

A **calibration curve** is a plot of a measurable quantity (in our case absorbance, as determined by spectrophotometry) against the concentration of known standards. This relationship, typically linear, allows for the determination of the concentration of unknown samples by interpolation.

The Beer-Lambert Law provides the foundation for generating calibration curves in spectrophotometry. It describes the relationship between absorbance (A) and the concentration (C) of a substance in solution. The law is given by the equation:

\[
A = m \cdot C
\]

Where:

- **A** is the absorbance (a dimensionless quantity, i.e. no units!).
- **m** is the slope (related to molar absorptivity and path length).
- **C** is the protein concentration (in mg/mL).

There are a few **key considerations** to make when using this law to determine the concentrations of unknown solutions:

- **Linear range**: the calibration curve is only valid within a certain concentration range. At very high concentrations, the relationship may no longer be linear (due to factors like light scattering). Extrapolating concentrations from our calibration curves that are beyond those of our known standards is very, very naughty. Don't do it!
- **Reproducibility**: It is critical that the same instrument (including scan settings and wavelength) be use for both the standards and unknowns.


Let's take a look at an example dataset from an experiment designed to generate a calibration curve for hemoglobin. Assume that starting from a stock solution of concentration 1.5 mg/mL a set of seven protein solutions have been created using a 1:1 serial dilution and that the absorbance of each solution has been measured at a wavelength of 560nm (yellow-green light).

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/Dy6FmCDB2NQ?si=s9CWD0uGE14Fa1iP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
>**Mac Users**
>
>It might not be possible for you to transpose the data like in the video above. If you can't, try this:
>
>1. Select the range of data you want to rearrange, including any row or column labels, and **Cmd+C** to copy.
>
>2. Select the first cell where you want to paste the data, and on the Home tab, click the arrow next to Paste, and then click Transpose.
>
>3. Watch [this](https://youtu.be/pBcPJp-BdjE?si=GYw5sguCCM64Zcpl) video if you're still stuck.

1. **Download the CSV File:** 
   - Here is an [example dataset](https://canvaswizards.org.uk/dataspell/calibration/999999999). Download it to your local machine.

2. **Import into Excel:**
   - Open Excel (Use the desktop version - you won't be able to do this using the online version!).
   - Go to `Data > From Text/CSV` and select the downloaded CSV file.
   - When the import wizard appears, **do not** click **Load**.
   
There's something not right about this dataset. It's sideways! The columns of data run horizontally instead of vertically. We don't like this, it makes it much harder to plot figures and perform any analysis.

We need to transform the data somehow and flip it to vertical instead of horizontal. We need to **transpose** the data.

3. **Transpose the data:**
   - On the import wizard click the **Transform Data** button. This will open up a new window called **Power Query Editor**.
   - Click the **Transform** tab and then click the **Transpose** button.
   - Click the **Use First Row as Headers** button.
   - Return to the **Home** tab and click **Close & Load** to import your transposed data.
   
4. **Generate a Calibration Curve:**
   - Click anywhere on your data table.
   - Click the **Insert** tab and select the **Scatter** chart from the charts option.
   - Get rid of the title.
   - Label the x-axis: "Protein Conc. (mg/mL)".
   - Label the y-axis: "Absorbance (Arb. units)".
   - Click the green plus (chart elements) icon at the top-right of the graph and select **More options** from **Trendline**.
   - Add a linear trendline and check the box to "Display Equation on Chart".
   - If you're including your figure in a report it's better practice to include details of the $m$ and $b$ values that you extract in your figure caption like in figure \@ref(fig:calibration-curve).
   
5. **Use the Calibration Curve:**

Now that you've extracted the m and b values from your calibration curve's linear trendline you can use it to find the concentration of an unknown sample by measuring its absorbance.

Our calibration curve for the dataset gives us the equation:

\[
A = 0.3827 \cdot C - 0.0024
\]

If an unknown protein solution has an absorbance of 0.50, you can rearrange the equation to solve for concentration \(C\):

\[
0.50 = 0.3827 \cdot C - 0.0024
\]

Solving for \(C\):

\[
C = \frac{0.50 + 0.0024}{0.3827} = 1.3 \, \text{mg/mL}
\]

Thus, the concentration of the unknown protein solution is 1.3 mg/mL.

Note that I've expressed my answer to 2 significant figures to match the minimum level of precision available to me during the calculation (i.e. 2 s.f for both 0.5 and 0.0024).

   
```{r calibration-curve, echo=FALSE, fig.cap="Calibration curve for Hemoglobin determined from a standard set generated from a 1:1 serial dilution from a starting 1.5 mg/mL solution. Values of m=0.3827 and b=0.0024 were extracted from the linear relationship A = m * C + b, where b is the systematic error associated with the measurement (most likely related pipetting inaccuracies).", out.width="100%"}
knitr::include_graphics("img/03/calibration_curve.png")
```
## Linear Regression

A linear trendline is visually helpful, but what what Excel is actually doing behind the scenes is something more powerful: **linear regression**.

Linear regression is a statistical method used to quantify how much the variation in a dependent variable can be attributed to changes in an independent variable. It helps us understand how one variable predicts or influences another. Additionally, it provides insight into how much of the variation is due to error or other unaccounted factors, and whether we need to explore other relationships or variables that may better explain the observed patterns.

Linear regression models are valuable because they offer an estimation of the relationship between variables, and by extension, help in predicting future outcomes based on known values of independent variables.

**Why "Linear"?**

In nature, over relatively small ranges of dependent and independent variables, the relationship between them can often be approximated as linear. This means that as one variable increases or decreases, the other responds in a predictable and proportional manner. While not always the case, assuming linearity can be useful for many applications, especially for the scope of this course.

> **Independent and Dependent Variables:**  
> - The **independent variable** is the one you manipulate or change to see its effect (e.g., hours of sunlight).  
> - The **dependent variable** is the one being measured or observed (e.g., sunflower growth).  
> These are sometimes referred to as explanatory and response variables, respectively.

**Linear Regression as a Hypothesis Test**

Performing a linear regression is also a test of a hypothesis. This time, our **null hypothesis** is that there is no linear relationship between the dependent and independent variables. In other words, any observed association between the two is just due to random chance.

More formally:

- **Null Hypothesis (H₀):** There is no linear relationship between the independent variable and the dependent variable. The slope of the regression line is equal to zero.  
  - H₀: β₁ = 0  
  (where β₁ represents the slope of the regression line)

- **Alternative Hypothesis (H₁):** There is a linear relationship between the independent variable and the dependent variable. The slope of the regression line is not equal to zero.  
  - H₁: β₁ ≠ 0

### The Linear Regression Equation

The linear regression equation provides a way to express the relationship between the independent variable (predictor) and the dependent variable (outcome) as a straight line:

\[
Y = β₀ + β₁X + ε
\]

- **Y**: The dependent variable (outcome) we are trying to predict.
- **X**: The independent variable (predictor) we use to make predictions.
- **β₀**: The intercept, or the value of **Y** when **X** is 0. This represents the starting point of the relationship.
- **β₁**: The slope, or the change in **Y** for every one-unit increase in **X**. It tells us how steep the relationship is.
- **ε**: The error term, which accounts for the variance in **Y** that cannot be explained by **X**.


### Performing a Linear Regression in Excel

Let’s work through an example using a full linear regression with Excel’s Analysis ToolPak. Suppose we're investigating the **growth per day** of sunflowers (dependent variable) based on the **number of hours of direct sunlight** they receive each day (independent variable).

In this case, we want to determine if there is a linear relationship between hours of sunlight and sunflower growth. By performing the regression analysis, we’ll be able to assess if and how sunlight impacts sunflower growth, and how strong that relationship is. 

Let’s get started with the analysis in Excel!

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/xQniupalMmg?si=2cKXNSr4pNydlyDZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```


**1. Download the data**:

   - Here is an [example sunflower dataset](https://canvaswizards.org.uk/dataspell/sunflowers/999999999). Download it to your local machine.

**2. Import the data**:

   - Open up a new Excel workbook.
   - Go to the **Data** tab and select the **Import from csv/txt** icon.
   - Once the wizards has loaded click **Load**.
   
**3. Perform Regression**:

   - Click anywhere on the sheet, select the **Data** tab again and select the **Data Analysis** button in the "Analysis" area of the tools ribbon.
   - If you can't see the button then you probably need to configure the Analysis ToolPak Add-In.
   - Select "Regression" and click "OK".
   - Select the "Input Y Range" - This is your dependent variable, i.e. how much your sunflowers grew per day (column B). Include the header.
   - Select the "Input X Range" - This is your independent variable, i.e. number of hours of direct sunlight per day (column A). Include the header.
   - Tick the **Labels" box.
   - Select the **New Worksheet Ply** output option
   - Check **all** the remaining boxes for **Residuals** and **Normal Probability** sections.
   - Click **OK**. Hopefully, you'll see something like that shown in figure  \@ref(fig:regression-output).
   
```{r regression-output, echo=FALSE, fig.cap="This is what you should see after performing a regression on your example sunflower dataset using Excel's Analysis ToolPak Add-In.", out.width="100%"}
knitr::include_graphics("img/03/regression_output.png")
```   

### Interpreting Your Linear Regression

Wow! Your linear regression has provided so much information—it can feel like an avalanche at first. But don’t worry, not all of it is important for our purposes. Let’s focus on what **is** interesting and useful.

**1. Summary Output Table**:

- **Multiple R**: This is the correlation coefficient between your observed and predicted values. It ranges from -1 to 1. A value closer to 1 or -1 indicates a strong relationship, whereas a value close to 0 means a weak relationship. In our case, a value of 0.65 indicates a reasonably strong correlation between growth and sunlight.
  
- **R Square (R²)**: This tells us how much of the variance in the dependent variable is explained by the independent variable(s). In our case, an R² of 0.42 means that 42% of the variation in the dependent variable is explained by the model. Conversely, it means that approx 58%, i.e. the majority, of the variance is explained by other unknown variables. In this context, these unknown variables could include things like daily average temperature, rainfall, humidity etc. 

- **Adjusted R Square**: This adjusts the R² value for the number of predictors in your model. It’s more useful when dealing with multiple independent variables as it accounts for any unnecessary complexity added by too many variables.

- **Standard Error**: This measures the average distance that the observed values fall from the regression line. A smaller standard error means a better fit. Note that the units of your standard error are the same as for your dependent variable, i.e. mm.

**2. ANOVA Table**

The ANOVA table tells us whether the overall regression model is significant.

- **Degrees of Freedom (df)**: This refers to the number of independent pieces of information that went into calculating the estimates. It’s a balance between the number of observations and the number of predictors in the model.

- **SS Regression (Sum of Squares due to Regression)**: This represents the variance in the dependent variable explained by the model.

- **SS Residual (Sum of Squares of Residuals)**: This represents the variance that the model doesn’t explain.

- **Significance F**: This is the p-value for the overall regression model. In our case, A p-value < 0.05 means that the model is statistically significant, and we reject the null hypothesis that there is no relationship between the dependent and independent variables.

**3. Coefficient Table**

This table is where you’ll find the key parameters for your linear regression model.

- **Intercept (β₀)**: This is the expected value of the dependent variable when the independent variable is zero. In other words, it's your y-intercept value.

- **Gradient (β₁)**: This is the slope of your line, showing how much the dependent variable changes with each unit increase in the independent variable.

- **P-values**: These tell us whether the coefficients (intercept and gradient) are statistically significant. If the p-value for the gradient is less than 0.05, we can say that the independent variable has a significant impact on the dependent variable.

**4. Residual Plots and Additional Tables**

- **Residual Plot**: This plot shows the differences between the observed and predicted values (residuals). Ideally, these residuals should be randomly scattered around 0, indicating a good model fit. If this is not the case then it might not be appropriate for you to be performing a linear regression.

- **Normal Probability Plot**: This checks if the residuals follow a normal distribution. If the points follow a straight line, the residuals are normally distributed. Again, if this is not the case then you should seek an alternative analysis.

- **Residual Output**: This provides detailed information about each residual (the error for each observation).

- **Probability Output**: Provides the probabilities of observing these residuals given the model fit.

### Making Predictions with Our Model

With the insights from our linear regression analysis, we can now use our model to make predictions using the equation

\[ \hat{Y} = \beta_0 + \beta_1 \hat{X} \]

where:
- \( \hat{Y} \) is the predicted value of the dependent variable.
- \( \beta_0 \) is the intercept, representing the expected value of \( Y \) when \( X \) is zero.
- \( \beta_1 \) is the gradient (or slope), indicating how much \( Y \) changes with a one-unit change in \( X \).
- \( \hat{X} \) is the value of the independent variable for which we want to make a prediction.

For example, to predict the daily growth of our sunflowers when they've received six hours of sunlight:

\[ \hat{Y} = 2.449 + ( 0.898  \times 6 )= 7.836 \]

So, the predicted value of \( \hat{Y} \) for \( \hat{X} = 6 \) is \( 7.836\) \( mm \).

But wait! Couldn’t I have achieved this by simply adding a trend line to a plot of \( X \) vs. \( Y \), as discussed in section \@ref{section:calibration-curves}? Yes, you could have extracted the coefficients \( \beta_0 \) and \( \beta_1 \) from the equation of the line, and even added the \( R^2 \) value to the plot. So why go through all this additional work? 

The full regression analysis allows us to determine the **standard error** (\(SE\)) of the model, which enables us to calculate the uncertainty associated with a predicted value \( \hat{Y} \). This is something we couldn’t do with a simple trend line!

This uncertainty, often referred to as the **margin of error** (\(ME\)), provides the range within which future individual observations are expected to fall. 

To calculate \(ME\):

\[ ME = t \times SE \]

where \(t\) is the t-statistic which, in this context, can be drawn from a **Student's t-distribution** and accounts for the uncertainty in estimating the population parameters from a sample. In excel you can calculate \(t\) using:

```excel
=T.INV.2T(alpha, df)
```

- **alpha** is the significance level, in this case 0.05.
- **df** is the degrees of freedom which can be calculated as \(df = n - k - 1\) where \(n\) is the number of data points (observables) and \(k\) is the number of independent variables. In our case, \(n=24\) and \(k=1\) so \(df=24-1-1=22\).




### Extending to Multiple Variables

Right now, we’re focusing on simple linear regression (one dependent and one independent variable). However, what if we wanted to examine the effect of **multiple independent variables** (e.g., sunlight **and** temperature on sunflower growth)? This is called **multiple regression** and we'd consider the following linear regression equation instead:

\[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n + \epsilon \]

Where:

- \( X_1, X_2, ..., X_n \): The independent variables (e.g., sunlight, temperature, etc.).
- \( \beta_0 \): As before, this is the intercept, representing the expected value of \( Y \) when all independent variables are 0.
- \( \beta_1, \beta_2, ..., \beta_n \): The coefficients of the independent variables, representing the change in \( Y \) for a one-unit change in \( X \), assuming all other variables remain constant.

Multiple regression allows us to assess the combined effect of several factors, which is crucial when studying complex systems where multiple variables might influence the outcome.

Unfortunately, Excel has its limits for this. But don’t worry—we’ll tackle multiple regression using R later!



## Complete your Weekly Assignments 

In the BIOS103 Canvas course you will find this week's **formative** and **summative** QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section's content. The assignments are identical in all but the following details:

   + You can attempt the **formative assignment** as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you're confident that you can get the correct answer on your own.
   + You can attempt the **summative** assignment **only once**. It will be identical to the formative assignment but will use different values and datasets. This assignment **will** contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days.
   
In **ALL** cases, when you click the button to "begin" a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit.

<!--chapter:end:03.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
\newpage

# Introduction to R: Part I

If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover **all of this section's content** and complete this week's **formative and summative assessments** in the BIOS103 Canvas Course.

We are going to diverge from what you have been doing in the labs this week in order to focus on introducing you to R. Our learning objectives are as follows:

- Start a new R Project in R Studio
- Read a dataset to a variable
- Inspect a dataset
- Subset a dataset by slicing and filtering
- Create summary tables of descriptive statistics
- Sort a table

You should have completed section \@ref(getting-ready-for-R) already and have R and R Studio installed on your personal machine. If you are unable to install R and RStudio then please use one of the University computers.

<h3>OMG. Why, are you making me learn to code?</h3>

In my experience, at least half of you will love learning to code. The rest of you will hate my guts. I'm OK with that. That's because I truly believe that in the near future you will see that even a basic understanding of coding gives you a huge advantage in your studies and future careers. However, if you really need me to give you explicit reasons as to why I'm making you learn to code then here are six:

**1. Data Literacy**

All the biosciences rely heavily on data analysis. Learning to code will equip you the skills to efficiently manipulate and interpret complex datasets. R, in particular, is well-suited for handling bioscientific data, from genomic sequences to clinical trials and environmental modelling.

**2. Reproducibility**

Coding promotes transparency and reproducibility in scientific analyses. Unlike manual methods, where calculations or procedures may be difficult to replicate, scripts provide a clear, step-by-step record that can be easily shared and re-run. This is vital for ensuring the integrity of scientific research.

**3. Automation**

Bioscientists often work with large datasets, and coding enables the automation of repetitive tasks such as data cleaning, statistical analyses, and reporting. This not only saves time but also minimises human error, allowing students and researchers to focus on interpreting their results.

**4. Career Readiness**

Coding has become an indispensable skill in the biosciences. Whether working in research, biotechnology, or environmental consultancy, understanding how to work with data in R (or similar tools) will give you a competitive edge when applying for jobs or pursuing further academic research.

**5. Critical Thinking**

Writing code fosters critical thinking and problem-solving skills. While learning to code you will have to break down complex problems, troubleshoot errors, and develop logical workflows. These skills are not only useful for coding but are also vital for scientific thinking and research.

**6. Integration with Lab Skills**

Coding complements traditional lab skills. In the era of bioinformatics and systems biology, being able to analyse experimental data computationally can provide additional insights that are often not apparent from lab experiments alone.


Have you accepted your fate then? Good. Now lets get something else straight: **I am NOT going to teach you to code**. You are going to do that yourself, over many months (and probably years). This book represents a mere "dipping of the toes" into the world of R coding and is by no means comprehensive. In fact, it is completely and utterly incomplete. You will need to fill in many of the blanks yourself as you encounter them. How you go about this is up to you, but checkout the [Appendix](#appendix) for good places to start.





## Reading and Inspecting Data

Let's dive in then! This week we're going to be introducing you to some fundamental data handling concepts in R using the fantastic [Pantheria](https://esajournals.onlinelibrary.wiley.com/doi/10.1890/08-1494.1) dataset of extant and recently extinct mammals.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/ReFt0oM49UA?si=Z0seJdmGXTOZRtgI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

1. **Open RStudio and start a new project**

2. **Download the data** from [here](https://canvaswizards.org.uk/dataspell/pantheria/999) (or right click, save as from [here](https://raw.githubusercontent.com/rtreharne/qs/refs/heads/main/data/04/pantheria_999.csv)). 

3. **Copy the downloaded file** into your RStudio project directory (folder). You should see the file appear in your **Files** window in the bottom-right corner of RStudio.

4. **Create a new R script file** and save it with a sensible filename (e.g. pantheria_summary.R)

5. **Read the data** to a [variable](#what-is-a-variable) called `pantheria_data`: In your new script file, write the following line of code on line 1:

```{r eval=FALSE, class.source = "numberLines"}
pantheria_data <- read.csv("pantheria_999.csv")
```
Run the line by clicking anywhere on the line and then clicking the **run** button (in the top-right corner of your script window) or by pressing **Ctrl+Enter** on your keyboard. You should see a new line appear in your **Environment** window in the top-right of your RStudio like in figure \@ref(fig:environment-window). If you don't see it then the here are the most common reasons as to why:

- You haven't used quotation marks around your data filename
- You've mis-spelled your filename
- You haven't put your data file in the correct folder
- You didn't create an R project first.

```{r environment-window, echo=FALSE, fig.cap="Details of your data variable should appear in your environment window after running line 1.", out.width="100%"}
knitr::include_graphics("img/04/environment.png")
```

```{r pantheria, echo=FALSE, fig.cap="By clicking on your data variable in your environment window you can inspect the data in RStudio", out.width="100%"}
knitr::include_graphics("img/04/pantheria.png")
```
><h3 id="what-is-a-variable">What is a variable in R?</h3>
>A variable in R is a symbolic name representing a value stored in memory. Variables are created by assigning values using `<-`.
>
>**Naming Convention**
>
>I prefer using lowercase words for variable names. If multiple words are needed, I separate them with an underscore "_", avoiding spaces. For example: my_variable.
>
>**Best Practices**
>
> - Descriptive Names: Choose clear, meaningful names.
> - Consistency: Use the same naming style throughout your code.
> - Avoid Reserved Words: Don’t use R’s reserved keywords (`if`, `else`, `for`, etc.)
> - Uniqueness: Ensure variable names are unique within your environment.
>
>These practices enhance code readability and maintainability.


Well done if you can see your data variable! You can click on your variable (in the environment window) and inspect your data directly. You should see something like that shown in figure \@ref(fig:pantheria). Wow! Look at all that data. Yummy. Now the world is your oyster, as they say. From here you can go in any number of directions and use R to summarise, visualise and analyse your data in order to gain novel insights.

Let's start small by asking the very simple question:

**"What order of mammal occurs the most frequently (has the most number of rows) in the dataset?".**

To answer this question, I need to go through my data row by row and count how many times a row corresponding to each order (e.g. Dermoptera, Chiroptera, Rodentia etc ...) appears. Let's extend my R script as follows:

```{r eval=FALSE, class.source = "numberLines"}
pantheria_data <- read.csv("pantheria_999.csv")

orders <- table(data$Order)

print(orders)
```

Let's break these additional lines down, step-by-step:

- **Firstly**, you'll notice that I've left lines 2 and 5 blank. You don't need to do this, but I like to because it gives my code a bit of breathing space and is often easier to identify a problem later.

- **In line 3**, I'm using the `table()` [function](#what-is-a-function) to count the number of times a unique value in my order column appears. By putting `data$Order` inside the table function's brackets we are "passing" it all of the values in the **Order** column as a big list. I'm then assigning the output of the table function to a new variable called `orders` so that I can refer to it later.

- **Lastly in line 5**, I'm using a print function to output the contents of the variable `orders` to my console as shown in figure \@ref(fig:console-output-order)

```{r console-output-order, echo=FALSE, fig.cap="When using `print(orders)` the following should appear in my RStudio console.", out.width="100%"}
knitr::include_graphics("img/04/orders-output.png")
```

><h3 id="what-is-a-function">What is a function in R?</h3>
>A function in R is a set of instructions that performs a specific task or calculation, taking inputs (arguments) and returning an output (result). So far in this course, we have used three pre-defined functions: read.csv() to read data from a CSV file, table() to summarise categorical data, and print() to display output in the console. While these functions are built into R, we can also create our own functions when we need to perform customised tasks—more on this later in the course.

You can see by inspecting your console output that the order **Rodentia** is the most frequently occurring order with **549** rows in the data. 

But what if there were lots more orders? It might not be so straightforward. Let's get our R script to pick out the most frequently occurring order for us:
```{r eval=FALSE, class.source = "numberLines"}
pantheria_data <- read.csv("pantheria_999.csv")

orders <- table(data$Order)

most_common_order <- names(which.max(orders))

print(most_common_order)
```

I've inserted the line `most_common_order <- names(which.max(orders))` at line 5. Let's break it down:

1. `which.max(orders)`:

  + **Function**: `which.max()` is a built-in R function that returns the index of the first maximum value in its input.
  + **Input**: In this case, the input is `orders`, which is the table we created earlier containing the counts of each order in the dataset.
  + **Output**: The function identifies the **index** position of the maximum count (i.e., the highest frequency) in the `orders` table. In our case "Rodentia" is at position 24 in the table.
  
2. `names(...)`:

  + **Function**: The `names()` function retrieves the names (or labels) associated with the elements of an object. For example if I used `names(orders)` the function would simply return a list of all the unique orders.
  + **Usage**: By wrapping `which.max(orders)` inside `names()`, we are saying, "give me the name of the order that corresponds to the maximum frequency index returned by `which.max()`."
  + **Output**: In our case the name "Rodentia" is returned.
  
3. `most_common_order`: I've assigned the result of my `names(which.max(orders))` to a new variable using `<-`.
  
4. Finally, I've updated my print statement to `print(most_common_order)'. This should now print out "Rodentia" in my console when I re-run my script.
  
Phew! That took a bit of explaining. Make sure you understand what just happened. The wrapping of a function in a function is a common practice in R and things can quickly become obfuscated. It's good practice to comment your code so that you (or someone else) can quickly make sense of what is going on. Here is my final code, with comments!

```{R eval=FALSE, class.source="numberLines"}
# Read the CSV file and store its contents in 'data'.
data <- read.csv("pantheria_999.csv")

# Create a frequency table of the 'Order' column.
orders <- table(data$Order)

# Get the name of the most common order.
most_common_order <- names(which.max(orders))

# Print the most common order to the console.
print(most_common_order)
```
## Subsetting

In data analysis, subsetting refers to the process of extracting a portion of a dataset based on specific criteria. This allows you to focus on the most relevant information, making analysis more efficient and tailored to your needs. There are two key methods for subsetting data in R: **Slicing** and **Filtering**.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/umjznd-sCRI?si=QBDhIErUU9c7-_gN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

### Slicing
Slicing is a crucial operation in data analysis, enabling you to extract specific rows, columns or both from an existing data frame. This technique helps streamline workflows, especially when working with large datasets, by narrowing down data to focus on relevant sections.

**Slicing Example**

Let's add a new line to our earlier script file:

```{R eval=FALSE}
slice <- data[1:1000, 1:10]
```
This code creates a new variable called `slice` and assigns to it the first 1,000 rows and the first 10 columns of the dataset `data`. 

Run the line and you should see a new dataframe called `slice` appear in your **Environment** window (top-right).

What if I'd wanted to select ALL the rows but still slice off the first 10 columns?
Well, that would look like this:

```{R eval=FALSE}
slice <- data[, 1:10]
```
And what if I don't want to use index values to select the columns (i.e., 1:10) and I want to select them by name? For example let's say I've inspected the column headers and I specifically want to select the following columns:

- **Order**
- **AdultBodyMass_g**
- **BasalMetRate_mLO2hr**

I can do that using this code:
```{R eval=FALSE}
slice <- data[, c("Order", "AdultBodyMass_g", "BasalMetRate_mLO2hr")]
```




Here I've defined a vector using the `c()` function containing the verbatim names of columns I want and used this instead of specifying a range of column indices.

### Filtering

Unlike slicing, which selects a subset of rows and columns based on their position, filtering involves evaluating each row against a set of **logical conditions** and including only those that meet the criteria.

**Filtering Example**

In the above slicing example we created a dataframe variable called `slice` which contains three columns. If you click on the variable in the environment window to select it you'll see something like that shown in \@ref(fig:slice-table).

```{r slice-table, echo=FALSE, fig.cap="You've sliced your data but you'll need to be able to filter in order to remove the -999 values.", out.width="100%"}
knitr::include_graphics("img/04/slice_table.png")
```

Hmmm, that's weird. What's with all the *-999* values? In this dataset the value -999 has been used to indicate where a value wasn't measured. I can remove these -999 values in my `slice` dataframe like this:
```{R eval=FALSE}
filtered <- slice[slice$AdultBodyMass_g >=0 & slice$BasalMetRate_mLO2hr >=0, ]
```
Let's break this down:

1. `filtered <-`:
  - This assigns the result of the filtering operation to a new variable called filtered. This variable will contain only the rows from the slice data frame that meet certain conditions.
2. `slice[slice$AdultBodyMass_g >= 0 & slice$BasalMetRate_mLO2hr >= 0, ]`:
  - This part performs the actual filtering operation on the slice data frame.
    + `slice$AdultBodyMass_g`:
      - This accesses the *AdultBodyMass_g* column of the slice data frame. The $ operator is used to refer to a specific column.
    + `slice$BasalMetRate_mLO2hr`:
      - Similarly, this accesses the *BasalMetRate_mLO2hr* column of the slice data frame.
    + `slice$AdultBodyMass_g >= 0`:
      - This condition checks each value in the *AdultBodyMass_g* column to see if it is greater than or equal to 0, generating a logical vector (TRUE or FALSE) for each row.
    + `slice$BasalMetRate_mLO2hr >= 0`:
      - This condition checks each value in the *BasalMetRate_mLO2hr* column to see if it is greater than or equal to 0, producing another logical vector.
    + `&`:
      - This is the logical AND operator, which combines the two logical vectors created by the previous conditions. The result is a new logical vector that is TRUE only for rows where both conditions are TRUE.
    + slice[... , ]:
      - The entire expression inside the square brackets is used to subset the slice data frame. The rows for which the combined condition is TRUE are selected, and all columns (indicated by the empty space after the comma) are returned.
      
In summary, the code creates a new data frame, `filtered`, which contains only the rows from the slice data frame where both the *AdultBodyMass_g* and *BasalMetRate_mLO2hr* values are greater than or equal to 0.

### So what?

Well, being as you've gone to all that trouble to slice and filter your data we should probably do something spectacular with it! Try adding the following code to your script to and running it.

```{R eval=FALSE}
plot(slice$AdultBodyMass_g, slice$BasalMetRate_mLO2hr,
     log="xy",
     xlab = "Adult Body Mass (g)", 
     ylab = "Basal Metabolic Rate (mLO2/hr)",
     main = "")
```

You should see something that looks like what is shown in figure \@ref(fig:logplot). 

```{r logplot, echo=FALSE, fig.cap="An all species log-log scatterplot of mammal adult body mass (g) vs basal metabolic rate (mLO$_2$/hr)", out.width="100%"}
knitr::include_graphics("img/04/plot.png")
```

Notice that I've used the argument `log="xy"` within my `plot()` function to make both the x and y axes scale logarithmically. This means that instead of the axes increasing in equal increments (e.g., 1, 2, 3), they increase by factors of ten (e.g., 10, 100, 1000).

This is particularly useful when the relationship between two variables spans several orders of magnitude, as is often the case in biological data. When both axes are scaled logarithmically, relationships that might look curved on a linear scale can appear as straight lines, making it easier to identify patterns.

In this case, the plot reveals a linear relationship between the two variables (e.g., adult body mass and basal metabolic rate) when viewed on a log-log scale. This linearity indicates that as one variable increases by a certain percentage, the other variable increases by a consistent percentage, rather than by a fixed amount. This relationship is described as **allometric** scaling and is remarkably common in nature.

What's fascinating is that this log-log linearity holds across all mammals! From the smallest mouse to the largest whale, body mass and metabolic rate follow a consistent scaling relationship when plotted logarithmically. This insight allows scientists to understand fundamental principles about how biological processes like metabolism are related to size in the animal kingdom.


## Summarising Data in R

Remember how in chapter 2 we generated a summary table of descriptive statistics using Excel? We can do it in R too! 

As an example, let's answer the following question:

**What are the top 10 families with the highest number of neonate body mass observations, along with their summary statistics (mean, median, max, min, and standard deviation)?**

The following script will do the job, as you can see by the accompanying output.


```{R class.source="numberLines"}
# Read the CSV and filter out invalid rows
data <- read.csv("pantheria_999.csv")
data_filtered <- data[data$NeonateBodyMass_g != -999, ]

# Calculate summary statistics for each Family
summary_table <- aggregate(NeonateBodyMass_g ~ Family, data_filtered, function(x) 
  c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x), Count = length(x)))

# Convert the list columns to individual columns
summary_table <- do.call(data.frame, summary_table)

# Rename the columns
colnames(summary_table) <- c("Family", "Mean", "Median", "Max", "Min", "Std. Dev.", "Count")

# Format all numeric values to 1 decimal place (except Count)
summary_table[, 2:6] <- round(summary_table[, 2:6], 1)

# Sort by Count in decreasing order and return top 10 most frequently occurring families in data
summary_table <- summary_table[order(-summary_table$Count), ][1:10, ]

# Write to CSV and print
write.csv(summary_table, "summary_table.csv", row.names = FALSE)
print(summary_table)
```
See if you can reproduce the output by creating a new script file in your project, copying and pasting the code above and clicking the **Source** button in the top right of your script editor window.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/sSV2hECLflY?si=X-NdWvY04uZe3WrJ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

<h3>What Next?</h3>
I don't need you to understand every line in the script. In fact there are some lines (6 -7 in particular) that are doing some seriously funky stuff that took me years to learn and understand fully. Nonetheless, ask yourselves what would you need to tweak to get the code to generate a summary table for another variable (e.g. *AdultBodyMass_g*) or group the results by **Order** instead of **Family**.

Often, when learning to code, it can feel overwhelming trying to understand every single detail of a script before running it. However, there’s value in a **“run it and see what happens”** approach, especially when you’re starting out. The beauty of coding is that you don’t need to fully grasp every element in order to get results.

In fact, many experienced coders begin with scripts they might not completely understand. The key is knowing just enough to use the script to answer your question. Once you see the output and observe how the code works, you can begin to tweak and modify it to suit a different dataset or to refine your results.

The process of experimenting with the code helps deepen your understanding over time. You’ll gradually learn how each part of the script contributes to the output, and that’s where real learning happens.

>**Turbo charge your learning with Chat-GPT**
>
>I have serious mis-givings about asking generative AI to generate code out of thin air. In my experience, GAI likes to show off and often over-complicate things. This can seriously confuse students who are just starting to learn to code.
>
>However, **using it to understand existing code**? Yes! Do it! What a fantastic way to learn. I use it every day when it comes to code.
>
>I cut and pasted the script above into Chat-GPT and gave it the following prompt: "Explain in detail".
>
>You can see the result [here](https://chatgpt.com/share/6703dee6-7f58-800a-baf0-fedc569fcc59). Isn't that amazing?


<h3>Feedback Please.</h3>

I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It's completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe src="https://canvaswizards.org.uk/likertysplit/qs/" width="400" style="max-width: 100%" height="600"></iframe>')
}
```

## Complete your Weekly Assignments

In the BIOS103 Canvas course you will find this week's **formative** and **summative** QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section's content. The assignments are identical in all but the following details:

   + You can attempt the **formative assignment** as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you're confident that you can get the correct answer on your own.
 + You can attempt the **summative** assignment **only once**. It will be identical to the formative assignment but will use different values and datasets. This assignment **will** contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days.
   
In **ALL** cases, when you click the button to "begin" a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit.

<!--chapter:end:04.Rmd-->

# Introduction to R part II: Visualisation

If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover **all of this section's content** and complete this week's **summative assessment** in the BIOS103 Canvas Course. There is no formative assessment this week.

Our brains are hardwired to recognise patterns, which means we can understand information much more easily when it's presented visually rather than as raw numbers. In today’s data-driven world, the ability to visualise information effectively is more important than ever. Learning to visualise data will equip you with essential skills for analysing and interpreting information, allowing you to communicate insights clearly and persuasively.

Effective visualisation also fosters critical thinking. It helps you identify trends, relationships, and anomalies within datasets, enhancing your analytical abilities and preparing you for future roles in research, industry, and beyond, where data-driven decision-making is key.

This week, we’re going to explore visualisation in R, focusing on creating impactful graphics using its native capabilities. Our objectives include:

+ **Histograms**: Visualising the distribution of continuous variables.
+ **Boxplots**: Showing data spread between groups and highlighting outliers.
+ **Scatterplots**: Displaying relationships between two continuous variables.

By the end of this chapter, you will be equipped to create these essential visualisations, enhancing your analytical skills and bringing your data to life!

In an effort to ensure that this week's chapter is relevant to your microbiology lab practicals, I have carefully crafted the following scenario to provide a bit more context for our visualisation.

## Mushroom Compost Scenario

**MegaMush**, a mushroom compost company based in the Netherlands, is trying to identify an issue with its pre-pasteurisation composting process across its five operational production sites.

You have been hired as an **independent bioscience data consultant** and have been tasked with summarising and visualising a dataset compiled over a period of 1 year and consolidated from each location.

Specifically, you have been asked to generate the following for incorporation into a report that will be presented by the chief technical officer of the company to the extended board of directors:

+ A **summary table** that shows the mean and standard deviations of the composting temperature, moisture, and viable bacterial count, *VBC* (cfu/g), at each site.
+ A **histogram** showing the distribution of estimated viable bacterial count, *VBC* from across all samples.
+ A grouped **boxplot** showing the distributions of estimated *VBC* for each site.
+ A **scatterplot** that shows the relationship between temperature and *VBC*.
+ Any additional visualisations (histogram, boxplot or scatterplot) that identify other interesting features in the data.

### The dataset

You can download the raw data [here](https://canvaswizards.org.uk/dataspell/compost/999) or from the [backup link](https://raw.githubusercontent.com/rtreharne/qs/main/data/05/compost_999.csv).

### Further information

The compost **temperature** and **moisture content** were measured directly during the sampling process using calibrated thermometers and moisture probes.

After collection, samples were immediately placed into sterile containers to prevent cross-contamination and transported to the central laboratory under refrigerated conditions at 4°C. To ensure the integrity of microbial counts, all samples were processed within 24 hours of collection.

In the laboratory, **0.5g** of each compost sample was dissolved in **10mL** of sterile reagent and mixed thoroughly to create the starting solution for subsequent serial dilution. In each case, a **1/1000** serial dilution was performed and **0.1mL** of the diluted sample was plated onto selective agar media. Plates were incubated for 48 hrs at 30°C before the number of viable colonies were counted.

This procedure was performed regularly for each site, ensuring consistent data collection on microbial counts, temperature, and moisture content across all locations over the course of a year.

Your tables and figures need to be formatted according to the [MegaMush report template](https://raw.githubusercontent.com/rtreharne/qs/main/data/05/Megamush_template.docx)


## Inspecting and Summarising

Right then, let's start. You know the drill by now:

+ **Create a New R Project**
+ **Download the dataset and move to project folder**
+ **Create a new script file and save it with a sensible filename**
+ **Read your dataset to a new variable called `data`**

That should create a `data` variable in my environment window. I could click on it and look at the data, but that method was so "last week". This time I'm going to simply add the line `head(data)` to my script and run it to spit out the first 5 rows of my data (including headers) in my console. Here's what my script (and its output) looks like so far:

```{R}
# Script File: mushroom_summary.R

# Read the data
data <- read.csv("compost_999.csv")

# Output the first 5 rows in console
head(data)
```
OK. So it doesn't look too pretty in my console, but I can glean the important information - i.e. the data headers and the types of data that are in each column. The headers are self explanatory , except perhaps for **Viable.counts** which is a direct count of the number of observed colonies formed on each sample plate. This is not to be confused with **Viable Bacterial Count**, VBC, defined as the number of colony forming units per gram (cfu/g).

<h3>Calculating VBC from Viable Counts</h3>

To calculate the **Viable Bacterial Count (VBC)** from the **Viable Counts** observed on the agar plates, we need to consider the specific parameters of the laboratory procedure. Here's how the calculation works step-by-step:

1. **Viable Counts**: The number of colonies (cfu) observed on an agar plate after incubation. Let's denote this **VC**
2. **Dilution Factor**: Remember, all starting solutions have undergone a **1/1000** serial dilution, we must first multiply by a factor of **1000**.
3. **Volume Plated**: From each diluted sample, **0.1 mL** was plated onto the agar. This means that only **0.1mL** of the diluted solution contributes the visible colonies counted. To express the counts in terms of **1mL**, we must multiply by another factor of **10**.
4. **Volume of Starting Solution**: At this points we have units of cfu/mL. To figure out how many cfu there were in the starting solution of 10 mL we need to multiply by yet another factor of **10**.
4. **Weight of Sample**: Finally, since **0.5 g** of each compost sample was used in the initial preparation, we need to express the results per gram. To convert out counts to reflect the weight of the sample, we divide by the weight used, which is **0.5 g**. This can be expressed as multiplying by a factor of **2**.

Putting all this together, the equation to calculate **VBC** from the observed **Viable Counts** can be expressed as:

\[
\text{VBC (cfu/g)} = \frac{\text{VC} \times (dilution\ factor)}{(spread\ plated\ volume)} \times \frac{(starting\ solution\ volume)}{(sample\ mass)}
\]

\[
\text{VBC (cfu/g)} = \frac{\text{VC} \times (1000)}{(0.1)} \times \frac{(10)}{(0.5)}
\]


Thus, the final calculation simplifies to:

\[
\text{VBC (cfu/g)} = \text{VC} \times 1000 \times 10 \times 10 \times 2 = \text{VC} \times 200,000
\]

This equation indicates that for every viable colony observed, the equivalent VBC is determined by multiplying by a factor of 200,000 to account for the dilution, volume plated, and sample weight, providing a standardised measure of viable bacteria per gram of compost.


### Summary Table

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/RajQptd1m30?si=ez-g13JaBEfUEmN1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
<br>

Let's tick the first task from the scenario off our list of things to do. Let's **create a summary table that shows the mean and standard deviations of the composting temperature, moisture, and viable bacterial count, VBC (cfu/g), at each site**.

Curses! I've completely forgotten how to create a summary table in R. It's the end of the world!

No it's not. Let's do the obvious (and easy) thing and revisit the script we used in the last chapter to generate a summary table and see if we can tweak things to make it work for us again.

Here's the original script (or the first couple of lines at least).

```{R}
# Read and filter data
data <- read.csv("pantheria_999.csv")
data_filtered <- data[data$NeonateBodyMass_g != -999, ]

# Calculate summary statistics for each Family
summary_table <- aggregate(NeonateBodyMass_g ~ Family, data_filtered, function(x) 
  c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x), Count = length(x)))

# Convert the list columns to individual columns
summary_table <- do.call(data.frame, summary_table)
```

To adapt this script to work for my new "compost_999.csv" I need to consider the following things:

1. **Update the dataset**: I need to update my `read.csv()` function by replacing `pantheria_999.csv` with `compost_999.csv`.
2. **Filter step removal**: I know that my compost_999.csv dataset has no missing or erroneous data (please take my word for this) so I don't need to worry about creating a new `data_filtered` variable. I can delete the line and update the rest of the code to just use my initial `data` variable throughout.
3. **Adjust the grouping and variables**: In my aggregate function I no longer need to summarise the `NeonateBodyMass_g` by `Family`. Instead I want to summarise `Temperature`, `Moisture`, and `Viable.counts` by `Location`.
4. **Simplify descriptive statistics**: I only need to calculate the mean and standard deviation within my aggregate function this time (forget about median, min and max).

With all that in mind, here's how I'd update the script:

```{R}
data <- read.csv("compost_999.csv")

# Calculate summary statistics for Temperature, Moisture, and Viable counts for each Location
summary_table <- aggregate(cbind(Temperature, Moisture, Viable.counts) ~ Location, 
                           data, 
                           function(x) c(Mean = mean(x), SD = sd(x)))

# Convert the list columns to individual columns
summary_table <- do.call(data.frame, summary_table)

# Output table in console
print(summary_table)
```
The biggest difference between the new and old scripts is the use of the `cbind()` function within my `aggregate()` function. The `cbind()` function allows me to group together multiple variables (Temperature, Moisure, Viable Counts) in order to calculate and present their descriptive stats simultaneously. Nice.

Our summary table is nearly complete. The last thing I need to do is calculate two more columns for the mean and standard deviations of the VBC (cfu/g). As I explained above, to get values of VBC I need to multiply my viable counts by a factor of 20,000. Here's how I'd update my summary table script:

```{R}
data <- read.csv("compost_999.csv")

# Calculate summary statistics for Temperature, Moisture, and Viable counts for each Location
summary_table <- aggregate(cbind(Temperature, Moisture, Viable.counts) ~ Location, 
                           data, 
                           function(x) c(Mean = mean(x), SD = sd(x)))

# Convert the list columns to individual columns
summary_table <- do.call(data.frame, summary_table)

# Define variable factor according to calculation for VBC. Divide by 1e7 to express answers in standard form.
factor <- 2e5 / 1e7

# Calculate additional columns vbc_mean and vbc_sd using factor
summary_table$vbc_mean <- summary_table$Viable.counts.Mean * factor
summary_table$vbc_sd <- summary_table$Viable.counts.SD * factor

# Save table to file
write.csv(summary_table, "compost_summary.csv", row.names = FALSE)

# Output table in console
print(summary_table)

```
This script should work for you too. Update your existing script with the code above and check that you get the same output as that above for the example dataset.

Did you notice that I used the `write.csv()` in the penultimate line? This means that you should now see a file called **summary_table**.csv in your **Files** window. You should import this file into a new Excel workbook, format the numbers to a sensible precision, style it how you like, and then cut and paste it into your [MegaMush report template](https://raw.githubusercontent.com/rtreharne/qs/main/data/05/Megamush_template.docx). Don't forget to include a table caption according to the **unbreakable rules** stated in Chatper 1.

## Histogram

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/2hzo4nY0vIw?si=p6YTZcx6eda7LTMv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
<br>

Let's complete the second task on our list: Create a **histogram showing the distribution of estimated viable bacterial count, VBC from across all samples**.


Create a new script file, copy and paste the code below and run the whole file (click the **Source** button). You should see something like that in figure \@ref(fig:histogram-vbc) in your **Plots** window.
```{r eval=FALSE}
# Your R code for generating the histogram

# Load the data
data <- read.csv("compost_999.csv")

# Calculate the conversion factor
factor <- 2e5 / 1e7

# Calculate VBC column
data$vbc <- data$Viable.counts * factor

# Calculate the mean VBC
vbc_mean <- mean(data$vbc)

# Generate a simple histogram for all "VBC"
hist(data$vbc,
     breaks = 20,
     main = "", 
     xlab = expression(VBC ~ "(" ~ x10^7 ~ cfu/g ~ ")"), 
     ylab = "Frequency",
     col = "lightblue",
     border = "black")

# Add a vertical line for the mean
abline(v = vbc_mean, col = "red", lwd = 2, lty = 2)

# I like a box around my figures
box()
```
```{r histogram-vbc, echo=FALSE, fig.cap="Histogram of *VBC* of samples taken from all sites over a period of 1 year. The mean *VBC* (dotted red line) is $2.9 \\times 10^7$ cfu/g.", out.width="100%"}
knitr::include_graphics("img/05/histogram.png")
```
Let's break this code down comment by comment:

1. **Load the Data**

```{r eval=FALSE}
data <- read.csv("compost_999.csv")
```

  - **Purpose**: This line uses the `read.csv()` function to read in a CSV file named "compost_999.csv" and stores its content in a data frame variable called `data`
  
2. **Calculate the Conversion Factor**

```{r eval=FALSE}
factor <- 2e5 / 1e7
```

 - **Purpose**: As for the summary table in the section above, this line calculates a conversion factor to convert viable counts into *VBC* in units of $\times 10^7$ cfu/g.
 - **Scientific Notation**: `2e5` means $2 \times 10^5$ and `1e7` means $1 \times 10^7$.
 - **Why divide by 1e7?**: So that I can quote all VBC values in standard form, e.g. $1.2 \times 10^7$ cfu/g.
 
3. **Calculate VBC column**:

```{R eval=FALSE}
data$vbc <- data$Viable.counts * factor
```

 - **Purpose**: This line creates a new column `vbc` in the `data` data frame. It calculates *VBM* by multiplying the `Viable.counts` column by the conversion factor.
 - **New Column**: `data$vbc` stores the calculated *VBC* for each row in the data frame.
 
4. **Calculate the mean VBC**

```{R eval=FALSE}
vbc_mean <- mean(data$vbc)
```

 - **Purpose**: This line computes the mean of the values in the `vbc` column and stores it to a new variable called `vbc_mean`. I'll use this later shortly when I make my histrogram plot.

5. **Generate a Simple Histogram for all VBC**

```{R eval=FALSE}
hist(data$vbc,
     breaks = 20,
     main = "", 
     xlab = expression(VBC ~ "(" ~ x10^7 ~ cfu/g ~ ")"), 
     ylab = "Frequency",
     col = "lightblue",
     border = "black")
```
  - **Purpose**: This block generates a histogram of the `vbc` values.
  - **Parameters**:
    - `breaks = 20`: Specifies the number of bins (bars) in the histogram.
    - `main = ""`: Prevents a title from showing (I'll use a caption later when I import the image to a document).
    - `xlab`: Customises the x-label. The `expression()` function is used to format it, which displays the label as $VBC (\times 10^7 cfu/g)$.
    - `ylab = Frequency`: Sets the y-axis label to "Frequency".
    - `col = "lightblue"`: Sets the color of the bars in the histogram to light blue.
    - `border = "black"`: Sets the border color of the bars to black
    
6. **Add a vertical line for the mean**
```{R eval=FALSE}
abline(v = vbc_mean, col = "red", lwd = 2, lty = 2)
```
  - **Purpose**: This line adds a vertical dashed line to the histogram at the mean value `vbc_mean`.
  - **Parameters**:
    - `v = vbc_mean`: Specifies the x-coordinate where the line is drawn (at the mean value).
    - `col = red`: Sets the color of the line to red.
    - `lwd = 2`: Sets the line width to 2 (give a thicker line than the default).
    - `lty = 2`: Specifies the line type as dashed.
    
7. **I like a box around my histogram**
```{R eval=FALSE}
box()
```
  - **Purpose**: This command make sure there is a full box around the histogram. By default, only the x and y axes will show otherwise.


<h3>How useful is this histogram?</h3>

Honestly? Not very. If you were hoping for a nice bell-curve (normal) distribution then you will be disappointed. All the histogram really tells us is that values of *VBC* are broadly distributed in the range $2.5 - 3.2 \times 10^7$ cfu/g and that there seems to be a slight "skew" towards higher values in the range.

Oh well, at least you now know how to make a histogram using R in the future. Perhaps we could learn a bit more from our second type of visualisation: The Grouped Boxplot.

**Feedback Please!**

I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It's completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you.

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe src="https://canvaswizards.org.uk/likertysplit/qs/" width="400" style="max-width: 100%" height="600"></iframe>')
}
```

## Grouped Boxplot

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/M163St9JPKg?si=UoxNtNpQXyh0H7Ho" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```
<br>

I love boxplots. There, I said it. They offer such a great way to visualise how your data is distributed, highlighting clear statistics like the median, quartiles, and potential outliers. But the real clincher is the their ability to compare multiple groups within your dataset at once to let you see if something interesting is going on between them.

Take a look at the script below. Start a new script, copy/paste the code and run all lines. You should see something similar to that shown in figure \@ref(fig:grouped-boxplot) appear.

```{R eval=FALSE}
# Your R code for generating a grouped boxplot.

# Load the data
data <- read.csv("compost_999.csv")

# Calculate the conversion factor
factor <- 2e5 / 1e7

# Calculate VBC column
data$vbc <- data$Viable.counts * factor

# Adjust the margins to prevent y-axis label from being cut off
par(mar = c(5, 5, 4, 2))  # Increase the second value for the left margin

# Generate a grouped boxplot for VBC and group by location
boxplot(vbc ~ Location, 
        data = data, 
        main = "", 
        xlab = "Location", 
        ylab = expression(VBC ~ "(" ~ x10^7 ~ cfu/g ~ ")"), 
        col = "lightblue", 
        border = "black",
        cex.axis = 0.8)
```
```{r grouped-boxplot, echo=FALSE, fig.cap="Viable bacterial counts (VBC) across five production sites. Rotterdam and Utrecht exhibit noticeably lower median VBC values compared to the other sites, indicating potential issues at these locations that may require further investigation.", out.width="100%"}
knitr::include_graphics("img/05/boxplot.png")
```

You'll notice that the first three lines of code are identical to the previous histogram example. Let's break down the boxplot function.

1. **Adjust the margins to prevent y-axis label superscripts from being cut off**
```{r eval=FALSE}
par(mar = c(5, 5, 4, 2))  # Increase the second value for left margin
```
  - **Purpose**: Annoyingly, if I use any label on the y-axis that has superscript elements then the default plot area in R isn't quite big enough and they get cut off. The `par(mar = c(bottom, left, top, right))` function adjusts the margins of the plot. By using a value of 5 for the second value (left margin), I'm ensuring that the y-axis is fully visible.

2. **Generate a grouped boxplot for VBC and group by location**
```{R eval=FALSE}
boxplot(vbc ~ Location, 
        data = data, 
        main = "", 
        ylab = "Location", 
        xlab = expression(VBC ~ "(" ~ x10^7 ~ cfu/g ~ ")"), 
        col = "lightblue", 
        border = "black",
        cex.axis = 0.8)
```
  - **Purpose**:
  - **Parameters**:
    - `vbc ~ Location`: This specifies the formula for the boxplot. It creates boxplots of the `vbc` column data and groups it by the `Location` factor.
    - `data = data`: The `data` argument specifies the data frame containing the variables in the above formula.
    - `cex.axis = 0.8`: This adjusts the size of the boxplot label text, making it 80% of the default size. I like to do this as it improves readability.

<h3>Interpreting the Boxplot</h3>

For a reminder of the important features of a boxplot please refer to **Anatomy of a Boxplot** section in Chapter 2.

The key comparative insight gained from the boxplot is that there is a noticeable trend of lower VBC values at the Rotterdam and Utrecht sites. Whether this trend is statistically significant and the reasons behind the lower values at these two locations remain to be determined. However, if I were the CTO of MegaMush and you presented me with this boxplot, I would likely be making urgent phone calls to my colleagues at the Rotterdam and Utrecht sites to investigate the underlying causes of these discrepancies.

I hope you'll agree that, unlike the histogram, which provides a general overview of data distribution, the boxplot offers a concise summary of central tendency and variability, making it easier to identify specific trends and outliers across different groups. This clarity is particularly valuable for decision-making, as it allows us to quickly pinpoint areas that require further exploration or intervention.

<h3>Ideas for Further Exploration</h3>

  + Boxplot to show distribution of **Temperature** values grouped by **Location**.
  + Boxplot to show distribution of **Moisture** values grouped by **Location**.
  + Boxplots of **VBC/Temperature/Moisture** grouped by **DayOfWeek**


## Scatterplot

```{r, results='asis', echo=FALSE}
if (knitr::is_html_output()) {
  cat('<iframe width="100%" height="400" src="https://www.youtube.com/embed/mC5nkDb5s14?si=98MHviet5Jpi1i8W" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>')
}
```

<br>

The last thing on our todo list is a scatterplot. Remember, you've been asked to plot the relationship between **Temperature** and *VBC*. I also want to colour code the points by location. The code below shows how I'd do this with native R. Figure \@ref(fig:scatterplot) shows your expected output.

```{R eval=FALSE}
# Your R code for generating a scatterplot.

# Load the data
data <- read.csv("compost_999.csv")

# Calculate the conversion factor
factor <- 2e5 / 1e7

# Calculate VBC column
data$vbc <- data$Viable.counts * factor

# Adjust the margins to prevent y-axis label from being cut off
par(mar = c(5, 5, 4, 2))  # Increase the second value for the left margin

# Define unique locations and corresponding colors
unique_locations <- sort(unique(data$Location))
colors <- rainbow(length(unique_locations), alpha=0.8)  # Or use your predefined colors

# Create a scatterplot of VBC vs. temperature, colored by location
plot(data$Temperature, data$vbc,
     main = "",
     xlab = "Temperature (°C)",
     ylab = expression(VBC ~ "(" ~ x10^7 ~ cfu/g ~ ")"),
     pch = 19,  # solid circles for points
     cex = 1.5,  # increase point size
     col = colors[as.numeric(factor(data$Location, levels = unique_locations))])  # color by location with transparency

# Add a legend without a border
legend("bottomleft", 
       legend = unique_locations,  # Use the unique locations
       col = colors,  # Ensure the correct color mapping
       pch = 19,  # same symbol as in the plot
       bty = "n",  # no box around the legend
       pt.cex = 1.5)  # match point size in the legend

# Add faint gridlines
grid(col = "gray90", lty = "dotted")
```
```{r scatterplot, echo=FALSE, fig.cap="Viable bacterial counts (VBC) vs Temperature. A strong negative correlation is observed. Points are colour coded according to location (see key).", out.width="100%"}
knitr::include_graphics("img/05/scatterplot.png")
```
Create a new script file in your existing project, copy/paste the code above and then run as source. Hopefully, you should be able to recreate the scatterplot in your plots window. The first three lines of code are the same as for our boxplot example. Let's break down the rest comment-by-comment:

1. **Define Unique Locations and Colors**:

```{R eval=FALSE}
unique_locations <- sort(unique(data$Location))
colors <- rainbow(length(unique_locations), alpha = 0.8)  # Or use your predefined colors
```

  - **Purpose**: to extract the unique production site locations from the data, sort them alphabetically, and assign each location a distinct colour for visualisation.
  - **Explanation**:
    - `unique(data$Location)`: finds all distinct location values (e.g., Amsterdam, Groningen, etc ...).
    - `sort()`: ensures the locations are listed in alphabetical order, which will also affect how they are represented in the legend.
    - `rainbow(length(unique), alpha =0.8)`: assigns a unique color to each location, creating a colour palette based on the number of unique locations. The `alpha = 0.8` parameter makes the points slightly transparent.
    
2. **Create a Scatterplot of VBC vs. Temperature, coloured by Location**:
```{R eval=FALSE}
plot(data$Temperature, data$vbc,
     main = "",
     xlab = "Temperature (°C)",
     ylab = expression(VBC ~ "(" ~ x10^7 ~ cfu/g ~ ")"),
     pch = 19,  # solid circles for points
     cex = 1.5,  # increase point size
     col = colors[as.numeric(factor(data$Location, levels = unique_locations))])

```
  - **Purpose**: To create a scatterplot that visualises the relationship between temperature and VBC, with points coloured according to the location of the data.
  - **Explanation**:
    - `plot(data$Temperature, data$vbc)` creates a basic scatterplot of VBC values (on the y-axis) versus temperature (on the x-axis).
    - `xlab` and `ylab` provide the axis labels.
    - `pch = 19` specifies solid circular points for the scatterplot.
    - `cex = 1.5` increases the size of points for better visibility.
    - `factor(data$Location, levels = unique_locations)` ensures that the points are coloured by location in the correct, alphabetically sorted order. The `as.numeric()` converts the factor levels into numeric indices to match with the `colors` vector.
3. **Add a Legend**:
```{R eval=FALSE}
legend("bottomleft", 
       legend = unique_locations,  # Use the unique locations
       col = colors,  # Ensure the correct color mapping
       pch = 19,  # same symbol as in the plot
       bty = "n",  # no box around the legend
       pt.cex = 1.5)
```
  - **Purpose**: To add a legend to the plot that identifies the colours representing each location, ensuring the viewer can easily distinguish between the locations.
  - **Explanation**:
    - `bottom-left` determines the location of the legend on your plot.
    - `legend = unique_locations` ensures the correct, alphabetically sorted location names are shown in the legend.
    - `col = colors` maps the same colours used in the plot to the corresponding locations in the legend.
    - `pch = 19` matches the symbol style of the points in the plot with those in the legend.
    - `bty = "n"` removes the default box around the legend to keep the visual clean.
    - `pt.cex = 1.5` adjusts the size of the symbols in the legend to match this size of the points in the scatterplot.
    
4. **Add Gridlines**:
```{R eval=FALSE}
grid(col = "gray90", lty = "dotted")
```
  - **Purpose**: Pretty self-explanitory this one I think. I like to have gridlines on my scatterplot as a guide to for the eye, but I leave it as optional for you.

<h3>Interpreting the Scatterplot</h3>

There's lots of information radiating out of this plot. Let's interpret what we're seeing:

  - **Overall Trend**: A clear negative correlation between temperature and VBC.
  - **Utrecht (purple)**: Highest temperatures (65 - 75<sup>o</sup>C) and lowest VBC values.
  - **Rotterdam (blue)**: Slightly lower VBC values for the same range of temperatures as Amsterdam and Groningen. This suggests that there may be another factor affecting VBC at Rotterdam.

After seeing this scatterplot I'd be advising the Utrecht site that they were "running hot!" and that they need to cool their compost by about 15<sup>o</sup>C in order to bring their VBCs back in line with the other sites. 

<h3>Ideas for Further Exploration</h3>

  - Scatterplot of VBC vs Moisture
  - Scatterplot of Moisture vs Temperature.

## Just for Fun

In this last sub-section I just wanted to show you something really cool. The code below uses the fantastlic **Plotly** library to generate a 3D scatterplot so that I can see the impact of both Temperature and Moisture on VBC at the same time. Plotly is not included in your RStudio by default, you'll need to install it first. Go to your **Console** window and type:
```{R, eval=FALSE}
install.packages("plotly")
```
Hit enter and then wait while the Plotly package is installed. Then start a new script file, copy/past the script below and run it as source.

```{R, warning=FALSE, message=FALSE, fig.cap="A 3D scatterplot showing Viable bacterial counts (VBC) vs both Temperature AND Moisture using the fantastic Plotly package.",  fig.label="fig:plotly"}
# Include the plotly library in your environment
library(plotly)

# Load the data
data <- read.csv("compost_999.csv")

# Calculate the conversion factor for cfu per gram
factor <- 2e5 / 1e7
data$vbc <- data$Viable.counts * factor

# Create an interactive 3D scatterplot
fig <- plot_ly(data = data, 
               x = ~Temperature, 
               y = ~Moisture,
               z = ~vbc, 

               color = ~Location,
               colors = "Set2",  # Choose a color palette
               type = 'scatter3d',
               mode = 'markers',
               marker = list(size = 5)) %>%
  layout(scene = list(xaxis = list(title = 'Temperature (°C)'),
                      zaxis = list(title = 'VBC'),
                      yaxis = list(title = 'Moisture (%)')))

# Show the figure
fig
```

How cool is that? You have an interactive, 3D graph that you can fiddle with and investigate the relationship between VBC, Temperature and Moisture. 

## Complete your Weekly Assignments

In the BIOS103 Canvas course you will find this week's **summative** QS assignments. There is no **formative** assignment this week! You should aim to complete the summative assignment before the end of the online workshop that corresponds to this section's content.

 + You can attempt the **summative** assignment **only once**. This assignment **will** contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days.
   
In **ALL** cases, when you click the button to "begin" a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit.

<!--chapter:end:05.Rmd-->

# Statistics in R: Part I

If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover **all of this section's content** and complete this week's **formative and summative assessments** in the BIOS103 Canvas Course.

## One-Way ANOVA in R

This section is almost a complete re-run of section \@ref(analysis-of-variance-anova). Except what we did in Excel, we'll learn to do in R. 

### Importing Data from an Online Source

```{r eval=TRUE}
zebrafish_data <- read.csv("https://raw.githubusercontent.com/rtreharne/qs/refs/heads/main/data/02/zebrafish_999.csv")

zebrafish_data$conc_pc <- as.factor(zebrafish_data$conc_pc)

head(zebrafish_data)
```
```{R eval=True}
# Calculate summary statistics for each group
zebrafish_summary <- aggregate(length_micron ~ conc_pc, zebrafish_data, function(x) 
  c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x)))

# Convert the list to data frame
zebrafish_summary <- do.call(data.frame, zebrafish_summary)

print(zebrafish_summary)
```

### Conducting a One-Way ANOVA

```{R eval=True}
# Perform a One-Way ANOVA (OMG).
zebrafish_aov <- aov(length_micron ~ conc_pc, data = zebrafish_data)

# Get the key results.
summary(zebrafish_aov)
```
### Post-Hoc Tests

```{R eval=True}
tukey_result <- TukeyHSD(zebrafish_aov)

print(tukey_result)
```

### Visualising the Data

```{R eval=True}

boxplot(
  length_micron ~ conc_pc, data = zebrafish_data,
  main = "",
  xlab = "Alcohol Conc. (%)",
  ylab = "Embryo Length (microns)",
  col = "lightblue"
  
)
```

## Two-way ANOVA in R

### Importing and Cleaning the Data

```{R eval=True}
activity_data <- read.csv("https://raw.githubusercontent.com/rtreharne/qs/main/data/08/activity_999.csv")

head(activity_data)
```
```{R eval=TRUE}
activity_data$ph <- as.factor(activity_data$ph)
activity_data$temperature <- as.factor(activity_data$temperature)

activity_data$activity <- as.numeric(activity_data$activity) 
#activity_data <- na.omit(activity_data)

activity_data <- activity_data[activity_data$activity >= 0,]

head(activity_data)
```
```{R eval=True}
# anova_result
anova_result <- aov(activity ~ ph * temperature, data = activity_data)

summary(anova_result)
```
```{R eval=True}
# Perform Tukey's HSD test
tukey_result <- TukeyHSD(anova_result)

tukey_df <- as.data.frame(tukey_result[["ph:temperature"]])

max(abs(tukey_df$diff))
```
### Performing a Two-Way ANOVA
### Conducting Post-Hoc Tests with Tukey's HSD
### Visualising the Data




<!--chapter:end:08.Rmd-->

---
title: "Quantiative Skills in Biosciences I"
author: "R. E. Treharne"
date: "2024-10-07"
output: html_document
---

# Appendix: Teach yourself R. {.unnumbered #appendix}

## LinkedIn Learning {.unnumbered}

Can you believe it!? As a student at the University of Liverpool you have free and full access to the [LinkedIn Learning platform](https://www.linkedin.com/learning). Login using your UoL credentials. It's completely choc-o-bloc with coding resources and you often get a neat little certificate every time you complete a course. 

+ **Complete guide to R: Wrangling, Visualizing, and Modeling Data**. Barton Poulson
+ **Data Wrangling in R**. Mike Chapple
+ **R for Data Science: Analysis and Visualization**. Barton Poulson
+ **Coding Exercises: R Data Science**. Mark Niemann-Ross
+ **Complete Your First Project in R**. Megan Silvey
+ **R for Data Science: Lunch Break Lessons**. Mark Neimann-Ross

## YouTube {.unnumbered}

Ah, YouTube. How I love you. But not shorts. Shorts are evil and rot your brain. Here are some great YouTube playlists that will introduce you to R.

+ [Introduction to R](https://www.youtube.com/playlist?list=PLiC1doDIe9rDjk9tSOIUZJU4s5NpEyYtE). DataDaft
+ [Statistics and Statistics with R Tutorials](https://www.youtube.com/playlist?list=PLqzoL9-eJTNBDdKgJgJzaQcY6OXmsXAHU). MarinStatsLectures
+ [R tutorial - Learn R Programming](https://www.youtube.com/playlist?list=PLjgj6kdf_snYBkIsWQYcYtUZiDpam7ygg). DataCamp
+ [Introduction to R Programming](https://www.youtube.com/playlist?list=PL8eNk_zTBST8j2BU5HYFQogdCjtrHyQAx). Data Science Dojo


## Books {.unnumbered}

Do people read books anymore? Yes they do. Here are 5 titles that are available to you in the [University of Liverpool](https://libguides.liverpool.ac.uk/library/) library right now.

+ **Hands-on programming with R**. Garret Grolemund
+ **R in action: data analysis and graphics with R and Tidyverse**. Robert Kabacoff
+ **R for Data Science**. Christopher Lortie
+ **Introduction to Statistics Using R**. Mustapha Akinkunmi
+ **An Introduction to Data Analysis in R**: Hands-on Coding, Data Mining and Statistics from Scratch. Zamora Alfonso *et. al*

<!--chapter:end:appendix_I.Rmd-->

