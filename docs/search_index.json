[["index.html", "Quantiative Skills in Biosciences I Cover", " Quantiative Skills in Biosciences I R. E. Treharne 2024-10-07 Cover "],["introduction.html", "Introduction", " Introduction This book accompanies the Quantitative Skills (QS) component of the BIOS103 - Introductory Practical Skills in Biosciences I course. If you are participating in the weekly timetabled QS workshops associated with this course then be sure to submit your weekly summative QS assignments via Canvas by the deadline specified for each workshop. All QS workshops will be delivered online via Teams. Access the meeting link from the BIOS103 Canvas course. "],["introduction-to-excel-and-r.html", "1 Introduction to Excel and R 1.1 Estimating the Volume of a Snail 1.2 Getting Ready for R 1.3 Complete your Weekly Assignments", " 1 Introduction to Excel and R If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. 1.1 Estimating the Volume of a Snail In this section, you will learn how to import data from a CSV file into Excel, perform basic calculations, create a scatterplot including a linear trendline to make predictions. The primary objective is to estimate the volume of a snail based on its mass using a provided dataset. Google Sheets Alternative If you like, you can do everything in the above video using Google Sheets instead! Here’s my alternate video just for Google Sheets. 1.1.1 Download and Import the CSV File Download the CSV File: Here is an example dataset. Download it to your local machine. If that link doesn’t you can get the dataset in your browser here. Right click and Save as. Import into Excel: Open Excel (Use the desktop version - you won’t be able to do this using the online version!). Go to Data &gt; From Text/CSV and select the downloaded CSV file. When the import wizard appears, click Load. 1.1.2 Calculate Volume \\(V\\) in Excel We will estimate the volume \\(V\\) of our snails using the formula for the volume of a sphere: \\[ V = \\frac{4}{3} \\pi r^3 \\] where \\(r\\) is the radius, which we assume is equal to half the Height L (mm) column. Add a New Column for Volume \\(V\\): In the third column, label it as Volume V (mm^3). In the first cell of this column, use the formula: = (4/3) * PI() * ((B2/2)^3) Drag the formula down to apply it to all rows. 1.1.3 Add a Linear Trendline and Equation Create a Scatter Plot: Select the Mass M (g) and Volume V (mm^3) columns. Go to the Insert tab and select Scatter Plot. Add Trendline: Click on the plot Click the green + icon that appears at the top right of the plot. Click the &gt; symbol on the Trendline option and click More options… From the trendline options menu that appears on the right, select the Linear trendline and check the box isplay Equation on chart. Right-click on a data point in the scatter plot. Interpret the Equation: The trendline equation will appear on the chart in the form of \\(y = ax + b\\), where: \\(y\\) is the volume. \\(x\\) is the mass. \\(a\\) and \\(b\\) are coefficients. For the Example dataset: a = 1341.7 \\(mm^3.g^{-1}\\) b = 1140.2 \\(mm^3\\) 1.1.4 Estimation of Volume for a Snail with Mass 10g Use the Trendline Equation: Substitute \\(x = 10\\) into the trendline equation to calculate the estimated volume \\(V\\). Express the volume in \\(cm^3\\) to one decimal place (Note: \\(1 cm^3 = 1000 mm^3\\)). For the example dataset: The estimated volume of a snail that is \\(10 g\\) is \\(14.6 cm^3\\). 1.2 Getting Ready for R Over the next couple of weeks you will continue to use Excel to load, manipulate, analyse and visualise data. Beyond this you will be using the coding language R exclusively. To prepare for this, you need to download, install and configure R and RStudio today. Chromebook Users I love a chromebook. Sadly, installing R and RStudio on one involves a bit of extra work compared to Windows and Mac. This YouTube video seems to have all the bases covered: “How to Install RStudio on a Chromebook” 1.2.1 Download and Install (Windows and Mac only) R and RStudio are actually separate things, although they are often mentioned together. R is a programming language and software environment specifically designed for statistical computing and data visualisation. Other examples of programming languages include Python, Java and Ruby. RStudio is the integrated development environment (IDE) for R. It provides a user-friendy interface that will allow you to write all your R scripts and compile them to do stuff. I’m using it to write this handbook right now! Despite their differences, you might hear the terms R and RStudio used interchangeably, as RStudio serves as the primary interface through which users interact with the R programming language. You need to download and install both R and Rstudio. Install R first from the Comprehensive R Archive Network (CRAN) Then install RStudio from the RStudio website Once both are installed, open up RStudio and get ready to create your first R Project. All University of Liverpool MWS machines already have R and RStudio installed and ready to use. 1.2.2 Creating Your First R Project in RStudio Follow these steps to set up and manage your first R project in RStudio: Open RStudio Launch RStudio from your applications menu. Start a New Project Click on the File menu at the top of the RStudio window. Select New Project… from the dropdown menu. Choose Project Type You will be prompted with three options: New Directory: Create a new project in a new directory. Existing Directory: Use an existing directory as the project’s folder. Version Control: Clone a project from a version control repository (e.g., GitHub). For your first project, select New Directory. Select Project Template Choose Empty Project. Click Next. Set Up Project Directory Directory name: Enter a name for your project folder. This will be the name of the directory created for your project. Subdirectory of: Choose the parent directory where the new project folder will be created. You can navigate to the desired location using the file browser. Click Create Project. RStudio Project Interface Once the project is created, you will see a new RStudio window or tab with the following components: Files pane: Displays the files and folders in your project directory. Script editor: Where you write and edit your R scripts. Console: Where you can directly enter and execute R commands. Environment/History: Shows your workspace objects and command history. Plots/Packages/Help/Viewer: Various tabs for viewing plots, managing packages, accessing help documentation, and viewing other outputs. Create and Save an R Script Click File &gt; New File &gt; R Script. Write some R code in the script editor. For example: To run your print command, click on the line and click the Run button at the top right of your script editor window or press Ctrl + ENTER (Cmd + Enter on Mac). You will see the following output in your console window: ## [1] &quot;Hello World!&quot; And there it is! You’ve just successfully compiled your first line of R. Congratulations! 1.2.3 Something More Complicated As you delve deeper into R programming, you’ll find that your scripts become more sophisticated than the “Hello World” example above. In the following example, I’ll walk you through a script to create a random number generator. Here’s the script in its entirety: # create a variable called &quot;seed&quot; seed &lt;- 999 # set the seed set.seed(seed) # generate a random number random_number &lt;- runif(1) # print the random number print(random_number) Cut and paste these lines into the script file we were working on earlier (overwrite the “Hello World” example). Now, you can run each line in turn as before (using Ctrl + ENTER) or you can run everything by clicking the Source button. You should see the following number appear in your console: Let’s break down each line of the script to understand its purpose and functionality. Creating a Variable Called “seed” # create a variable called &quot;seed&quot; seed &lt;- 123 seed &lt;- 123: Here, we are using the &lt;- operator to assign the value 123 to the variable named seed. In R, variables are used to store data that can be reused or manipulated later in the script. The number 123 is arbitrary in this case, but we use it to illustrate how to set a seed for random number generation. Setting the Seed # set the seed set.seed(seed) The set.seed() function initializes the R environment’s build in random number generator with the value stored in seed. Setting a seed is essential for reproducibility, meaning that if someone else runs this code with the same seed, they will get the same random number output in their console. This is particularly useful in simulations and randomised experiments where consistent results are needed. Generating a Random Number # generate a random number random_number &lt;- runif(1) random_number &lt;- runif(1): This line generates a single random number between 0 and 1. The function runif() generates random numbers from a uniform distribution, which means that each number within the specified range has an equal probability of being selected. The 1 inside the parentheses specifies that only one random number should be generated. The resulting number is then assigned to the variable called random_number. Printing the Random Number # print the random number print(random_number) print(random_number): The print() function outputs the value stored in random_number to the console. This is useful for verifying the output of your code and ensuring that the operations have been executed correctly. Summary This example script demonstrates a more complex task than the basic “Hello World” script. It introduces key concepts like variable assignment with &lt;-, setting a seed for reproducibility using set.seed(), generating random numbers with runif(), and printing results using print(). As you continue learning R, these foundational concepts will become increasingly important, enabling you to build more advanced and meaningful analyses. Remember, comments (#) are your friends! They help explain what each part of your code does, making it easier for you and others to understand and maintain your scripts. Be liberal with your comments. You’ll thank yourself later (trust me). Give me feedback I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 1.3 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["summarising-data-and-anova-in-excel.html", "2 Summarising Data and ANOVA in Excel 2.1 Summarising Data 2.2 Analysis of Variance (ANOVA) 2.3 Complete your Weekly Assignments", " 2 Summarising Data and ANOVA in Excel If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. In this section we will only be using Excel. No R today! Specifically, you will be developing two important skills: Summarising Data. Constructing and testing hypotheses. The ultimate aim is to gain insight and learn something new about the world from the data that we have painstaking measured in our well designed lab experiments. This is a cornerstone of what being a scientist is all about. 2.1 Summarising Data Raw data is beautiful, but messy. Showing another person your raw data and expecting them to immediately understand it, no matter how proud you are of the toil expended to generate the data, is an unrealistic expectation. You need to boil your data down into something that another person can grasp instantaneously. Let’s take a look at a Zebrafish dataset from an experiment that is uncannily similar to the one you performed in your lab practical this week. 2.1.1 Download and Import the CSV File Download the CSV File: Here is an example dataset. Download it to your local machine. Import into Excel: Open Excel (Use the desktop version - you won’t be able to do this using the online version!). Go to Data &gt; From Text/CSV and select the downloaded CSV file. When the import wizard appears, click Load. You should now see something like that in Figure 2.1. There are 3 columns: ID - A unique number to identify a measurement. conc_pc - The ethanol concentration (%) that the each embryo was treated with. length_micron - The measured lengths, in \\(\\mu m\\) of the embryos. We call this format, in which each row corresponds to a single measurement, a long format. Figure 2.1: You should see this (or something like it) after you have imported your date into Excel. 2.1.2 Generating a Summary Table Excel Alternative If you are having trouble accessing the Desktop version of Excel then here is an alternative video. “Summary Table with Google Sheets” Identify your Groups Click anywhere in your table. Select the Data menu and click the Advanced icon in the Sort &amp; Filter section. This will bring up a window called “Advanced Filter”. Select the Copy to another location action. Your list range should already be set to $B:$B, but if not make it so. Set the Copy to cell to E1 Make sure the Unique records only check box is selected and click OK. You should now see a complete list of your alcohol concentration groups in a column with a header conc_pc. Make this into a new table by clicking on any of the concentration values, and then Insert &gt; New table &gt; OK. Nice. Now you’re ready to start building out your summary table horizontally. Let’s start with calculating the mean Zebrafish length for each group. Right now, your spreadsheet should look roughly the same as the screenshot in figure 2.2 Figure 2.2: Constructing a summary table Calculating a Mean Column Create a new column in your summary table by typing the word Mean in cell F1. Calculate the mean of the Zebrafish lengths for the control group (0% alcohol concentration) by entering the following formula into cell F2. =AVERAGE(IF($B1:$B$161=$E2,$C1:$C$161)) A Deeper Explanation The formula =AVERAGE(IF($B$2:$B$161=$E2,$C$2:$C$161)) is an array formula that calculates the average length of Zebrafish for a specific group based on the concentration of alcohol. It’s a bit of a beast isn’t it? Let’s break it down. $B$2:$B$161: The $ symbols before both the column letter B and the row numbers 2 and 161 lock the entire range. This means that when you copy the formula to other cells, this range will not change; it will always refer to cells B2 to B161. $E2: The $ before the column letter E locks the column, but since there’s no $ before the row number 2, the row number can change if the formula is dragged down across rows. This cell is used to compare each value in the range $B$2:$B$161 to the specific concentration value in the corresponding row in column E. $C$2:$C$161: Similar to the range for column B, this locks the range of cells in column C from which the values will be averaged, conditional on the IF statement. AVERAGE(IF(…)): The IF function checks each row in the range $B$2:$B$161 to see if it matches the value in the corresponding row in column E. If it matches, the corresponding value in column $C$2:$C$161 is included in the average calculation. The AVERAGE function then calculates the mean of these filtered values. This approach is particularly useful when you want to calculate conditional averages across a dataset, ensuring that the correct cells are referenced even when copying the formula to different parts of the spreadsheet. Calculating More Columns Create four more columns with headers: Std. Dev. Median Min Max Drag the cell F2 to G2. Change the word AVERAGE in the formula in G2 to STDEV. This will calculate the standard deviation for the group and the remaining cells in the column should also auto complete. Do the same for the median, min and max columns. Be sure to use the corresponding function. A Deeper Explanation When analysing data, it’s important to understand the basic statistical measures that summarise the data’s distribution. Here are some key terms: Mean: The mean, often referred to as the average, is the sum of all values in a dataset divided by the number of values. It provides a central value for the data. However, the mean can be influenced by outliers (extremely high or low values). Standard Deviation: The standard deviation measures the amount of variation or dispersion in a dataset. It is calculated as the square root of the variance, where variance is the average of the squared differences between each data point and the mean. A low standard deviation indicates that the data points tend to be close to the mean, while a high standard deviation indicates more spread out data. Median: The median is the middle value in a dataset when the values are arranged in ascending or descending order. If the dataset has an odd number of values, the median is the central value. If the dataset has an even number of values, the median is the average of the two central values. The median is less affected by outliers compared to the mean. Min: The minimum (min) value is the smallest value in the dataset. It provides a measure of the lower bound of the data. Max: The maximum (max) value is the largest value in the dataset. It provides a measure of the upper bound of the data. These measures are fundamental for understanding the distribution of data. The mean and median give you central tendencies, while the standard deviation tells you how spread out the data is. The minimum and maximum values provide the range within which all the data points fall. 2.1.3 Presenting Your Summary Table At some point you may wish to include your Excel summary table in a Word document. There’s a lot of wiggle room on how you choose to format your table but there are a few unbreakable rules: The table MUST have a caption. The caption should be placed ABOVE the table (not below as for a figure or graph). The caption should be numbered accordingly. For example, if this is the first table in your document the figure caption should start “Figure 1: …”. The caption should be descriptive and unambiguous. The reader should be able to quickly interpret what is going on without having to read the body of the text. Any symbols or variables or units should be defined in the caption. The data should be formatted to a sensible number of decimal places (i.e. if you’re measurements are made to 1 decimal place, your summary values should not be quoted to more than this). Follow these rules and you can’t go wrong. Figure 2.3 shows how my summary table looks when copied and pasted into Word. I like to make my tables span the entire width of my document using the Auto-fit to window command. I also like to center my columns. These are personal preferences, but you can’t deny they look great! Figure 2.3: Formatting a summary table in Microsoft Word. 2.2 Analysis of Variance (ANOVA) You’re about to learn a critical skill that is important for becoming a scientist: formulating hypotheses and testing them. This is a cornerstone of scientific inquiry. You’ve already summarised your data using descriptive statistics. Now, we’ll move on to another branch of statistics called inferential statistics. This involves using an appropriate statistical test to determine whether specific hypotheses that we construct should be accepted or rejected. Knowing which statistical test to use depends on the data and context. It takes time and lots of practice to become proficient at this, and it’s completely normal to forget which test you need or how to interpret the result. We’ll stick with out Zebrafish dataset and perform an Analysis of Variance (ANOVA) to determine whether there is something we can learn from our data. We’ll formulate a hypothesis and use Excel to perform the ANOVA. Then we’ll interpret the results. But before we dive into this, let’s create a boxplot to visually inspect the data and see if we can generate some gut intuition as to what might be going on. 2.2.1 Grouped Boxplots in Excel You might not have seen a grouped boxplot before. That’s OK. I’m confident that you’ll have a good intuition for what they show. However, for more information the key components of a boxplot read the text in the Anatomy of a Boxplot section. Let’s dive straight in for now though. Create the Boxplot Excel Alternative If you are having trouble accessing the Desktop version of Excel then here is an alternative video. “Boxplots with Excel Online” Insert a Boxplot: Select your data. Go to the Insert tab, click on Insert Statistical Chart, and choose Box and Whisker. Select Data: Your plot will look a bit weird. That’s because we need to configure the groupings properly. Click the Select data button. Remove the conc_pc series from the left-hand list. Click the Edit button on the (currently empty) right-hand list. Boxplots in the wrong order? Sort the conc_pc column from smallest to largest. Re-scale y-axis: It’s best to re-scale the y-axis to maximise the space used by the boxplots. This will make any effect easier to see. Double-click on the numbers in the y-axis. Set the Bounds &gt; Minimum: to 1000. Add Axis Labels: Click the green + icon in the top-right of your graph. Check the Axes titles box. Double-click on each label in turn and update with appropriate labels: X-axis: “Alcohol Conc. (%)” Y-axis: “Embryo Length (\\(\\mu m\\)) Note: To use the \\(\\mu\\) symbol click Insert &gt; Symbols &gt; Symbol. Find the symbol in the list and click Insert. Get Rid of Chart Title: If you’re going to be presenting this figure in a report or poster then it should not have a title above the axes. Instead you should include a figure caption below the plot. The same unbreakable rules for your caption are the same as those described above for table captions. Just make sure your caption is below the figure instead of above. Export Your Figure: Right click on your figure anywhere outside the plot area and you should see the option to Save as picture. Save it somewhere sensible as a .png file and then insert it into a Word document with a sensible caption. Figure 2.4: Distribution of Zebrafish embryo lengths organised by Alcohol treatments. Interpret the Boxplot Figure 2.4 shows our finished boxplot. Alcohol Concentration 0%: The median embryo length is approximately 2500 µm, with the mean slightly above the median. The data is relatively spread out, as shown by the wide box and long whiskers. There are no outliers in this group. Alcohol Concentration 1.5%: The median length is slightly lower than the 0% group, but the mean is still fairly close. The box and whiskers are narrower than in the 0% group, indicating less variability in embryo length. One outlier is present above the whisker, indicating a particularly large embryo in this concentration or possibly, a random measurement error. Alcohol Concentration 2%: The median and mean have both decreased, showing a reduction in embryo length as alcohol concentration increases. The box is narrower, indicating less variability, but there are several outliers both above and below the whiskers, suggesting that while most embryos were of a similar size, a few were much larger or smaller. Alcohol Concentration 2.5%: The median and mean lengths have further decreased, indicating a continued negative effect of alcohol concentration on embryo length. The box is of similar size to the 2% group, but with longer whiskers, indicating more variability in the data. There is one outlier below the whisker, indicating a particularly small embryo in this concentration. In summary, As alcohol concentration increases, the median and mean embryo lengths decrease, indicating a negative correlation between alcohol concentration and embryo length. Variability in embryo length decreases slightly up to 2% concentration but then increases again at 2.5%, as evidenced by the longer whiskers. The presence of outliers, particularly at higher concentrations, suggests that while most embryos are affected similarly, some experience more extreme changes in size. However, visual patterns alone cannot confirm the existence of a true relationship between alcohol concentration and embryo length. To rigorously explore whether the observed trends are statistically significant or merely due to chance, we must construct testable hypotheses. This process will allow us to formalise our observations and set the stage for appropriate statistical analysis. Anatomy of a Boxplot A boxplot is a standardised way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. The box in the boxplot represents the interquartile range (IQR), which is the range between Q1 and Q3. The central line within the box indicates the median, which is the middle value of the dataset. Sometimes, an “X” symbol is also included within the box, representing the mean of the dataset. However, it is important to note that the mean is not always shown in a boxplot. The “whiskers” of the boxplot extend from the box to the smallest and largest values within 1.5 times the IQR from the first and third quartiles, respectively. These whiskers help to indicate the spread of the majority of the data. Outliers are data points that fall outside the range defined by the whiskers. These are typically plotted as individual points beyond the ends of the whiskers, highlighting data points that are unusually high or low compared to the rest of the dataset. To determine whether a data point is an outlier, you compare it to the thresholds defined by the IQR: Any data point below Q1 - 1.5 * IQR is considered a lower outlier. Any data point above Q3 + 1.5 * IQR is considered an upper outlier. Summary: Box: Represents the interquartile range (IQR), the middle 50% of the data. Central Line: Indicates the median value. X (if shown): Indicates the mean value. Whiskers: Extend to the smallest and largest values within 1.5 times the IQR from the quartiles. Outliers: Data points that lie outside the whiskers, typically displayed as individual points. 2.2.2 Constructing Testable Hypotheses Given the patterns observed in the boxplot, where higher alcohol concentrations appear to be associated with shorter embryo lengths, it is essential to move from visual interpretation to a more rigorous statistical approach. This involves constructing and testing hypotheses to determine whether the observed trends are statistically significant. Formulating the Hypotheses In the context of your data, the primary goal is to determine whether different alcohol concentrations have a statistically significant effect on embryo length. To do this, we formulate a null hypothesis (\\(H_{0}\\)) and an alternative hypothesis (\\(H_{1}\\)): Null Hypothesis (\\(H_{0}\\)): There is no statistically significant difference in the mean embryo lengths between the different alcohol concentration groups. Any observed differences are attributed to random variation rather than an effect of alcohol concentration. \\[ H_0: \\bar{x}_0 = \\bar{x}_{1.5} = \\bar{x}_2 = \\bar{x}_{2.5} \\] Here, \\(\\bar{x}_0\\), \\(\\bar{x}_{1.5}\\), \\(\\bar{x}_2\\), and \\(\\bar{x}_{2.5}\\) represent the mean embryo lengths at 0%, 1.5%, 2%, and 2.5% alcohol concentrations, respectively. Alternative Hypothesis (\\(H_{1}\\)): At least one of the mean embryo lengths differs significantly from the others, suggesting that alcohol concentration has a measurable impact on embryo length. \\[ H_1: \\text{At least one } \\bar{x} \\text{ is different} \\] The Origin of Hypothesis Testing Hypothesis testing emerged in the early 20th century through the work of statisticians like Ronald A. Fisher, who applied these methods to agricultural experiments, leading to significant advancements in statistical methodology. However, Fisher’s legacy is also marred by his support for eugenics, reflecting the darker intersections of early statistical science with discriminatory ideologies. While hypothesis testing remains central to scientific research, the use of p-values has faced criticism. P-values, often misinterpreted, simply measure data compatibility with the null hypothesis, not the truth of the hypothesis itself. The conventional threshold (p &lt; 0.05) can lead to arbitrary decisions, and practices like p-hacking undermine the validity of results. Alternatives and complements to p-values include confidence intervals (which offer a range of likely values for parameters), Bayesian methods (which incorporate prior knowledge into probability assessments), and effect sizes (which quantify the magnitude of an effect). Understanding these tools and their limitations enables researchers to draw more nuanced and reliable conclusions from their data. Getting Ready to Test A One-way ANOVA is a statistical test used to determine whether there are significant differences between the means of three or more independent (unrelated) groups. In this case, the groups are the different levels of alcohol concentration (0%, 1.5%, 2%, and 2.5%). The term “one-way” refers to the fact that we are analysing the effect of a single factor (alcohol concentration) on the dependent variable (embryo length). If we were interested in analysing the impact of an additional dependent variables (e.g. incubation temperature), as well as any interaction effects with alcohol concentration, we would likely want to perform a “two-way” ANOVA. We are using ANOVA in this context because: Multiple Group Comparisons: We have more than two groups (four alcohol concentration levels), and ANOVA is designed to handle comparisons across multiple groups simultaneously. This is more efficient and statistically sound than performing multiple t-tests, which would increase the likelihood of Type I errors (false positives). Assessing Variability: ANOVA compares the variability within each group (i.e., how much embryo lengths vary within each alcohol concentration) to the variability between groups (i.e., how much the group means differ from each other). This helps us determine if the observed differences in means are greater than what we would expect by chance alone. Assumptions of ANOVA For ANOVA to be valid, certain assumptions must be met: Independence of Observations: The data points in each group are independent of each other. In other words, each data point corresponds to a unique embryo. Normality: The distribution of the residuals (differences between observed and predicted values) should be approximately normal. Homogeneity of Variances: The variances within each group should be roughly equal. In this case, I have engineered the dataset so that these assumptions are met. However, in practice, when working with real data, it is crucial to check whether these assumptions hold before applying ANOVA. If the assumptions are violated, the results of the ANOVA may not be reliable, and alternative methods may be necessary. I’ll show you how to check that the assumptions for a statistical test are met in Chapter 5 and the available alternative tests for cases where the assumptions are not met. 2.2.3 Performing a One-Way Anova in Excel Excel Alternative If you are having trouble accessing the Desktop version of Excel then here is an alternative video. “ANOVA with Google Sheets” Please be reminded that the guidance in the video above, and in the text below, is only relevant if you are using the desktop version of Excel on a Windows machine. It might work on a Mac - there’s no guarantee, and it won’t be relevant if you are using Google Sheets. If you can’t do the following with your personal device then please use one of the University machines. Step 1: Install and Activate the Data Analysis ToolPak To perform a One-Way ANOVA in Excel, you need to install and activate the Analysis ToolPak add-on. Open Excel and go to File &gt; Options. In the Options menu, select Add-ins. At the bottom, next to “Manage”, ensure Excel Add-ins is selected and click Go. In the Add-Ins box, check the Analysis ToolPak option and click OK. You should now see a Data Analysis button in the Data ribbon. Step 2: Create a Pivot Table to Reshape Data Before performing the ANOVA, you need to reshape your long data into a wide format using a pivot table. Here’s how to do it: Select your dataset (including headers). Go to the Insert tab and click on PivotTable. In the Create PivotTable dialog box, select where you want the PivotTable to be placed (e.g., New Worksheet). In the PivotTable Fields pane: Drag conc_pc to the Columns area. Drag id to the Rows area. Drag length_micron to the Values area. The resulting pivot table will have the concentration levels as columns and the measurements as rows, which is the required format for One-Way ANOVA. Step 3: Perform the One-Way ANOVA Click on the Data ribbon and select Data Analysis. In the Data Analysis dialog, select ANOVA: Single Factor and click OK. In the ANOVA: Single Factor dialog box: Input Range: Select the range of your reshaped data (including the labels). Grouped By: Choose Columns (since each group is in a separate column). Labels in First Row: Check this option if you included labels. Alpha: Set this to 0.05 (the default significance level). Output Options: Choose where you want to display the results (e.g., New Worksheet Ply). You can name the new sheet “Results”. Click OK to run the analysis. You should see a new sheet that looks identical to that shown in 2.5. Figure 2.5: Results tables of one-way ANOVA in Excel Step 4: Interpret the Results Excel will generate a new worksheet with the ANOVA results. The output includes two tables: Summary Table: Count: Number of observations in each group. Sum: Sum of all values in each group. Average: Mean value of each group. Variance: Variability within each group. ANOVA Table: Source of Variation: Between Groups: Variability between the groups. Within Groups: Variability within each group. SS (Sum of Squares): Measure of the total variation. df (Degrees of Freedom): Calculated as the number of groups minus 1 for between groups, and total observations minus the number of groups for within groups. MS (Mean Square): SS divided by df. F (F-Statistic): Ratio of MS between groups to MS within groups. P-Value: Indicates if the results are statistically significant. F crit: Critical value of F for the given alpha level. Step 5: Make a Conclusion Compare the P-Value to the alpha level (0.05): If P-Value ≤ 0.05, reject the null hypothesis and conclude that there is a significant difference between the groups. If P-Value &gt; 0.05, fail to reject the null hypothesis and conclude that there is no significant difference between the groups. In your ANOVA result, a p-value of 2.48E-08 means that the probability of observing the differences between your group means by random chance is exceedingly low (just 0.0000000248). Since this p-value is far below the common threshold of 0.05, we can reject the null hypothesis (\\(H_0\\)), and conclude that there are statistically significant differences between the groups. While the one-way ANOVA tells us that there are significant differences between the groups, it does not specify which groups differ from each other. To determine where these differences lie, additional testing, known as post-hoc testing, is required. Post-hoc tests, allow us to compare the group means directly and identify which specific groups are significantly different. Although post-hoc testing requires a bit more work, it can significantly enhance our understanding of the data and provide deeper insights into the relationships between groups. Understanding Exponent Notation In statistical analysis, particularly when working with software like Excel or R, you might values expressed in scientific notation, often using the letter “E” followed by a number. For example, the p-value 2.48E-08 appears in your ANOVA results. What Does 2.48E-08 Mean? The notation 2.48E-08 is Excel’s way of representing the number 2.48 × 10⁻⁸. Here’s how to break it down: 2.48: This is the base number. E-08: This indicates that the base number (2.48) should be multiplied by 10 raised to the power of -8. So, 2.48E-08 is mathematically equivalent to: \\[ 2.48 \\times 10^{-8} = 0.0000000248 \\] This value is extremely small, which is typical for p-values when the test results are highly significant. Why Use Scientific Notation? Scientific notation is used to conveniently express very large or very small numbers that would otherwise be cumbersome to write out in full. In the case of p-values, this format is particularly useful because significant results often involve very small numbers. Instead of writing 0.0000000248, which can be error-prone and hard to read, Excel uses 2.48E-08 to convey the same information succinctly. 2.2.4 Performing Post-hoc Tests in Excel Following a significant ANOVA, we need to perform additional tests to determine where the differences between the groups lie. These are called Post-hoc tests. Step 1: Create a new table to list group comparisons In cell A19, underneath your ANOVA result table, Create a new column label called Groups. List all the possible ways two groups can be compared to each other (there are six ways in total): 0% v 1.5% 0% v 2.0% 0% v 2.5% 1.5% v 2.0% 1.5% v 2.5% 2.0% v 2.5% Create additional column headers P-value and Significant? in cells B19 and C19 respectively. Step 2: Perform T-tests for each group comparison Starting in cell B20 type: =TTEST(wide!B$4:B$164, wide!C$4:C$164, 2, 2) Copy the cell down for the remaining rows and update the columns in the formula to correspond with the respective groups that are being compared. What is a T-test? Unlike ANOVA, which compares multiple groups simultaneously to see if there are any significant differences in the means of the groups, a T-test can only compare two groups at a time. The assumptions for a T-test are similar to those for ANOVA: the data should be normally distributed, the samples should be independent, and the variances of the two groups should be equal if using a two-sample T-test assuming equal variances. Like ANOVA, the key output of a T-test is the p-value. In Excel you perform a T-test like this: =T.TEST(list1, list2, tails, type) where: list1: Corresponds to the list of values in your first group. list2: Corresponds to the list of values in your second group. tails: Requires a value of 1 or 2 indicating whether the test is one-tailed or two-tailed respectively. A one-tailed T-test tests for a difference in a specific direction (greater or less), while a two-tailed T-test tests for any difference regardless of direction. type: Requires a value of 1, 2, or 3 indicating: 1: A paired t-test (i.e., groups are not independent). 2: An independent t-test with equal variances between groups. 3: An independent t-test with unequal variances between groups (also known as a Welch test). T-tests are powerful for pairwise comparisons but need to be used in conjunction with corrections like Bonferroni when multiple T-tests are conducted, as performing multiple tests increases the risk of Type I errors (false positives). Let’s create a new table underneath my ANOVA table in Excel, say starting in cell A19 Step 3: Calculate Bonferroni Corrected Alpha Level - To calculate the Bonferroni corrected alpha level divide the initial threshold value of 0.05 by the number of t-tests you are performing, i.e. 6. - Use the corrected value by comparing it to the p-value of each of the t-tests to determine if the difference in the means of groups is significant. Step 4: Extend Your Conclusions The post-hoc tests indicate that there are significant differences in embryo lengths were found between the following pairs of alcohol concentrations: 0% vs. 2% 0% vs. 2.5% 1.5% vs. 2% 1.5% vs. 2.5% These findings suggest that increases in alcohol concentration from 0% to 2%, and from 1.5% to 2.5%, lead to significant changes in embryo length. Feedback Please. I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 2.3 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["calibration-curves-and-linear-regression-in-excel.html", "3 Calibration Curves and Linear Regression in Excel 3.1 Calibration Curves 3.2 Linear Regression 3.3 Complete your Weekly Assignments", " 3 Calibration Curves and Linear Regression in Excel If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. In this section we’ll be re-visiting linear trendlines in Excel in the context of calibration curves and extending our knowledge by going deeper into the world of linear regression. We will be working on two important skills: Manipulating data (transposing) Making predictions from linear models. 3.1 Calibration Curves A calibration curve is a plot of a measurable quantity (in our case absorbance, as determined by spectrophotometry) against the concentration of known standards. This relationship, typically linear, allows for the determination of the concentration of unknown samples by interpolation. The Beer-Lambert Law provides the foundation for generating calibration curves in spectrophotometry. It describes the relationship between absorbance (A) and the concentration (C) of a substance in solution. The law is given by the equation: \\[ A = m \\cdot C \\] Where: A is the absorbance (a dimensionless quantity, i.e. no units!). m is the slope (related to molar absorptivity and path length). C is the protein concentration (in mg/mL). There are a few key considerations to make when using this law to determine the concentrations of unknown solutions: Linear range: the calibration curve is only valid within a certain concentration range. At very high concentrations, the relationship may no longer be linear (due to factors like light scattering). Extrapolating concentrations from our calibration curves that are beyond those of our known standards is very, very naughty. Don’t do it! Reproducibility: It is critical that the same instrument (including scan settings and wavelength) be use for both the standards and unknowns. Let’s take a look at an example dataset from an experiment designed to generate a calibration curve for hemoglobin. Assume that starting from a stock solution of concentration 1.5 mg/mL a set of seven protein solutions have been created using a 1:1 serial dilution and that the absorbance of each solution has been measured at a wavelength of 560nm (yellow-green light). Mac Users It might not be possible for you to transpose the data like in the video above. If you can’t, try this: Select the range of data you want to rearrange, including any row or column labels, and Cmd+C to copy. Select the first cell where you want to paste the data, and on the Home tab, click the arrow next to Paste, and then click Transpose. Watch this video if you’re still stuck. Download the CSV File: Here is an example dataset. Download it to your local machine. Import into Excel: Open Excel (Use the desktop version - you won’t be able to do this using the online version!). Go to Data &gt; From Text/CSV and select the downloaded CSV file. When the import wizard appears, do not click Load. There’s something not right about this dataset. It’s sideways! The columns of data run horizontally instead of vertically. We don’t like this, it makes it much harder to plot figures and perform any analysis. We need to transform the data somehow and flip it to vertical instead of horizontal. We need to transpose the data. Transpose the data: On the import wizard click the Transform Data button. This will open up a new window called Power Query Editor. Click the Transform tab and then click the Transpose button. Click the Use First Row as Headers button. Return to the Home tab and click Close &amp; Load to import your transposed data. Generate a Calibration Curve: Click anywhere on your data table. Click the Insert tab and select the Scatter chart from the charts option. Get rid of the title. Label the x-axis: “Protein Conc. (mg/mL)”. Label the y-axis: “Absorbance (Arb. units)”. Click the green plus (chart elements) icon at the top-right of the graph and select More options from Trendline. Add a linear trendline and check the box to “Display Equation on Chart”. If you’re including your figure in a report it’s better practice to include details of the \\(m\\) and \\(b\\) values that you extract in your figure caption like in figure 3.1. Use the Calibration Curve: Now that you’ve extracted the m and b values from your calibration curve’s linear trendline you can use it to find the concentration of an unknown sample by measuring its absorbance. Our calibration curve for the dataset gives us the equation: \\[ A = 0.3827 \\cdot C - 0.0024 \\] If an unknown protein solution has an absorbance of 0.50, you can rearrange the equation to solve for concentration \\(C\\): \\[ 0.50 = 0.3827 \\cdot C - 0.0024 \\] Solving for \\(C\\): \\[ C = \\frac{0.50 + 0.0024}{0.3827} = 1.3 \\, \\text{mg/mL} \\] Thus, the concentration of the unknown protein solution is 1.3 mg/mL. Note that I’ve expressed my answer to 2 significant figures to match the minimum level of precision available to me during the calculation (i.e. 2 s.f for both 0.5 and 0.0024). Figure 3.1: Calibration curve for Hemoglobin determined from a standard set generated from a 1:1 serial dilution from a starting 1.5 mg/mL solution. Values of m=0.3827 and b=0.0024 were extracted from the linear relationship A = m * C + b, where b is the systematic error associated with the measurement (most likely related pipetting inaccuracies). 3.2 Linear Regression A linear trendline is visually helpful, but what what Excel is actually doing behind the scenes is something more powerful: linear regression. Linear regression is a statistical method used to quantify how much the variation in a dependent variable can be attributed to changes in an independent variable. It helps us understand how one variable predicts or influences another. Additionally, it provides insight into how much of the variation is due to error or other unaccounted factors, and whether we need to explore other relationships or variables that may better explain the observed patterns. Linear regression models are valuable because they offer an estimation of the relationship between variables, and by extension, help in predicting future outcomes based on known values of independent variables. Why “Linear”? In nature, over relatively small ranges of dependent and independent variables, the relationship between them can often be approximated as linear. This means that as one variable increases or decreases, the other responds in a predictable and proportional manner. While not always the case, assuming linearity can be useful for many applications, especially for the scope of this course. Independent and Dependent Variables: - The independent variable is the one you manipulate or change to see its effect (e.g., hours of sunlight). - The dependent variable is the one being measured or observed (e.g., sunflower growth). These are sometimes referred to as explanatory and response variables, respectively. Linear Regression as a Hypothesis Test Performing a linear regression is also a test of a hypothesis. This time, our null hypothesis is that there is no linear relationship between the dependent and independent variables. In other words, any observed association between the two is just due to random chance. More formally: Null Hypothesis (H₀): There is no linear relationship between the independent variable and the dependent variable. The slope of the regression line is equal to zero. H₀: β₁ = 0 (where β₁ represents the slope of the regression line) Alternative Hypothesis (H₁): There is a linear relationship between the independent variable and the dependent variable. The slope of the regression line is not equal to zero. H₁: β₁ ≠ 0 3.2.1 The Linear Regression Equation The linear regression equation provides a way to express the relationship between the independent variable (predictor) and the dependent variable (outcome) as a straight line: \\[ Y = β₀ + β₁X + ε \\] Y: The dependent variable (outcome) we are trying to predict. X: The independent variable (predictor) we use to make predictions. β₀: The intercept, or the value of Y when X is 0. This represents the starting point of the relationship. β₁: The slope, or the change in Y for every one-unit increase in X. It tells us how steep the relationship is. ε: The error term, which accounts for the variance in Y that cannot be explained by X. 3.2.2 Performing a Linear Regression in Excel Let’s work through an example using a full linear regression with Excel’s Analysis ToolPak. Suppose we’re investigating the growth per day of sunflowers (dependent variable) based on the number of hours of direct sunlight they receive each day (independent variable). In this case, we want to determine if there is a linear relationship between hours of sunlight and sunflower growth. By performing the regression analysis, we’ll be able to assess if and how sunlight impacts sunflower growth, and how strong that relationship is. Let’s get started with the analysis in Excel! 1. Download the data: Here is an example sunflower dataset. Download it to your local machine. 2. Import the data: Open up a new Excel workbook. Go to the Data tab and select the Import from csv/txt icon. Once the wizards has loaded click Load. 3. Perform Regression: Click anywhere on the sheet, select the Data tab again and select the Data Analysis button in the “Analysis” area of the tools ribbon. If you can’t see the button then you probably need to configure the Analysis ToolPak Add-In. Select “Regression” and click “OK”. Select the “Input Y Range” - This is your dependent variable, i.e. how much your sunflowers grew per day (column B). Include the header. Select the “Input X Range” - This is your independent variable, i.e. number of hours of direct sunlight per day (column A). Include the header. Tick the **Labels” box. Select the New Worksheet Ply output option Check all the remaining boxes for Residuals and Normal Probability sections. Click OK. Hopefully, you’ll see something like that shown in figure 3.2. Figure 3.2: This is what you should see after performing a regression on your example sunflower dataset using Excel’s Analysis ToolPak Add-In. 3.2.3 Interpreting Your Linear Regression Wow! Your linear regression has provided so much information—it can feel like an avalanche at first. But don’t worry, not all of it is important for our purposes. Let’s focus on what is interesting and useful. 1. Summary Output Table: Multiple R: This is the correlation coefficient between your observed and predicted values. It ranges from -1 to 1. A value closer to 1 or -1 indicates a strong relationship, whereas a value close to 0 means a weak relationship. In our case, a value of 0.65 indicates a reasonably strong correlation between growth and sunlight. R Square (R²): This tells us how much of the variance in the dependent variable is explained by the independent variable(s). In our case, an R² of 0.42 means that 42% of the variation in the dependent variable is explained by the model. Conversely, it means that approx 58%, i.e. the majority, of the variance is explained by other unknown variables. In this context, these unknown variables could include things like daily average temperature, rainfall, humidity etc. Adjusted R Square: This adjusts the R² value for the number of predictors in your model. It’s more useful when dealing with multiple independent variables as it accounts for any unnecessary complexity added by too many variables. Standard Error: This measures the average distance that the observed values fall from the regression line. A smaller standard error means a better fit. Note that the units of your standard error are the same as for your dependent variable, i.e. mm. 2. ANOVA Table The ANOVA table tells us whether the overall regression model is significant. Degrees of Freedom (df): This refers to the number of independent pieces of information that went into calculating the estimates. It’s a balance between the number of observations and the number of predictors in the model. SS Regression (Sum of Squares due to Regression): This represents the variance in the dependent variable explained by the model. SS Residual (Sum of Squares of Residuals): This represents the variance that the model doesn’t explain. Significance F: This is the p-value for the overall regression model. In our case, A p-value &lt; 0.05 means that the model is statistically significant, and we reject the null hypothesis that there is no relationship between the dependent and independent variables. 3. Coefficient Table This table is where you’ll find the key parameters for your linear regression model. Intercept (β₀): This is the expected value of the dependent variable when the independent variable is zero. In other words, it’s your y-intercept value. Gradient (β₁): This is the slope of your line, showing how much the dependent variable changes with each unit increase in the independent variable. P-values: These tell us whether the coefficients (intercept and gradient) are statistically significant. If the p-value for the gradient is less than 0.05, we can say that the independent variable has a significant impact on the dependent variable. 4. Residual Plots and Additional Tables Residual Plot: This plot shows the differences between the observed and predicted values (residuals). Ideally, these residuals should be randomly scattered around 0, indicating a good model fit. If this is not the case then it might not be appropriate for you to be performing a linear regression. Normal Probability Plot: This checks if the residuals follow a normal distribution. If the points follow a straight line, the residuals are normally distributed. Again, if this is not the case then you should seek an alternative analysis. Residual Output: This provides detailed information about each residual (the error for each observation). Probability Output: Provides the probabilities of observing these residuals given the model fit. 3.2.4 Making Predictions with Our Model With the insights from our linear regression analysis, we can now use our model to make predictions using the equation \\[ \\hat{Y} = \\beta_0 + \\beta_1 \\hat{X} \\] where: - \\(\\hat{Y}\\) is the predicted value of the dependent variable. - \\(\\beta_0\\) is the intercept, representing the expected value of \\(Y\\) when \\(X\\) is zero. - \\(\\beta_1\\) is the gradient (or slope), indicating how much \\(Y\\) changes with a one-unit change in \\(X\\). - \\(\\hat{X}\\) is the value of the independent variable for which we want to make a prediction. For example, to predict the daily growth of our sunflowers when they’ve received six hours of sunlight: \\[ \\hat{Y} = 2.449 + ( 0.898 \\times 6 )= 7.836 \\] So, the predicted value of \\(\\hat{Y}\\) for \\(\\hat{X} = 6\\) is \\(7.836\\) \\(mm\\). But wait! Couldn’t I have achieved this by simply adding a trend line to a plot of \\(X\\) vs. \\(Y\\), as discussed in section @ref{section:calibration-curves}? Yes, you could have extracted the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) from the equation of the line, and even added the \\(R^2\\) value to the plot. So why go through all this additional work? The full regression analysis allows us to determine the standard error (\\(SE\\)) of the model, which enables us to calculate the uncertainty associated with a predicted value \\(\\hat{Y}\\). This is something we couldn’t do with a simple trend line! This uncertainty, often referred to as the margin of error (\\(ME\\)), provides the range within which future individual observations are expected to fall. To calculate \\(ME\\): \\[ ME = t \\times SE \\] where \\(t\\) is the t-statistic which, in this context, can be drawn from a Student’s t-distribution and accounts for the uncertainty in estimating the population parameters from a sample. In excel you can calculate \\(t\\) using: =T.INV.2T(alpha, df) alpha is the significance level, in this case 0.05. df is the degrees of freedom which can be calculated as \\(df = n - k - 1\\) where \\(n\\) is the number of data points (observables) and \\(k\\) is the number of independent variables. In our case, \\(n=24\\) and \\(k=1\\) so \\(df=24-1-1=22\\). 3.2.5 Extending to Multiple Variables Right now, we’re focusing on simple linear regression (one dependent and one independent variable). However, what if we wanted to examine the effect of multiple independent variables (e.g., sunlight and temperature on sunflower growth)? This is called multiple regression and we’d consider the following linear regression equation instead: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon \\] Where: \\(X_1, X_2, ..., X_n\\): The independent variables (e.g., sunlight, temperature, etc.). \\(\\beta_0\\): As before, this is the intercept, representing the expected value of \\(Y\\) when all independent variables are 0. \\(\\beta_1, \\beta_2, ..., \\beta_n\\): The coefficients of the independent variables, representing the change in \\(Y\\) for a one-unit change in \\(X\\), assuming all other variables remain constant. Multiple regression allows us to assess the combined effect of several factors, which is crucial when studying complex systems where multiple variables might influence the outcome. Unfortunately, Excel has its limits for this. But don’t worry—we’ll tackle multiple regression using R later! 3.3 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["introduction-to-r-part-i.html", "4 Introduction to R: Part I 4.1 Reading and Inspecting Data 4.2 Subsetting 4.3 Summarising Data in R 4.4 Complete your Weekly Assignments", " 4 Introduction to R: Part I If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. We are going to diverge from what you have been doing in the labs this week in order to focus on introducing you to R. Our learning objectives are as follows: Start a new R Project in R Studio Read a dataset to a variable Inspect a dataset Subset a dataset by slicing and filtering Create summary tables of descriptive statistics Sort a table You should have completed section 1.2 already and have R and R Studio installed on your personal machine. If you are unable to install R and RStudio then please use one of the University computers. OMG. Why, are you making me learn to code? In my experience, at least half of you will love learning to code. The rest of you will hate my guts. I’m OK with that. That’s because I truly believe that in the near future you will see that even a basic understanding of coding gives you a huge advantage in your studies and future careers. However, if you really need me to give you explicit reasons as to why I’m making you learn to code then here are six: 1. Data Literacy All the biosciences rely heavily on data analysis. Learning to code will equip you the skills to efficiently manipulate and interpret complex datasets. R, in particular, is well-suited for handling bioscientific data, from genomic sequences to clinical trials and environmental modelling. 2. Reproducibility Coding promotes transparency and reproducibility in scientific analyses. Unlike manual methods, where calculations or procedures may be difficult to replicate, scripts provide a clear, step-by-step record that can be easily shared and re-run. This is vital for ensuring the integrity of scientific research. 3. Automation Bioscientists often work with large datasets, and coding enables the automation of repetitive tasks such as data cleaning, statistical analyses, and reporting. This not only saves time but also minimises human error, allowing students and researchers to focus on interpreting their results. 4. Career Readiness Coding has become an indispensable skill in the biosciences. Whether working in research, biotechnology, or environmental consultancy, understanding how to work with data in R (or similar tools) will give you a competitive edge when applying for jobs or pursuing further academic research. 5. Critical Thinking Writing code fosters critical thinking and problem-solving skills. While learning to code you will have to break down complex problems, troubleshoot errors, and develop logical workflows. These skills are not only useful for coding but are also vital for scientific thinking and research. 6. Integration with Lab Skills Coding complements traditional lab skills. In the era of bioinformatics and systems biology, being able to analyse experimental data computationally can provide additional insights that are often not apparent from lab experiments alone. Have you accepted your fate then? Good. Now lets get something else straight: I am NOT going to teach you to code. You are going to do that yourself, over many months (and probably years). This book represents a mere “dipping of the toes” into the world of R coding and is by no means comprehensive. In fact, it is completely and utterly incomplete. You will need to fill in many of the blanks yourself as you encounter them. How you go about this is up to you, but checkout the Appendix for good places to start. 4.1 Reading and Inspecting Data Let’s dive in then! This week we’re going to be introducing you to some fundamental data handling concepts in R using the fantastic Pantheria dataset of extant and recently extinct mammals. Open RStudio and start a new project Download the data from here (or right click, save as from here). Copy the downloaded file into your RStudio project directory (folder). You should see the file appear in your Files window in the bottom-right corner of RStudio. Create a new R script file and save it with a sensible filename (e.g. pantheria_summary.R) Read the data to a variable called pantheria_data: In your new script file, write the following line of code on line 1: pantheria_data &lt;- read.csv(&quot;pantheria_999.csv&quot;) Run the line by clicking anywhere on the line and then clicking the run button (in the top-right corner of your script window) or by pressing Ctrl+Enter on your keyboard. You should see a new line appear in your Environment window in the top-right of your RStudio like in figure 4.1. If you don’t see it then the here are the most common reasons as to why: You haven’t used quotation marks around your data filename You’ve mis-spelled your filename You haven’t put your data file in the correct folder You didn’t create an R project first. Figure 4.1: Details of your data variable should appear in your environment window after running line 1. Figure 4.2: By clicking on your data variable in your environment window you can inspect the data in RStudio What is a variable in R? A variable in R is a symbolic name representing a value stored in memory. Variables are created by assigning values using &lt;-. Naming Convention I prefer using lowercase words for variable names. If multiple words are needed, I separate them with an underscore “_“, avoiding spaces. For example: my_variable. Best Practices Descriptive Names: Choose clear, meaningful names. Consistency: Use the same naming style throughout your code. Avoid Reserved Words: Don’t use R’s reserved keywords (if, else, for, etc.) Uniqueness: Ensure variable names are unique within your environment. These practices enhance code readability and maintainability. Well done if you can see your data variable! You can click on your variable (in the environment window) and inspect your data directly. You should see something like that shown in figure 4.2. Wow! Look at all that data. Yummy. Now the world is your oyster, as they say. From here you can go in any number of directions and use R to summarise, visualise and analyse your data in order to gain novel insights. Let’s start small by asking the very simple question: “What order of mammal occurs the most frequently (has the most number of rows) in the dataset?”. To answer this question, I need to go through my data row by row and count how many times a row corresponding to each order (e.g. Dermoptera, Chiroptera, Rodentia etc …) appears. Let’s extend my R script as follows: pantheria_data &lt;- read.csv(&quot;pantheria_999.csv&quot;) orders &lt;- table(data$Order) print(orders) Let’s break these additional lines down, step-by-step: Firstly, you’ll notice that I’ve left lines 2 and 5 blank. You don’t need to do this, but I like to because it gives my code a bit of breathing space and is often easier to identify a problem later. In line 3, I’m using the table() function to count the number of times a unique value in my order column appears. By putting data$Order inside the table function’s brackets we are “passing” it all of the values in the Order column as a big list. I’m then assigning the output of the table function to a new variable called orders so that I can refer to it later. Lastly in line 5, I’m using a print function to output the contents of the variable orders to my console as shown in figure 4.3 Figure 4.3: When using print(orders) the following should appear in my RStudio console. What is a function in R? A function in R is a set of instructions that performs a specific task or calculation, taking inputs (arguments) and returning an output (result). So far in this course, we have used three pre-defined functions: read.csv() to read data from a CSV file, table() to summarise categorical data, and print() to display output in the console. While these functions are built into R, we can also create our own functions when we need to perform customised tasks—more on this later in the course. You can see by inspecting your console output that the order Rodentia is the most frequently occurring order with 549 rows in the data. But what if there were lots more orders? It might not be so straightforward. Let’s get our R script to pick out the most frequently occurring order for us: pantheria_data &lt;- read.csv(&quot;pantheria_999.csv&quot;) orders &lt;- table(data$Order) most_common_order &lt;- names(which.max(orders)) print(most_common_order) I’ve inserted the line most_common_order &lt;- names(which.max(orders)) at line 5. Let’s break it down: which.max(orders): Function: which.max() is a built-in R function that returns the index of the first maximum value in its input. Input: In this case, the input is orders, which is the table we created earlier containing the counts of each order in the dataset. Output: The function identifies the index position of the maximum count (i.e., the highest frequency) in the orders table. In our case “Rodentia” is at position 24 in the table. names(...): Function: The names() function retrieves the names (or labels) associated with the elements of an object. For example if I used names(orders) the function would simply return a list of all the unique orders. Usage: By wrapping which.max(orders) inside names(), we are saying, “give me the name of the order that corresponds to the maximum frequency index returned by which.max().” Output: In our case the name “Rodentia” is returned. most_common_order: I’ve assigned the result of my names(which.max(orders)) to a new variable using &lt;-. Finally, I’ve updated my print statement to `print(most_common_order)’. This should now print out “Rodentia” in my console when I re-run my script. Phew! That took a bit of explaining. Make sure you understand what just happened. The wrapping of a function in a function is a common practice in R and things can quickly become obfuscated. It’s good practice to comment your code so that you (or someone else) can quickly make sense of what is going on. Here is my final code, with comments! # Read the CSV file and store its contents in &#39;data&#39;. data &lt;- read.csv(&quot;pantheria_999.csv&quot;) # Create a frequency table of the &#39;Order&#39; column. orders &lt;- table(data$Order) # Get the name of the most common order. most_common_order &lt;- names(which.max(orders)) # Print the most common order to the console. print(most_common_order) 4.2 Subsetting In data analysis, subsetting refers to the process of extracting a portion of a dataset based on specific criteria. This allows you to focus on the most relevant information, making analysis more efficient and tailored to your needs. There are two key methods for subsetting data in R: Slicing and Filtering. 4.2.1 Slicing Slicing is a crucial operation in data analysis, enabling you to extract specific rows, columns or both from an existing data frame. This technique helps streamline workflows, especially when working with large datasets, by narrowing down data to focus on relevant sections. Slicing Example Let’s add a new line to our earlier script file: slice &lt;- data[1:1000, 1:10] This code creates a new variable called slice and assigns to it the first 1,000 rows and the first 10 columns of the dataset data. Run the line and you should see a new dataframe called slice appear in your Environment window (top-right). What if I’d wanted to select ALL the rows but still slice off the first 10 columns? Well, that would look like this: slice &lt;- data[, 1:10] And what if I don’t want to use index values to select the columns (i.e., 1:10) and I want to select them by name? For example let’s say I’ve inspected the column headers and I specifically want to select the following columns: Order AdultBodyMass_g BasalMetRate_mLO2hr I can do that using this code: slice &lt;- data[, c(&quot;Order&quot;, &quot;AdultBodyMass_g&quot;, &quot;BasalMetRate_mLO2hr&quot;)] Here I’ve defined a vector using the c() function containing the verbatim names of columns I want and used this instead of specifying a range of column indices. 4.2.2 Filtering Unlike slicing, which selects a subset of rows and columns based on their position, filtering involves evaluating each row against a set of logical conditions and including only those that meet the criteria. Filtering Example In the above slicing example we created a dataframe variable called slice which contains three columns. If you click on the variable in the environment window to select it you’ll see something like that shown in 4.4. Figure 4.4: You’ve sliced your data but you’ll need to be able to filter in order to remove the -999 values. Hmmm, that’s weird. What’s with all the -999 values? In this dataset the value -999 has been used to indicate where a value wasn’t measured. I can remove these -999 values in my slice dataframe like this: filtered &lt;- slice[slice$AdultBodyMass_g &gt;=0 &amp; slice$BasalMetRate_mLO2hr &gt;=0, ] Let’s break this down: filtered &lt;-: This assigns the result of the filtering operation to a new variable called filtered. This variable will contain only the rows from the slice data frame that meet certain conditions. slice[slice$AdultBodyMass_g &gt;= 0 &amp; slice$BasalMetRate_mLO2hr &gt;= 0, ]: This part performs the actual filtering operation on the slice data frame. slice$AdultBodyMass_g: This accesses the AdultBodyMass_g column of the slice data frame. The $ operator is used to refer to a specific column. slice$BasalMetRate_mLO2hr: Similarly, this accesses the BasalMetRate_mLO2hr column of the slice data frame. slice$AdultBodyMass_g &gt;= 0: This condition checks each value in the AdultBodyMass_g column to see if it is greater than or equal to 0, generating a logical vector (TRUE or FALSE) for each row. slice$BasalMetRate_mLO2hr &gt;= 0: This condition checks each value in the BasalMetRate_mLO2hr column to see if it is greater than or equal to 0, producing another logical vector. &amp;: This is the logical AND operator, which combines the two logical vectors created by the previous conditions. The result is a new logical vector that is TRUE only for rows where both conditions are TRUE. slice[… , ]: The entire expression inside the square brackets is used to subset the slice data frame. The rows for which the combined condition is TRUE are selected, and all columns (indicated by the empty space after the comma) are returned. In summary, the code creates a new data frame, filtered, which contains only the rows from the slice data frame where both the AdultBodyMass_g and BasalMetRate_mLO2hr values are greater than or equal to 0. 4.2.3 So what? Well, being as you’ve gone to all that trouble to slice and filter your data we should probably do something spectacular with it! Try adding the following code to your script to and running it. plot(slice$AdultBodyMass_g, slice$BasalMetRate_mLO2hr, log=&quot;xy&quot;, xlab = &quot;Adult Body Mass (g)&quot;, ylab = &quot;Basal Metabolic Rate (mLO2/hr)&quot;, main = &quot;&quot;) You should see something that looks like what is shown in figure 4.5. Figure 4.5: An all species log-log scatterplot of mammal adult body mass (g) vs basal metabolic rate (mLO\\(_2\\)/hr) Notice that I’ve used the argument log=\"xy\" within my plot() function to make both the x and y axes scale logarithmically. This means that instead of the axes increasing in equal increments (e.g., 1, 2, 3), they increase by factors of ten (e.g., 10, 100, 1000). This is particularly useful when the relationship between two variables spans several orders of magnitude, as is often the case in biological data. When both axes are scaled logarithmically, relationships that might look curved on a linear scale can appear as straight lines, making it easier to identify patterns. In this case, the plot reveals a linear relationship between the two variables (e.g., adult body mass and basal metabolic rate) when viewed on a log-log scale. This linearity indicates that as one variable increases by a certain percentage, the other variable increases by a consistent percentage, rather than by a fixed amount. This relationship is described as allometric scaling and is remarkably common in nature. What’s fascinating is that this log-log linearity holds across all mammals! From the smallest mouse to the largest whale, body mass and metabolic rate follow a consistent scaling relationship when plotted logarithmically. This insight allows scientists to understand fundamental principles about how biological processes like metabolism are related to size in the animal kingdom. 4.3 Summarising Data in R Remember how in chapter 2 we generated a summary table of descriptive statistics using Excel? We can do it in R too! As an example, let’s answer the following question: What are the top 10 families with the highest number of neonate body mass observations, along with their summary statistics (mean, median, max, min, and standard deviation)? The following script will do the job, as you can see by the accompanying output. # Read the CSV and filter out invalid rows data &lt;- read.csv(&quot;pantheria_999.csv&quot;) data_filtered &lt;- data[data$NeonateBodyMass_g != -999, ] # Calculate summary statistics for each Family summary_table &lt;- aggregate(NeonateBodyMass_g ~ Family, data_filtered, function(x) c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x), Count = length(x))) # Convert the list columns to individual columns summary_table &lt;- do.call(data.frame, summary_table) # Rename the columns colnames(summary_table) &lt;- c(&quot;Family&quot;, &quot;Mean&quot;, &quot;Median&quot;, &quot;Max&quot;, &quot;Min&quot;, &quot;Std. Dev.&quot;, &quot;Count&quot;) # Format all numeric values to 1 decimal place (except Count) summary_table[, 2:6] &lt;- round(summary_table[, 2:6], 1) # Sort by Count in decreasing order and return top 10 most frequently occurring families in data summary_table &lt;- summary_table[order(-summary_table$Count), ][1:10, ] # Write to CSV and print write.csv(summary_table, &quot;summary_table.csv&quot;, row.names = FALSE) print(summary_table) ## Family Mean Median Max Min Std. Dev. Count ## 9 Bovidae 8291.2 4819.6 39843.1 500.0 8521.8 36 ## 17 Cercopithecidae 465.9 450.0 890.0 240.7 156.7 19 ## 69 Mustelidae 149.7 30.0 1894.4 2.0 429.0 19 ## 18 Cervidae 4252.0 3082.2 13500.0 400.0 4027.9 18 ## 13 Canidae 167.6 102.1 412.3 28.0 127.6 14 ## 47 Heteromyidae 3.1 3.0 7.7 1.0 1.9 14 ## 80 Phocidae 16490.5 15122.7 39393.0 3050.0 10434.1 14 ## 93 Sciuridae 10.3 6.1 33.0 3.3 10.0 13 ## 41 Felidae 186.4 161.6 409.9 72.0 102.4 12 ## 56 Leporidae 74.4 90.0 123.0 25.9 39.0 11 See if you can reproduce the output by creating a new script file in your project, copying and pasting the code above and clicking the Source button in the top right of your script editor window. What Next? I don’t need you to understand every line in the script. In fact there are some lines (6 -7 in particular) that are doing some seriously funky stuff that took me years to learn and understand fully. Nonetheless, ask yourselves what would you need to tweak to get the code to generate a summary table for another variable (e.g. AdultBodyMass_g) or group the results by Order instead of Family. Often, when learning to code, it can feel overwhelming trying to understand every single detail of a script before running it. However, there’s value in a “run it and see what happens” approach, especially when you’re starting out. The beauty of coding is that you don’t need to fully grasp every element in order to get results. In fact, many experienced coders begin with scripts they might not completely understand. The key is knowing just enough to use the script to answer your question. Once you see the output and observe how the code works, you can begin to tweak and modify it to suit a different dataset or to refine your results. The process of experimenting with the code helps deepen your understanding over time. You’ll gradually learn how each part of the script contributes to the output, and that’s where real learning happens. Turbo charge your learning with Chat-GPT I have serious mis-givings about asking generative AI to generate code out of thin air. In my experience, GAI likes to show off and often over-complicate things. This can seriously confuse students who are just starting to learn to code. However, using it to understand existing code? Yes! Do it! What a fantastic way to learn. I use it every day when it comes to code. I cut and pasted the script above into Chat-GPT and gave it the following prompt: “Explain in detail”. You can see the result here. Isn’t that amazing? Feedback Please. I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 4.4 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["introduction-to-r-part-ii-visualisation.html", "5 Introduction to R part II: Visualisation 5.1 Mushroom Compost Scenario 5.2 Inspecting and Summarising 5.3 Histogram 5.4 Grouped Boxplot 5.5 Scatterplot 5.6 Just for Fun 5.7 Complete your Weekly Assignments", " 5 Introduction to R part II: Visualisation If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s summative assessment in the BIOS103 Canvas Course. There is no formative assessment this week. Our brains are hardwired to recognise patterns, which means we can understand information much more easily when it’s presented visually rather than as raw numbers. In today’s data-driven world, the ability to visualise information effectively is more important than ever. Learning to visualise data will equip you with essential skills for analysing and interpreting information, allowing you to communicate insights clearly and persuasively. Effective visualisation also fosters critical thinking. It helps you identify trends, relationships, and anomalies within datasets, enhancing your analytical abilities and preparing you for future roles in research, industry, and beyond, where data-driven decision-making is key. This week, we’re going to explore visualisation in R, focusing on creating impactful graphics using its native capabilities. Our objectives include: Histograms: Visualising the distribution of continuous variables. Boxplots: Showing data spread between groups and highlighting outliers. Scatterplots: Displaying relationships between two continuous variables. By the end of this chapter, you will be equipped to create these essential visualisations, enhancing your analytical skills and bringing your data to life! In an effort to ensure that this week’s chapter is relevant to your microbiology lab practicals, I have carefully crafted the following scenario to provide a bit more context for our visualisation. 5.1 Mushroom Compost Scenario MegaMush, a mushroom compost company based in the Netherlands, is trying to identify an issue with its pre-pasteurisation composting process across its five operational production sites. You have been hired as an independent bioscience data consultant and have been tasked with summarising and visualising a dataset compiled over a period of 1 year and consolidated from each location. Specifically, you have been asked to generate the following for incorporation into a report that will be presented by the chief technical officer of the company to the extended board of directors: A summary table that shows the mean and standard deviations of the composting temperature, moisture, and viable bacterial count, VBC (cfu/g), at each site. A histogram showing the distribution of estimated viable bacterial count, VBC from across all samples. A grouped boxplot showing the distributions of estimated VBC for each site. A scatterplot that shows the relationship between temperature and VBC. Any additional visualisations (histogram, boxplot or scatterplot) that identify other interesting features in the data. 5.1.1 The dataset You can download the raw data here or from the backup link. 5.1.2 Further information The compost temperature and moisture content were measured directly during the sampling process using calibrated thermometers and moisture probes. After collection, samples were immediately placed into sterile containers to prevent cross-contamination and transported to the central laboratory under refrigerated conditions at 4°C. To ensure the integrity of microbial counts, all samples were processed within 24 hours of collection. In the laboratory, 0.5g of each compost sample was dissolved in 10mL of sterile reagent and mixed thoroughly to create the starting solution for subsequent serial dilution. In each case, a 1/1000 serial dilution was performed and 0.1mL of the diluted sample was plated onto selective agar media. Plates were incubated for 48 hrs at 30°C before the number of viable colonies were counted. This procedure was performed regularly for each site, ensuring consistent data collection on microbial counts, temperature, and moisture content across all locations over the course of a year. Your tables and figures need to be formatted according to the MegaMush report template 5.2 Inspecting and Summarising Right then, let’s start. You know the drill by now: Create a New R Project Download the dataset and move to project folder Create a new script file and save it with a sensible filename Read your dataset to a new variable called data That should create a data variable in my environment window. I could click on it and look at the data, but that method was so “last week”. This time I’m going to simply add the line head(data) to my script and run it to spit out the first 5 rows of my data (including headers) in my console. Here’s what my script (and its output) looks like so far: # Script File: mushroom_summary.R # Read the data data &lt;- read.csv(&quot;compost_999.csv&quot;) # Output the first 5 rows in console head(data) ## SampleID Location DateTime DayOfWeek Temperature Moisture ## 1 MAA_20230802 Maastricht 2023-08-02 Wednesday 54.8 71.8 ## 2 ROT_20230611 Rotterdam 2023-06-11 Sunday 55.5 61.8 ## 3 UTR_20230818 Utrecht 2023-08-18 Friday 68.2 78.5 ## 4 AMS_20230913 Amsterdam 2023-09-13 Wednesday 57.6 73.0 ## 5 UTR_20230619 Utrecht 2023-06-19 Monday 70.5 76.3 ## 6 GRO_20231109 Groningen 2023-11-09 Thursday 52.9 70.3 ## Viable.counts ## 1 153 ## 2 144 ## 3 139 ## 4 149 ## 5 133 ## 6 155 OK. So it doesn’t look too pretty in my console, but I can glean the important information - i.e. the data headers and the types of data that are in each column. The headers are self explanatory , except perhaps for Viable.counts which is a direct count of the number of observed colonies formed on each sample plate. This is not to be confused with Viable Bacterial Count, VBC, defined as the number of colony forming units per gram (cfu/g). Calculating VBC from Viable Counts To calculate the Viable Bacterial Count (VBC) from the Viable Counts observed on the agar plates, we need to consider the specific parameters of the laboratory procedure. Here’s how the calculation works step-by-step: Viable Counts: The number of colonies (cfu) observed on an agar plate after incubation. Let’s denote this VC Dilution Factor: Remember, all starting solutions have undergone a 1/1000 serial dilution, we must first multiply by a factor of 1000. Volume Plated: From each diluted sample, 0.1 mL was plated onto the agar. This means that only 0.1mL of the diluted solution contributes the visible colonies counted. To express the counts in terms of 1mL, we must multiply by another factor of 10. Volume of Starting Solution: At this points we have units of cfu/mL. To figure out how many cfu there were in the starting solution of 10 mL we need to multiply by yet another factor of 10. Weight of Sample: Finally, since 0.5 g of each compost sample was used in the initial preparation, we need to express the results per gram. To convert out counts to reflect the weight of the sample, we divide by the weight used, which is 0.5 g. This can be expressed as multiplying by a factor of 2. Putting all this together, the equation to calculate VBC from the observed Viable Counts can be expressed as: \\[ \\text{VBC (cfu/g)} = \\frac{\\text{VC} \\times (dilution\\ factor)}{(spread\\ plated\\ volume)} \\times \\frac{(starting\\ solution\\ volume)}{(sample\\ mass)} \\] \\[ \\text{VBC (cfu/g)} = \\frac{\\text{VC} \\times (1000)}{(0.1)} \\times \\frac{(10)}{(0.5)} \\] Thus, the final calculation simplifies to: \\[ \\text{VBC (cfu/g)} = \\text{VC} \\times 1000 \\times 10 \\times 10 \\times 2 = \\text{VC} \\times 200,000 \\] This equation indicates that for every viable colony observed, the equivalent VBC is determined by multiplying by a factor of 200,000 to account for the dilution, volume plated, and sample weight, providing a standardised measure of viable bacteria per gram of compost. 5.2.1 Summary Table Let’s tick the first task from the scenario off our list of things to do. Let’s create a summary table that shows the mean and standard deviations of the composting temperature, moisture, and viable bacterial count, VBC (cfu/g), at each site. Curses! I’ve completely forgotten how to create a summary table in R. It’s the end of the world! No it’s not. Let’s do the obvious (and easy) thing and revisit the script we used in the last chapter to generate a summary table and see if we can tweak things to make it work for us again. Here’s the original script (or the first couple of lines at least). # Read and filter data data &lt;- read.csv(&quot;pantheria_999.csv&quot;) data_filtered &lt;- data[data$NeonateBodyMass_g != -999, ] # Calculate summary statistics for each Family summary_table &lt;- aggregate(NeonateBodyMass_g ~ Family, data_filtered, function(x) c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x), Count = length(x))) # Convert the list columns to individual columns summary_table &lt;- do.call(data.frame, summary_table) To adapt this script to work for my new “compost_999.csv” I need to consider the following things: Update the dataset: I need to update my read.csv() function by replacing pantheria_999.csv with compost_999.csv. Filter step removal: I know that my compost_999.csv dataset has no missing or erroneous data (please take my word for this) so I don’t need to worry about creating a new data_filtered variable. I can delete the line and update the rest of the code to just use my initial data variable throughout. Adjust the grouping and variables: In my aggregate function I no longer need to summarise the NeonateBodyMass_g by Family. Instead I want to summarise Temperature, Moisture, and Viable.counts by Location. Simplify descriptive statistics: I only need to calculate the mean and standard deviation within my aggregate function this time (forget about median, min and max). With all that in mind, here’s how I’d update the script: data &lt;- read.csv(&quot;compost_999.csv&quot;) # Calculate summary statistics for Temperature, Moisture, and Viable counts for each Location summary_table &lt;- aggregate(cbind(Temperature, Moisture, Viable.counts) ~ Location, data, function(x) c(Mean = mean(x), SD = sd(x))) # Convert the list columns to individual columns summary_table &lt;- do.call(data.frame, summary_table) # Output table in console print(summary_table) ## Location Temperature.Mean Temperature.SD Moisture.Mean Moisture.SD ## 1 Amsterdam 58.25165 2.126257 74.94286 2.028253 ## 2 Groningen 54.67905 1.852581 70.03905 1.869157 ## 3 Maastricht 56.84135 2.105592 72.42500 2.019793 ## 4 Rotterdam 56.82762 2.015302 62.32762 1.971016 ## 5 Utrecht 69.69368 2.051456 75.57263 1.743159 ## Viable.counts.Mean Viable.counts.SD ## 1 150.5934 3.602401 ## 2 152.0381 3.116099 ## 3 150.7404 3.424626 ## 4 142.6571 3.292849 ## 5 134.0000 3.497720 The biggest difference between the new and old scripts is the use of the cbind() function within my aggregate() function. The cbind() function allows me to group together multiple variables (Temperature, Moisure, Viable Counts) in order to calculate and present their descriptive stats simultaneously. Nice. Our summary table is nearly complete. The last thing I need to do is calculate two more columns for the mean and standard deviations of the VBC (cfu/g). As I explained above, to get values of VBC I need to multiply my viable counts by a factor of 20,000. Here’s how I’d update my summary table script: data &lt;- read.csv(&quot;compost_999.csv&quot;) # Calculate summary statistics for Temperature, Moisture, and Viable counts for each Location summary_table &lt;- aggregate(cbind(Temperature, Moisture, Viable.counts) ~ Location, data, function(x) c(Mean = mean(x), SD = sd(x))) # Convert the list columns to individual columns summary_table &lt;- do.call(data.frame, summary_table) # Define variable factor according to calculation for VBC. Divide by 1e7 to express answers in standard form. factor &lt;- 2e5 / 1e7 # Calculate additional columns vbc_mean and vbc_sd using factor summary_table$vbc_mean &lt;- summary_table$Viable.counts.Mean * factor summary_table$vbc_sd &lt;- summary_table$Viable.counts.SD * factor # Save table to file write.csv(summary_table, &quot;compost_summary.csv&quot;, row.names = FALSE) # Output table in console print(summary_table) ## Location Temperature.Mean Temperature.SD Moisture.Mean Moisture.SD ## 1 Amsterdam 58.25165 2.126257 74.94286 2.028253 ## 2 Groningen 54.67905 1.852581 70.03905 1.869157 ## 3 Maastricht 56.84135 2.105592 72.42500 2.019793 ## 4 Rotterdam 56.82762 2.015302 62.32762 1.971016 ## 5 Utrecht 69.69368 2.051456 75.57263 1.743159 ## Viable.counts.Mean Viable.counts.SD vbc_mean vbc_sd ## 1 150.5934 3.602401 3.011868 0.07204801 ## 2 152.0381 3.116099 3.040762 0.06232198 ## 3 150.7404 3.424626 3.014808 0.06849252 ## 4 142.6571 3.292849 2.853143 0.06585699 ## 5 134.0000 3.497720 2.680000 0.06995439 This script should work for you too. Update your existing script with the code above and check that you get the same output as that above for the example dataset. Did you notice that I used the write.csv() in the penultimate line? This means that you should now see a file called summary_table.csv in your Files window. You should import this file into a new Excel workbook, format the numbers to a sensible precision, style it how you like, and then cut and paste it into your MegaMush report template. Don’t forget to include a table caption according to the unbreakable rules stated in Chatper 1. 5.3 Histogram Let’s complete the second task on our list: Create a histogram showing the distribution of estimated viable bacterial count, VBC from across all samples. Create a new script file, copy and paste the code below and run the whole file (click the Source button). You should see something like that in figure 5.1 in your Plots window. # Your R code for generating the histogram # Load the data data &lt;- read.csv(&quot;compost_999.csv&quot;) # Calculate the conversion factor factor &lt;- 2e5 / 1e7 # Calculate VBC column data$vbc &lt;- data$Viable.counts * factor # Calculate the mean VBC vbc_mean &lt;- mean(data$vbc) # Generate a simple histogram for all &quot;VBC&quot; hist(data$vbc, breaks = 20, main = &quot;&quot;, xlab = expression(VBC ~ &quot;(&quot; ~ x10^7 ~ cfu/g ~ &quot;)&quot;), ylab = &quot;Frequency&quot;, col = &quot;lightblue&quot;, border = &quot;black&quot;) # Add a vertical line for the mean abline(v = vbc_mean, col = &quot;red&quot;, lwd = 2, lty = 2) # I like a box around my figures box() Figure 5.1: Histogram of VBC of samples taken from all sites over a period of 1 year. The mean VBC (dotted red line) is \\(2.9 \\times 10^7\\) cfu/g. Let’s break this code down comment by comment: Load the Data data &lt;- read.csv(&quot;compost_999.csv&quot;) Purpose: This line uses the read.csv() function to read in a CSV file named “compost_999.csv” and stores its content in a data frame variable called data Calculate the Conversion Factor factor &lt;- 2e5 / 1e7 Purpose: As for the summary table in the section above, this line calculates a conversion factor to convert viable counts into VBC in units of \\(\\times 10^7\\) cfu/g. Scientific Notation: 2e5 means \\(2 \\times 10^5\\) and 1e7 means \\(1 \\times 10^7\\). Why divide by 1e7?: So that I can quote all VBC values in standard form, e.g. \\(1.2 \\times 10^7\\) cfu/g. Calculate VBC column: data$vbc &lt;- data$Viable.counts * factor Purpose: This line creates a new column vbc in the data data frame. It calculates VBM by multiplying the Viable.counts column by the conversion factor. New Column: data$vbc stores the calculated VBC for each row in the data frame. Calculate the mean VBC vbc_mean &lt;- mean(data$vbc) Purpose: This line computes the mean of the values in the vbc column and stores it to a new variable called vbc_mean. I’ll use this later shortly when I make my histrogram plot. Generate a Simple Histogram for all VBC hist(data$vbc, breaks = 20, main = &quot;&quot;, xlab = expression(VBC ~ &quot;(&quot; ~ x10^7 ~ cfu/g ~ &quot;)&quot;), ylab = &quot;Frequency&quot;, col = &quot;lightblue&quot;, border = &quot;black&quot;) Purpose: This block generates a histogram of the vbc values. Parameters: breaks = 20: Specifies the number of bins (bars) in the histogram. main = \"\": Prevents a title from showing (I’ll use a caption later when I import the image to a document). xlab: Customises the x-label. The expression() function is used to format it, which displays the label as \\(VBC (\\times 10^7 cfu/g)\\). ylab = Frequency: Sets the y-axis label to “Frequency”. col = \"lightblue\": Sets the color of the bars in the histogram to light blue. border = \"black\": Sets the border color of the bars to black Add a vertical line for the mean abline(v = vbc_mean, col = &quot;red&quot;, lwd = 2, lty = 2) Purpose: This line adds a vertical dashed line to the histogram at the mean value vbc_mean. Parameters: v = vbc_mean: Specifies the x-coordinate where the line is drawn (at the mean value). col = red: Sets the color of the line to red. lwd = 2: Sets the line width to 2 (give a thicker line than the default). lty = 2: Specifies the line type as dashed. I like a box around my histogram box() Purpose: This command make sure there is a full box around the histogram. By default, only the x and y axes will show otherwise. How useful is this histogram? Honestly? Not very. If you were hoping for a nice bell-curve (normal) distribution then you will be disappointed. All the histogram really tells us is that values of VBC are broadly distributed in the range \\(2.5 - 3.2 \\times 10^7\\) cfu/g and that there seems to be a slight “skew” towards higher values in the range. Oh well, at least you now know how to make a histogram using R in the future. Perhaps we could learn a bit more from our second type of visualisation: The Grouped Boxplot. Feedback Please! I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 5.4 Grouped Boxplot I love boxplots. There, I said it. They offer such a great way to visualise how your data is distributed, highlighting clear statistics like the median, quartiles, and potential outliers. But the real clincher is the their ability to compare multiple groups within your dataset at once to let you see if something interesting is going on between them. Take a look at the script below. Start a new script, copy/paste the code and run all lines. You should see something similar to that shown in figure 5.2 appear. # Your R code for generating a grouped boxplot. # Load the data data &lt;- read.csv(&quot;compost_999.csv&quot;) # Calculate the conversion factor factor &lt;- 2e5 / 1e7 # Calculate VBC column data$vbc &lt;- data$Viable.counts * factor # Adjust the margins to prevent y-axis label from being cut off par(mar = c(5, 5, 4, 2)) # Increase the second value for the left margin # Generate a grouped boxplot for VBC and group by location boxplot(vbc ~ Location, data = data, main = &quot;&quot;, xlab = &quot;Location&quot;, ylab = expression(VBC ~ &quot;(&quot; ~ x10^7 ~ cfu/g ~ &quot;)&quot;), col = &quot;lightblue&quot;, border = &quot;black&quot;, cex.axis = 0.8) Figure 5.2: Viable bacterial counts (VBC) across five production sites. Rotterdam and Utrecht exhibit noticeably lower median VBC values compared to the other sites, indicating potential issues at these locations that may require further investigation. You’ll notice that the first three lines of code are identical to the previous histogram example. Let’s break down the boxplot function. Adjust the margins to prevent y-axis label superscripts from being cut off par(mar = c(5, 5, 4, 2)) # Increase the second value for left margin Purpose: Annoyingly, if I use any label on the y-axis that has superscript elements then the default plot area in R isn’t quite big enough and they get cut off. The par(mar = c(bottom, left, top, right)) function adjusts the margins of the plot. By using a value of 5 for the second value (left margin), I’m ensuring that the y-axis is fully visible. Generate a grouped boxplot for VBC and group by location boxplot(vbc ~ Location, data = data, main = &quot;&quot;, ylab = &quot;Location&quot;, xlab = expression(VBC ~ &quot;(&quot; ~ x10^7 ~ cfu/g ~ &quot;)&quot;), col = &quot;lightblue&quot;, border = &quot;black&quot;, cex.axis = 0.8) Purpose: Parameters: vbc ~ Location: This specifies the formula for the boxplot. It creates boxplots of the vbc column data and groups it by the Location factor. data = data: The data argument specifies the data frame containing the variables in the above formula. cex.axis = 0.8: This adjusts the size of the boxplot label text, making it 80% of the default size. I like to do this as it improves readability. Interpreting the Boxplot For a reminder of the important features of a boxplot please refer to Anatomy of a Boxplot section in Chapter 2. The key comparative insight gained from the boxplot is that there is a noticeable trend of lower VBC values at the Rotterdam and Utrecht sites. Whether this trend is statistically significant and the reasons behind the lower values at these two locations remain to be determined. However, if I were the CTO of MegaMush and you presented me with this boxplot, I would likely be making urgent phone calls to my colleagues at the Rotterdam and Utrecht sites to investigate the underlying causes of these discrepancies. I hope you’ll agree that, unlike the histogram, which provides a general overview of data distribution, the boxplot offers a concise summary of central tendency and variability, making it easier to identify specific trends and outliers across different groups. This clarity is particularly valuable for decision-making, as it allows us to quickly pinpoint areas that require further exploration or intervention. Ideas for Further Exploration Boxplot to show distribution of Temperature values grouped by Location. Boxplot to show distribution of Moisture values grouped by Location. Boxplots of VBC/Temperature/Moisture grouped by DayOfWeek 5.5 Scatterplot The last thing on our todo list is a scatterplot. Remember, you’ve been asked to plot the relationship between Temperature and VBC. I also want to colour code the points by location. The code below shows how I’d do this with native R. Figure 5.3 shows your expected output. # Your R code for generating a scatterplot. # Load the data data &lt;- read.csv(&quot;compost_999.csv&quot;) # Calculate the conversion factor factor &lt;- 2e5 / 1e7 # Calculate VBC column data$vbc &lt;- data$Viable.counts * factor # Adjust the margins to prevent y-axis label from being cut off par(mar = c(5, 5, 4, 2)) # Increase the second value for the left margin # Define unique locations and corresponding colors unique_locations &lt;- sort(unique(data$Location)) colors &lt;- rainbow(length(unique_locations), alpha=0.8) # Or use your predefined colors # Create a scatterplot of VBC vs. temperature, colored by location plot(data$Temperature, data$vbc, main = &quot;&quot;, xlab = &quot;Temperature (°C)&quot;, ylab = expression(VBC ~ &quot;(&quot; ~ x10^7 ~ cfu/g ~ &quot;)&quot;), pch = 19, # solid circles for points cex = 1.5, # increase point size col = colors[as.numeric(factor(data$Location, levels = unique_locations))]) # color by location with transparency # Add a legend without a border legend(&quot;bottomleft&quot;, legend = unique_locations, # Use the unique locations col = colors, # Ensure the correct color mapping pch = 19, # same symbol as in the plot bty = &quot;n&quot;, # no box around the legend pt.cex = 1.5) # match point size in the legend # Add faint gridlines grid(col = &quot;gray90&quot;, lty = &quot;dotted&quot;) Figure 5.3: Viable bacterial counts (VBC) vs Temperature. A strong negative correlation is observed. Points are colour coded according to location (see key). Create a new script file in your existing project, copy/paste the code above and then run as source. Hopefully, you should be able to recreate the scatterplot in your plots window. The first three lines of code are the same as for our boxplot example. Let’s break down the rest comment-by-comment: Define Unique Locations and Colors: unique_locations &lt;- sort(unique(data$Location)) colors &lt;- rainbow(length(unique_locations), alpha = 0.8) # Or use your predefined colors Purpose: to extract the unique production site locations from the data, sort them alphabetically, and assign each location a distinct colour for visualisation. Explanation: unique(data$Location): finds all distinct location values (e.g., Amsterdam, Groningen, etc …). sort(): ensures the locations are listed in alphabetical order, which will also affect how they are represented in the legend. rainbow(length(unique), alpha =0.8): assigns a unique color to each location, creating a colour palette based on the number of unique locations. The alpha = 0.8 parameter makes the points slightly transparent. Create a Scatterplot of VBC vs. Temperature, coloured by Location: plot(data$Temperature, data$vbc, main = &quot;&quot;, xlab = &quot;Temperature (°C)&quot;, ylab = expression(VBC ~ &quot;(&quot; ~ x10^7 ~ cfu/g ~ &quot;)&quot;), pch = 19, # solid circles for points cex = 1.5, # increase point size col = colors[as.numeric(factor(data$Location, levels = unique_locations))]) Purpose: To create a scatterplot that visualises the relationship between temperature and VBC, with points coloured according to the location of the data. Explanation: plot(data$Temperature, data$vbc) creates a basic scatterplot of VBC values (on the y-axis) versus temperature (on the x-axis). xlab and ylab provide the axis labels. pch = 19 specifies solid circular points for the scatterplot. cex = 1.5 increases the size of points for better visibility. factor(data$Location, levels = unique_locations) ensures that the points are coloured by location in the correct, alphabetically sorted order. The as.numeric() converts the factor levels into numeric indices to match with the colors vector. Add a Legend: legend(&quot;bottomleft&quot;, legend = unique_locations, # Use the unique locations col = colors, # Ensure the correct color mapping pch = 19, # same symbol as in the plot bty = &quot;n&quot;, # no box around the legend pt.cex = 1.5) Purpose: To add a legend to the plot that identifies the colours representing each location, ensuring the viewer can easily distinguish between the locations. Explanation: bottom-left determines the location of the legend on your plot. legend = unique_locations ensures the correct, alphabetically sorted location names are shown in the legend. col = colors maps the same colours used in the plot to the corresponding locations in the legend. pch = 19 matches the symbol style of the points in the plot with those in the legend. bty = \"n\" removes the default box around the legend to keep the visual clean. pt.cex = 1.5 adjusts the size of the symbols in the legend to match this size of the points in the scatterplot. Add Gridlines: grid(col = &quot;gray90&quot;, lty = &quot;dotted&quot;) Purpose: Pretty self-explanitory this one I think. I like to have gridlines on my scatterplot as a guide to for the eye, but I leave it as optional for you. Interpreting the Scatterplot There’s lots of information radiating out of this plot. Let’s interpret what we’re seeing: Overall Trend: A clear negative correlation between temperature and VBC. Utrecht (purple): Highest temperatures (65 - 75oC) and lowest VBC values. Rotterdam (blue): Slightly lower VBC values for the same range of temperatures as Amsterdam and Groningen. This suggests that there may be another factor affecting VBC at Rotterdam. After seeing this scatterplot I’d be advising the Utrecht site that they were “running hot!” and that they need to cool their compost by about 15oC in order to bring their VBCs back in line with the other sites. Ideas for Further Exploration Scatterplot of VBC vs Moisture Scatterplot of Moisture vs Temperature. 5.6 Just for Fun In this last sub-section I just wanted to show you something really cool. The code below uses the fantastlic Plotly library to generate a 3D scatterplot so that I can see the impact of both Temperature and Moisture on VBC at the same time. Plotly is not included in your RStudio by default, you’ll need to install it first. Go to your Console window and type: install.packages(&quot;plotly&quot;) Hit enter and then wait while the Plotly package is installed. Then start a new script file, copy/past the script below and run it as source. # Include the plotly library in your environment library(plotly) # Load the data data &lt;- read.csv(&quot;compost_999.csv&quot;) # Calculate the conversion factor for cfu per gram factor &lt;- 2e5 / 1e7 data$vbc &lt;- data$Viable.counts * factor # Create an interactive 3D scatterplot fig &lt;- plot_ly(data = data, x = ~Temperature, y = ~Moisture, z = ~vbc, color = ~Location, colors = &quot;Set2&quot;, # Choose a color palette type = &#39;scatter3d&#39;, mode = &#39;markers&#39;, marker = list(size = 5)) %&gt;% layout(scene = list(xaxis = list(title = &#39;Temperature (°C)&#39;), zaxis = list(title = &#39;VBC&#39;), yaxis = list(title = &#39;Moisture (%)&#39;))) # Show the figure fig Figure 5.4: A 3D scatterplot showing Viable bacterial counts (VBC) vs both Temperature AND Moisture using the fantastic Plotly package. How cool is that? You have an interactive, 3D graph that you can fiddle with and investigate the relationship between VBC, Temperature and Moisture. 5.7 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s summative QS assignments. There is no formative assignment this week! You should aim to complete the summative assignment before the end of the online workshop that corresponds to this section’s content. You can attempt the summative assignment only once. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["statistics-in-r-part-i.html", "6 Statistics in R: Part I 6.1 One-Way ANOVA in R 6.2 Two-way ANOVA in R", " 6 Statistics in R: Part I If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. The aim of this chapter is to show that there are often several ways to approach data analysis. Here, we’ll use R to perform tasks we’ve previously done in Excel. Why make the switch? While Excel is widely used (and is still extremely useful), R can greatly reduce the need for manual data transformations that Excel often requires, making data handling much easier and more efficient. Also, creating R scripts provides a clear, reproducible record of your analysis which is incredibly important for anyone who wishes to repeat your analysis and confirm your results. Key learning objectives for this chapter include: Reading data direct from the web using R. Organising and cleaning your data in R ready for analysis. Performing a One-Way ANOVA and post-hoc tests in R. Performing a Two-Way ANOVA and post-hoc tests in R. Visualising relationships between two independent variables and a dependent one. 6.1 One-Way ANOVA in R This section is almost a complete re-run of section 2.2. Except what we did in Excel, we’ll learn to do in R. I strongly recommend that you have a quick scroll through the whole of section 2.2 right now to refresh your memory. Seriously, do it. 6.1.1 Importing Data from an Online Source Before we begin, did you know that the humble read.csv() function can also read a CSV file from a URL, making it extremely easy to work with datasets stored online. The code below imports a CSV file containing zebrafish data, which includes information about the length of zebrafish embryos under various concentrations of alcohol. The read.csv() function fetches the data from the provided URL, which points to a raw GitHub file. Create a new R project for this week and start a new script file. Copy the following lines the the file and run them. # Read data direct from the web! zebrafish_data &lt;- read.csv(&quot;https://raw.githubusercontent.com/rtreharne/qs/refs/heads/main/data/02/zebrafish_999.csv&quot;) # Convert the conc_pc column into a factor. IMPORTANT! zebrafish_data$conc_pc &lt;- as.factor(zebrafish_data$conc_pc) # Inspect the data head(zebrafish_data) ## id conc_pc length_micron ## 1 2146 0 2103.4 ## 2 3853 0 2703.3 ## 3 2313 2 2279.5 ## 4 1638 2.5 2020.8 ## 5 1669 2.5 1708.4 ## 6 2644 2.5 2256.3 After importing the data, we convert the conc_pc column (which represents alcohol concentration percentages) into a factor variable using as.factor(). This is important because factors are categorical variables in R, and we’ll treat them as such in our analysis. Note that if you don’t perform this step then you’ll get errors when trying to do ANOVA. Finally, the head() function is used to display the first few rows of the data, giving us an overview of its structure. This is a common way to check that the data has been loaded correctly and to understand the initial format of the dataset. By importing data directly from an online source, you can quickly access and analyse datasets without needing to manually download and load files, streamlining your workflow. You’re welcome. 6.1.2 Summarising (Descriptive Statistics) The code below will calculate descriptive statistics for the zebrafish dataset to summarise the length of the zebrafish embryos (length_micron) across different alcohol concentrations (conc_pc). Remember, descriptive statistics provide a quick overview of the central tendency and variability within the data, helping us understand patterns and outliers. # This code extends the code block above. # Calculate summary statistics for each group zebrafish_summary &lt;- aggregate(length_micron ~ conc_pc, zebrafish_data, function(x) c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x))) # Convert the list to data frame zebrafish_summary &lt;- do.call(data.frame, zebrafish_summary) # Inspect the summary print(zebrafish_summary) ## conc_pc length_micron.Mean length_micron.Median length_micron.Max ## 1 0 2410.874 2451.15 2890.1 ## 2 1.5 2405.318 2397.05 2975.6 ## 3 2 2300.736 2279.50 2666.3 ## 4 2.5 2105.539 2157.30 2692.1 ## length_micron.Min length_micron.SD ## 1 1859.5 251.2151 ## 2 1962.4 255.3249 ## 3 1746.3 206.8577 ## 4 1548.2 266.6929 # Save to file write.csv(zebrafish_summary, file = &quot;zebrafish_summary.csv&quot;, row.names = FALSE) We begin by using the aggregate() function, which allows us to group the data by alcohol concentration (conc_pc) and calculate several statistics for each group: the mean, median, maximum, minimum, and standard deviation (SD). These metrics help us understand the typical value (mean and median) and the spread (max, min, SD) within each concentration level. Once the summary statistics have been calculated, we convert the output from the aggregate() function (which is in list form) into a more manageable data frame format using the do.call(data.frame, ...) function. This makes it easier to inspect and manipulate the results. To examine the summary statistics, we print() the zebrafish_summary dataframe. This provides an organised view of the key statistics for each group in the dataset. Finally, it is a good practice to save your results to a file. In this case, we save the zebrafish_summary dataframe to a CSV file, allowing us to easily store, share, or reload the data for further analysis. The write.csv() function does this, ensuring the summary statistics are saved in a structured format, without row names, for easy use in future work. As before, you can import this .csv into Excel to style it for a report. 6.1.3 Grouped Boxplots in R As before, the best way to visualise our dataset is using a grouped boxplot. Can you remember how fiddly this was in Excel? We had to create a pivot table before we could make a plot - yuck. Not so in R! The code below let’s us get the job done quickly, without faffing to produce the plot shown in figure 6.1. # This code extends the code blocks above. # Generate a grouped boxplot boxplot( length_micron ~ conc_pc, data = zebrafish_data, main = &quot;&quot;, xlab = &quot;Alcohol Conc. (%)&quot;, ylab = &quot;Embryo Length (microns)&quot;, col = &quot;lightblue&quot; ) Figure 6.1: Distribution of Zebrafish embryo lengths organised by Alcohol treaments. My interpretation of the plot is exactly the same as that discussed in section 2.2.1. 6.1.4 Conducting a One-Way ANOVA We are about to perform a One-Way Analysis of Variance (ANOVA) in R to examine whether there is a statistically significant difference in zebrafish embryo lengths across different alcohol concentrations. ANOVA allows us to compare the means of more than two groups to determine if any of the group means are different from each other. The code below does the job in a single line. Beat that Excel! # This code extends the code blocks above. # Perform a One-Way ANOVA. zebrafish_aov &lt;- aov(length_micron ~ conc_pc, data = zebrafish_data) # See the key results. summary(zebrafish_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## conc_pc 3 2485048 828349 13.64 5.97e-08 *** ## Residuals 156 9470556 60709 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We use the aov() function in R, where length_micron is the dependent variable (embryo length) and conc_pc is the independent variable (alcohol concentration). The function generates an ANOVA table that summarises the results of the test. The output from the summary() function shows the following key components: Df (Degrees of Freedom): This indicates how many independent pieces of information are used to estimate the variance. For conc_pc, the degrees of freedom is 3, as there are four levels of alcohol concentration. The residual degrees of freedom (156) corresponds to the leftover variation after accounting for the groups. Sum Sq (Sum of Squares): This value represents the total variability in the data. The conc_pc group has a sum of squares of 2,485,048, indicating a considerable amount of variation between the different alcohol concentrations. Mean Sq (Mean Square): The mean square is the sum of squares divided by the corresponding degrees of freedom. This gives us an estimate of the variance within each group. The mean square for conc_pc is 828,349. F value: Sometimes referred to as the “F Statistic”, this is the ratio of the mean square of the group (between-group variation) to the mean square of the residuals (within-group variation). A large F value suggests that the group means are likely different. In this case, the F value is 13.64, which is quite large. Pr(&gt;F) (p-value): This is the probability of observing an F value as extreme as the one calculated, under the null hypothesis that all group means are equal. A small p-value indicates strong evidence against the null hypothesis. In our case, the p-value is 5.97e-08, which is very small (much smaller than 0.05), suggesting that there is a statistically significant difference between the groups. The significance codes at the bottom of the output indicate the level of significance: here, *** means a highly significant result. Interpretation: Since the p-value is extremely small (much less than 0.05), we reject the null hypothesis that all group means are equal. This means there is a statistically significant difference in embryo lengths across the different alcohol concentrations. This result suggests that the alcohol concentration has a notable effect on the growth of the zebrafish embryos. In the next step, we will conduct post-hoc tests to further investigate which specific groups differ from each other. 6.1.5 Post-Hoc Tests As discussed previously in section 2.2.4, after running an ANOVA, a significant result tells us that there is at least one difference among the group means, but it doesn’t specify which pairs of groups differ. To identify the specific group pairs with significant differences, we use post-hoc tests. In this section, we apply Tukey’s Honest Significant Difference (HSD) test to perform pairwise comparisons and determine which group means differ significantly. In R, we can do this be extending our previuos code with two simple lines: # This code extends the code blocks above. # Perform TukeyHSD tukey_result &lt;- TukeyHSD(zebrafish_aov) # View the results print(tukey_result) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = length_micron ~ conc_pc, data = zebrafish_data) ## ## $conc_pc ## diff lwr upr p adj ## 1.5-0 -5.555388 -148.8123 137.70156 0.9996328 ## 2-0 -110.137912 -252.4274 32.15154 0.1886631 ## 2.5-0 -305.334785 -445.8132 -164.85636 0.0000005 ## 2-1.5 -104.582524 -250.4332 41.26814 0.2487027 ## 2.5-1.5 -299.779397 -443.8638 -155.69500 0.0000014 ## 2.5-2 -195.196873 -338.3194 -52.07438 0.0029202 Interpreting the Output Comparisons: Each row corresponds to a comparison between two groups (e.g., 2.5-0). Mean Difference: The diff column displays the difference in means between groups. Confidence Interval: The lwr and upr columns give the 95% confidence interval for each difference. Adjusted p-value: The p adj column provides the p-values adjusted for multiple comparisons (like for our Bonferonni correction). In the output, pairs with small p-values (e.g., 2.5-0 with a p-value of 0.0000005) indicate significant differences in group means, confirming which specific pairs show statistically significant differences. In our case, there is a signficant difference between the conc_pc groups: 2.5-0, 2.5-1.5 and 2.5-2. Who Was Tukey? The TukeyHSD() test is named after John Tukey, a key figure in 20th-century statistics known for his contributions to data analysis and multiple comparison techniques. Interestingly, while collaborating with Claude Shannon on information theory, Tukey coined the term “bit” (short for binary digit), leaving a remarkable legacy that spans statistics and computer science. Tukey’s Honest Significant Difference (HSD) test is a post-hoc analysis specifically designed for use after an ANOVA. It helps to identify significant pairwise group differences while controlling for the family-wise error rate (the likelihood of at least one false positive across all comparisons). Tukey’s HSD is widely favoured because it strikes a balance between controlling the family-wise error rate and maintaining sensitivity to detect real differences. By contrast, Bonferroni-corrected t-tests (what we used in Excel in section @ref{post-hoc-tests}) offer a more conservative approach, dividing the significance level by the number of comparisons, which can reduce the power of the test. While Bonferroni is useful in highly exploratory settings with numerous comparisons, Tukey’s HSD is particularly effective for balanced designs in ANOVA. In summary, Tukey’s HSD test provides a clear, statistically sound approach to determine specific differences between groups, supporting a rigorous interpretation of ANOVA results. 6.2 Two-way ANOVA in R In this section, we’ll explore how to perform a two-way ANOVA in R to evaluate the combined effects of two independent variables on a single outcome variable. For this example, imagine you are overseeing a series of experiments to understand how both pH level and temperature impact the activity of an enzyme. Data from ten independent groups has been consolidated to enable a comprehensive analysis. It’s helpful to clarify the difference between one-way and two-way ANOVA. In a one-way ANOVA, we examine the effect of a single independent variable (or factor) on the dependent variable, allowing us to determine if there is a statistically significant difference in the outcome across different levels of that factor. However, a two-way ANOVA expands on this by allowing us to investigate two independent variables simultaneously. This approach not only assesses the effect of each factor independently (the “main effects”) but also explores the interaction effect — whether the influence of one factor on the outcome variable changes depending on the level of the other factor. For instance, in our scenario, a two-way ANOVA allows us to answer questions like: Does pH level alone significantly affect enzyme activity? Does temperature alone have a significant impact? Most importantly, does the effect of pH on enzyme activity differ depending on the temperature? Understanding these interactions is critical when both factors might influence the outcome in a combined or conditional manner, making two-way ANOVA a powerful tool for more complex experimental designs. With this foundation, we can proceed to import and prepare the data for analysis. 6.2.1 Importing and Cleaning the Data Start a new R script and copy/paste the code block below to read the example dataset to a variable called activity_data. activity_data &lt;- read.csv(&quot;https://raw.githubusercontent.com/rtreharne/qs/main/data/08/activity_999.csv&quot;) head(activity_data) ## group ph temperature activity ## 1 10 7 20 0.0448 ## 2 5 7 20 ## 3 9 9 40 0.0558 ## 4 9 7 20 ERROR ## 5 4 9 20 -1 ## 6 7 5 30 0.0423 The dataset includes some key columns: group: Identifier for each independent experimental group. ph: pH levels at which the enzyme activity was measured. temperature: Temperature at which the enzyme activity was measured. activity: The values in this column actually refer to the measured changes in absorbance of samples (using a spectrophotometer) over a period of time (typically minutes) - units: \\(\\Delta A/min\\). Since ph and temperature are categorical factors, we need to ensure they are correctly set as factors in R, otherwise our ANOVA won’t work. Let’s extend our code as follows: # This code extends the code blocks above. # Set ph and temperature as factors activity_data$ph &lt;- as.factor(activity_data$ph) activity_data$temperature &lt;- as.factor(activity_data$temperature) We also see that some rows have the value “ERROR” or -1. We need to “clean” our dataset to ensure that all rows with these values in the activity column are removed. We can do that by extending our script with the following: # This code extends the code blocks above. # Make activity column numeric. activity_data$activity &lt;- as.numeric(activity_data$activity) ## Warning: NAs introduced by coercion # Remove any rows with NA values. activity_data &lt;- na.omit(activity_data) # Insist that all values in activity column are positive. An example of subsetting with a conditional statement! activity_data &lt;- activity_data[activity_data$activity &gt;= 0,] # Re-inspect the data head(activity_data) ## group ph temperature activity ## 1 10 7 20 0.0448 ## 3 9 9 40 0.0558 ## 6 7 5 30 0.0423 ## 9 3 7 40 0.0418 ## 11 8 5 20 0.0441 ## 12 7 9 20 0.0256 Great! Now we should only have numerical data in our activity column. Before we learn how to perform a two-way ANOVA let’s try and visualise our data. 6.2.2 Visualising the data Visualising data, before performing statistical tests, offers immediate insights into patterns, trends, and any anomalies that could impact analysis. It helps assess assumptions, identify outliers, and reveal interactions between variables, providing a foundation for selecting the right tests and ensuring accurate results. Although we know we’ll be performing a two-way ANOVA in this case, it’s often not always clear which test is best suited, or if assumptions have been met, until some preliminary visualisation has been carried out. In this case, our good old friend the grouped boxplot might not be very helful. Boxplots are great at comparing distributions across levels of a single variable, but they’re limited when trying to visualise interactions between two factors. Instead, we can turn to a couple of alternatives that let us display the combined effect of tow factors on a dependent variable. 1. Interaction Plot In an interaction plot, the levels of one variable are typically plotted on the x-axis, and the response variable is plotted on the y-axis. Different lines are drawn for each level of the second variable, and the lines’ slopes or patterns reveal if and how the variables interact. Parallel lines suggest no interaction between the variables. Non-parallel lines indicate that the effect of one variable depends on the level of the other, showing an interaction. Extend your script with the lines of code and run them to generate the plot shown in figure 6.2 # This code extends the code blocks above. # Interaction plot of pH and Temperature on enzyme activity interaction.plot( activity_data$temperature, activity_data$ph, activity_data$activity, col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), lwd = 2, xlab = &quot;Temperature (°C)&quot;, ylab = expression(&quot;Activity (&quot;*Delta*&quot;A/min)&quot;), legend=FALSE ) # Legend placement and formatting legend( &quot;top&quot;, legend = levels(activity_data$ph), col = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), lty = c(3, 2, 1), lwd = 2, title = &quot;pH Levels&quot;, bty= &quot;n&quot; ) Let’s break this code down so that we understand it comment by comment: interaction.plot(): This function creates the main plot showing how enzyme activity changes with temperature and pH. Colours (col = c(\"red\", \"green\", \"blue\")) are used to distinguish between the different pH levels. I’ve turned the default legend off using legend = False, because it’s not as beautiful as I want it to be. legend(): This function adds a custom legend at the top of the plot (\"top\") where I think it looks best and doesn’t interfere with any of the plotted lines. The legend items are based on the levels of the ph factor. The line types (lty = c(3, 2, 1)) specify different styles (dotted, dashed and solid) to correspond tho the lines styles generated automatically by interaction.plot(). The lwd = 2 argument ensures the width of the lines in the legend match the line width used in the plot. The bty = \"n\" argument removes the box around the legend (this is optional, but I don’t like boxes around my legends because I’m fussy). Figure 6.2: Interaction plot showing the effect of temperature and pH on enzyme activity. How should we iterpret what is going on in figure 6.2? pH = 5: The trend shows that the enzyme activity decreases as temperature increases and that activity is higher at 20 °C than for a pH of 7 and 9. pH = 9: The trend shows that the enzyme activity increases as temperature increases and that activity is higher at 40 °C than for a pH of 7 and 5. The lines for pH levels 5 and 9 cross each other, indicating an interaction between temperature and pH. I.e. the effect of temperature on enzyme activity changes depending on the pH level. pH = 7: there is little to no observable dependence of enzyme activity on temperature. 2. Heatmap Another way of visualising the above is with a heat map. Try appending the following code to your script and running the lines to get the output shown in figure 6.3. # This code extends the code blocks above. # You&#39;ll need the reshape2 library for this one. You might need to install if first. library(reshape2) # Reshape the data for heatmap visualization matrix &lt;- acast(activity_data, ph ~ temperature, value.var = &quot;activity&quot;, mean) # Create a white-red color palette color_palette &lt;- colorRampPalette(c(&quot;white&quot;, &quot;red&quot;))(256) # Create heatmap with blue-red colors and color key heatmap(matrix, Rowv = NA, Colv = NA, scale = &quot;none&quot;, col = color_palette, # Use the blue-red color palette xlab = &quot;Temperature (°C)&quot;, ylab = &quot;pH&quot;, margins = c(5, 5), # Adjust margins if needed ) Figure 6.3: Heat map showing the interaction between temperature and pH on enzyme activity. Low activity is indicated by white and high activity by red. If you can’t get this code to run then don’t worry, it’s not critically important (it won’t affect your ability to complete this week’s summative task). I just wanted to highlight that when it comes to visualisation in R, you’ve got lots of options. If you really want to understand what the code is doing then you can see what my good friend ChatGPT has to say about it. I’d interpret the heatmap in figure 6.3 in exactly the same way as the interaction plot. One big disadvantage however, is that it only shows the relative changes in activity - I’d need to spend a bit of time figuring out how to put a colorbar key at the side of the plot to indicate activity values with respect to colour. Right now though, I think we’d really better getting to the important bit though: Finally! A Two-way ANOVA. 6.2.3 Conducting a Two-Way ANOVA Extend your existing script with the code below. # This code extends the code block above. # Perform a two-way ANOVA and save to a variable called &quot;anova_result&quot; anova_result &lt;- aov(activity ~ ph * temperature, data = activity_data) # View a summary of the ANOVA summary(anova_result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ph 2 0.000957 0.000479 25.05 1.21e-10 *** ## temperature 2 0.000490 0.000245 12.82 5.00e-06 *** ## ph:temperature 4 0.016283 0.004071 213.10 &lt; 2e-16 *** ## Residuals 250 0.004776 0.000019 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And that’s it! You’ll notice that performing a two-way ANOVA in R is almost exactly the same as performing a one-way ANOVA. The critical difference is formula we use in the aov(): activity ~ ph * temperature. We’ve included both the factors (ph and temperatures) we’re interested in within the formula separated (confusingly perhaps!) with a * symbol. The summary table is similar to what we would expect to see for a one-way ANOVA, but it’s got a few extra rows which we should discuss: Main Effects (pH and Temperature): - Each factor - pH and Temperature - has its own row in the table. the F values and p-values for each main effect tell us if that factor alone has a statistically significant impact on enzyme activity. - In our case, both pH and Temperature have p-values well below the 0.05 threshold. This indicates that both pH and Temperature have a strong effect on enzyme activity. Interaction effects (pH:Temperature): - The interaction term, shown as ph:temperature, tells us if the effect of one factor depends on the level of the other factor. - In our case, the interaction effect is highly significant (p &lt; 0.001). this suggests that the impact of pH on enzyme activity changes depending on the temperature level, and vice versa. 6.2.4 Post-Hoc tests We’re on the home stretch now. Now we’ve firmly established that there is some significant differences between the means of the groups we can proceed to perform our TukeyHSD(): # Perform Tukey&#39;s HSD test tukey_result &lt;- TukeyHSD(anova_result) print(tukey_result) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = activity ~ ph * temperature, data = activity_data) ## ## $ph ## diff lwr upr p adj ## 7-5 0.0044183644 0.002823274 0.0060134552 0.0000000 ## 9-5 0.0037190537 0.002168714 0.0052693932 0.0000001 ## 9-7 -0.0006993107 -0.002264321 0.0008657001 0.5438710 ## ## $temperature ## diff lwr upr p adj ## 30-20 -0.0002284719 -0.001817209 0.001360265 0.9386138 ## 40-20 0.0027577790 0.001169042 0.004346516 0.0001698 ## 40-30 0.0029862509 0.001450088 0.004522414 0.0000215 ## ## $`ph:temperature` ## diff lwr upr p adj ## 7:20-5:20 -0.0104350631 -0.0141988811 -0.006671245 0.0000000 ## 9:20-5:20 -0.0252768696 -0.0292283805 -0.021325359 0.0000000 ## 5:30-5:20 -0.0169983696 -0.0207370550 -0.013259684 0.0000000 ## 7:30-5:20 -0.0091688696 -0.0131203805 -0.005217359 0.0000000 ## 9:30-5:20 -0.0110487484 -0.0147636697 -0.007333827 0.0000000 ## 5:40-5:20 -0.0218908696 -0.0256813118 -0.018100427 0.0000000 ## 7:40-5:20 -0.0093570234 -0.0132719529 -0.005442094 0.0000000 ## 9:40-5:20 0.0011508951 -0.0025415202 0.004843310 0.9877802 ## 9:20-7:20 -0.0148418065 -0.0185181802 -0.011165433 0.0000000 ## 5:30-7:20 -0.0065633065 -0.0100099068 -0.003116706 0.0000003 ## 7:30-7:20 0.0012661935 -0.0024101802 0.004942567 0.9769340 ## 9:30-7:20 -0.0006136852 -0.0040344930 0.002807122 0.9997549 ## 5:40-7:20 -0.0114558065 -0.0149584822 -0.007953131 0.0000000 ## 7:40-7:20 0.0010780397 -0.0025589863 0.004715066 0.9912213 ## 9:40-7:20 0.0115859583 0.0081896049 0.014982312 0.0000000 ## 5:30-9:20 0.0082785000 0.0046278607 0.011929139 0.0000000 ## 7:30-9:20 0.0161080000 0.0122396881 0.019976312 0.0000000 ## 9:30-9:20 0.0142281212 0.0106018230 0.017854419 0.0000000 ## 5:40-9:20 0.0033860000 -0.0003176267 0.007089627 0.1035482 ## 7:40-9:20 0.0159198462 0.0120889101 0.019750782 0.0000000 ## 9:40-9:20 0.0264277647 0.0228245260 0.030031003 0.0000000 ## 7:30-5:30 0.0078295000 0.0041788607 0.011480139 0.0000000 ## 9:30-5:30 0.0059496212 0.0025564856 0.009342757 0.0000036 ## 5:40-5:30 -0.0048925000 -0.0083681555 -0.001416845 0.0005270 ## 7:40-5:30 0.0076413462 0.0040303350 0.011252357 0.0000000 ## 9:40-5:30 0.0181492647 0.0147807844 0.021517745 0.0000000 ## 9:30-7:30 -0.0018798788 -0.0055061770 0.001746419 0.7918724 ## 5:40-7:30 -0.0127220000 -0.0164256267 -0.009018373 0.0000000 ## 7:40-7:30 -0.0001881538 -0.0040190899 0.003642782 1.0000000 ## 9:40-7:30 0.0103197647 0.0067165260 0.013923003 0.0000000 ## 5:40-9:30 -0.0108421212 -0.0142922013 -0.007392041 0.0000000 ## 7:40-9:30 0.0016917249 -0.0018946761 0.005278126 0.8656961 ## 9:40-9:30 0.0121996435 0.0088575587 0.015541728 0.0000000 ## 7:40-5:40 0.0125338462 0.0088692746 0.016198418 0.0000000 ## 9:40-5:40 0.0230417647 0.0196159301 0.026467599 0.0000000 ## 9:40-7:40 0.0105079186 0.0069448352 0.014071002 0.0000000 # You&#39;ll need these last two lines to complete your summative test this week # Get the interaction effects table and conver to a dataframe #tukey_df &lt;- as.data.frame(tukey_result[[&quot;ph:temperature&quot;]]) # Find the maximum absolute difference between the means of the ph:Temperature group-wise comparisons. #max(abs(tukey_df$diff)) Woah! That’s a lot of information. Let me try to summarise what’s going on here: 1. Main Effects: - pH: This part shows pairwise comparisons between pH levels: - 7-5: The difference in activity between pH 7 and 5 is significant (p adj &lt; 0.05), with a positive difference of 0.0044. - 9-5: Significant difference, indicating pH 9 activity is lower than pH 5. - 9-7: Not significant (p adj = 0.484), indicating similar activity between pH 9 and 7. - Temperature: This section compares activity across temperature levels: 40-20 and 40-30 both show significant differences, suggesting that activity at 40°C is notably different from 20°C and 30°C. 30-20 is not significant, indicating similar activity between 20°C and 30°C. 2. Interaction Effects (pH:Temperature): Each combination of pH and temperature levels is compared. For instance: 7:20-5:20: Activity at pH 7 and 20°C differs significantly from pH 5 and 20°C. 9:40-7:40: A significant difference indicates pH 9 at 40°C differs from pH 7 at 40°C. Many combinations show significant differences (p adj &lt; 0.05), especially involving pH 5 at 20°C. This likely reflects the complex interaction effects found in the ANOVA, showing how specific pH and temperature combinations impact activity differently. General Tips for Interpretation: Significance: Look for adjusted p-values (p adj &lt; 0.05) to identify significant differences. Positive or Negative Differences: The diff column tells you if one level has higher or lower activity relative to another. Interactions: Significant differences in the interaction effectspH:Temperature` table suggest that the combined effect of pH and temperature is crucial for understanding enzyme activity. "],["statistics-in-r-part-ii.html", "7 Statistics in R: Part II 7.1 Linear Regression in R 7.2 Multiple Linear Regression (MLR) in R.", " 7 Statistics in R: Part II If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. We’re going to revisit linear regression in this chapter. Our aim is to do what we were able to do with Excel in section 3.2, but with R. We’ll also extend our knowledge by learning how to perform a multiple linear regression (MLR), i.e. regression involving more than one independent variables. The key objectives for this chapter are as follows: Linearising data with logarithms. Performing a linear regression in R. Visualising linear relationships and adding trendlines in R. Predicting values using your regression models. Extending linear regression to multiple variables (MLR). You should have a quick scroll through section 3.2 again before you continue Yes, I mean right now. 7.1 Linear Regression in R To keep this week’s QS relevant to what you’ve been doing in the lab, I’ve contrived the following scenario: Because of your impressive data science and coding skills, you’ve been invited to join a research team investigating how cells survive under stress. Your challenge is to uncover the survival secrets of two yeast strains: the wild-type (WT) and a genetically modified mutant strain (3KO). In the lab, both strains were exposed to increasing doses of hydrogen peroxide (H₂O₂)—a chemical that causes oxidative stress by damaging DNA and other essential cellular components. Your task is to help the team figure out the LD50 for each strain — the dose that reduces survival by 50%. This isn’t just an academic exercise. Oxidative stress is a key factor in ageing and diseases like cancer, so understanding how cells respond to it could have far-reaching implications for science and medicine. Here’s your dataset containing the results of a dose-response assay. It has the following headers: dose (mM of H₂O₂): The level of oxidative stress applied. strain_type (WT or 3KO): The two strains being tested. counts_per_plate: The number of surviving colonies at each dose. Preliminary work has already established that at a zero dose (0 mM) of hydrogen peroxide, both yeast strains (WT and 3KO) have an average colony count of 300 colonies. Your challenge is to use linear regression in R to model the relationship between H₂O₂ dose and survival. By calculating and comparing the LD50 values for each strain, you’ll help the team uncover how genetic differences impact a cell’s ability to withstand damage. So, let’s put your QS skills to work! 7.1.1 Reading the data and calculating a survival column Create a new R project and start a new script file. Read your data to a new variable called yeast_data. Use the head() function on your data to have a sneak peak. This is what you should see. ## dose strain_type counts_per_plate ## 1 1.0 WT 270 ## 2 1.9 WT 237 ## 3 2.8 WT 200 ## 4 3.7 WT 190 ## 5 4.6 WT 168 ## 6 5.5 WT 159 I’d like to calculate another column containing the survival as a percentage for each row. Remember, the zero dose colony counts for both WT and 3KO strains was 300. I’m going to let you think about how do do this, but here’s what your head values should look like: ## dose strain_type counts_per_plate survival ## 1 1.0 WT 270 90.00000 ## 2 1.9 WT 237 79.00000 ## 3 2.8 WT 200 66.66667 ## 4 3.7 WT 190 63.33333 ## 5 4.6 WT 168 56.00000 ## 6 5.5 WT 159 53.00000 7.1.2 Visualising the data As ever, before deciding how to proceed with any further statistical analysis, it is good practice to visualise your data. Use the code below to generate a scatterplot that shows the dose-survival response for both yeast strains. You should see something similar to what is shown in figure 7.1. # Extend your existing code with the following: # Plot Dose vs Survival for Wild Type First plot(yeast_data$dose[yeast_data$strain_type == &quot;WT&quot;], yeast_data$survival[yeast_data$strain_type == &quot;WT&quot;], type = &quot;p&quot;, # &#39;p&#39; for points only pch = 16, xlab = expression(Dose~(mM~H[2]*O[2])), ylab = &quot;Survival (%)&quot;, ylim = c(0, 100) ) # Add points for Mutant Strain points(yeast_data$dose[yeast_data$strain_type == &quot;3KO&quot;], yeast_data$survival[yeast_data$strain_type == &quot;3KO&quot;], pch = 1) # Color for 3KO strain # Add the legend legend(&quot;topright&quot;, legend = c(&quot;WT&quot;, &quot;3KO&quot;), pch = c(16, 1), bty = &quot;n&quot;) Figure 7.1: Dose response curves for wilt type (WT) and mutant (3KO) yeast strains. What general observations can we make from 7.1? 1. General Trend: As the dose of H2O2. increases, the survival percentage decreases for both strians, indicating a negative effect of increasing oxidative stress on yeast survival. 2. Comparison of Strains: The WT strain consistently shows higher survival percentages than the 3KO strain at all doses, suggesting that the WT strain is more resistant to oxidative stress caused by H2O2. Dose-Specific Patterns: At lower doses (e.g., 2mM), both strains have relatively high survival percentages, but the WT strain has a clear advantage. At higher doses (e.g., 8-10 mM), the surviavl percentages drop significancly for both strains, with the 3KO strain showing a much steeper decline. The key implication of these observations is that compared to the WT strain, the 3KO strain’s survival is less robust to oxidative stress, potentially due to a lack of key genes. 7.1.3 Linearising the data Look closely at figure 7.1 again. Is the relationship between survival percentage and dose linear? I don’t think so. For both yeast strains (especially the 3KO strian), my brain is trying to draw a curved line through the data points. Applying a linear model (regression) to this data would likely result in misleading predictions, such as survival percentages below 0% or above 100%. It would also likely over or under-estimate my LD50 values for each strain. I’d say that the data demonstrates a non-linear decay. To apply linear regression appropriately, the data should first be linearised. Linearisation involves transforming the non-linear relationship into a linear one that satisfies the assumptions of linear regression. This will allow us to achieve a better model fit and to make more accurate predictions. In our case we’re going to apply a logarithmic transformation. Don’t worry – I’m not expecting you to develop a deep understanding of logarithms here. In short, we use logarithmic transformations to simplify data, especially when dealing with non-linear relationships. This helps make patterns more linear and easier to interpret, without the need for complex maths (and non-linear models). Let’s add the following line to our script file: # Extend your existing code with the following: # Calculate a log_survival column yeast_data$log_survival &lt;- log10(yeast_data$survival) This code creates a new variable called log_survival in the yeast_data dataset by applying the log10() function (a base-10 logarithm) to each value in the survival column. Let’s re-visualise the data, but now for the log_survival column. # Extend your existing code with the following: # Plot Dose vs Survival for Wild Type First plot(yeast_data$dose[yeast_data$strain_type == &quot;WT&quot;], yeast_data$log_survival[yeast_data$strain_type == &quot;WT&quot;], type = &quot;p&quot;, # &#39;p&#39; for points only pch = 16, xlab = expression(Dose~(mM~H[2]*O[2])), ylab = &quot;Log (Survival (%))&quot;, ylim = c(1.2, 2) ) # Add points for Mutant Strain points(yeast_data$dose[yeast_data$strain_type == &quot;3KO&quot;], yeast_data$log_survival[yeast_data$strain_type == &quot;3KO&quot;], pch = 1) # Color for 3KO strain # Add the legend legend(&quot;topright&quot;, legend = c(&quot;WT&quot;, &quot;3KO&quot;), pch = c(16, 1), bty = &quot;n&quot;) Figure 7.2: Log (Survival (5) vs Dose response curves for wild type (WT) and mutant (3KO) yeast strains. In the code above I’ve simply replaced any instance of yeast_data$survival with yeast_data$log_survival. I’ve also re-scaled my y-axis to values between 1.2 - 2 and updated my y-axis label to “Log (Survival (%))”. You should see a result that looks like figure 7.2. Now, my brain would be quite happy to try and draw straight lines through both the WT and 3KO series - and you know what that means, right? You got it, it’s linear regression time! Non-linear Decay in Nature Non-linear decay is a common phenomenon in nature, where processes don’t follow a straight-line pattern but instead curve as they progress. Two notable types of non-linear decay are exponential decay and sigmoidal decay. Exponential Decay This occurs when a quantity decreases at a constant rate. A biological example is the decline of a population in a resource-limited environment, where individuals die off at a rate proportional to the current population size, like in certain bacteria or viruses under unfavourable conditions. Sigmoidal Decay This “S” shaped curve starts slowly, then declines rapidly, and levels off at a minimum. It’s seen in biological processes like drug responses, where cells show little change at first, then quickly drop in survival, before stabilizing at low levels. 7.1.4 Performing a Linear Regression in R For the WT strain here’s how I’d do a linear regression in R: # Extend your existing code with the following: # Create a linear model for the WT data lm_WT &lt;- lm(log_survival ~ dose, data = yeast_data[yeast_data$strain_type == &quot;WT&quot;, ]) # View summary summary(lm_WT) ## ## Call: ## lm(formula = log_survival ~ dose, data = yeast_data[yeast_data$strain_type == ## &quot;WT&quot;, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.023921 -0.003527 -0.002781 0.007677 0.020428 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.981583 0.008346 237.43 &lt; 2e-16 *** ## dose -0.047769 0.001348 -35.44 5.6e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01272 on 9 degrees of freedom ## Multiple R-squared: 0.9929, Adjusted R-squared: 0.9921 ## F-statistic: 1256 on 1 and 9 DF, p-value: 5.601e-11 Let’s unpack these two lines and the summary output: Create a linear model: lm_WT &lt;-: This assigns the output of the linear regression model to a new object lamed lm_WT. You can then refer to lm_WT to examine the results of the model, or make predictions, later. lm(log_survival ~ dose, ...): The lm() function fits a linear regression model. Here, the dependent variable (log_survival) is being predicted based on the independent variable (dose). This means the model will examine how changes in dose are associated with changes in log_survival. data = yeast_data[yeast_data$strain_type == \"WT\", ]: The data argument specifies the dataset to use. In this case, it’s a subset of the yeast_data dataset. The subset is created by selecting only the rows where strain_type is equal to “WT”. This means the linear model is being applied only to the data for the wilt type (WT) strain. View summary: summary(lm_WT): This command generates a summary of the linear regression model stored in the lm_WT object. Intepreting the Linear Regression Summary The output from the summary(lm_WT) line indicates a strong, significantly negative relationship between dose and log_survival for the WT strain. A low residual standard error value of 0.0127 indicates that predictions are very close to observed values. An R-squared value of 0.993 indicates that 99.3% of the variation in log_survival is explained by the dose. Both the intercept and slope values from the linear fit (0.9815 and -0.0478 respectively) have associated p-values &lt; 0.001 indicating that each of these coefficients are extremely significant. Overall, the model strongly supports that increasing the dose reduces survival on a logarithmic scale. It provides an excellent fit to the data with minimal residuals. 7.1.5 Calculating LD50 The last thing to do then is to use our model to calculate the LD50. Remember, this is a standard measure used to indicate the dose of a substance required to kill 50% of a test population. In the code below I extract the coefficients from my linear model and substitute a y-value of log(50) (~1.70) into my formula to predict the dose value: # Extend your existing code with the following: # Get your model coefficients intercept_WT &lt;- lm_WT$coeff[1] # Extract the intercept slope_WT &lt;- lm_WT$coeff[2] # Extract the slope # Calculate the dose predicted at a log_survival of log(50) ld50_WT &lt;- (log10(50) - intercept_WT)/slope_WT # print ld50_WT print(ld50_WT) ## (Intercept) ## 5.916249 And there you have it! My LD50 value for my wild type (WT) strain is 5.92 mM (2 d.p). I can update my figure 7.2 to show my line of best fit generated by my linear model and annotate the line with my LD50 value for WT using the following code: # Extend your existing code with the following: # Add regression line abline(lm_WT) # Annotate the plot with the WT LD50 abline(v = ld50_WT, lty = 2, col = &quot;red&quot;) # Vertical dotted line at LD50 text(ld50_WT, log50, labels = paste(&quot;LD50 =&quot;, round(ld50_WT, 2)), pos = 4, col = &quot;red&quot;) Figure 7.3: Log (Survival (%) vs Dose response curves for wild type (WT) and mutant (3KO) yeast strains. Straight black line indicates result of linear regression model for WT. Vertical dotted red line indicates LD50 value for WT. Can you do the same for the 3KO mutant strain? You’ll need to if you want to complete this week’s summative test. 7.2 Multiple Linear Regression (MLR) in R. We can extend our knowledge of regression by exploring Multiple Linear Regression (MLR). Unlike simple linear regression, which focuses on a single predictor, MLR enables us to account for the effects of multiple factors simultaneously, making it ideal for more complex real-world problems. As an example, let’s revisit our MegaMush scenario from Chapter 5. We explored how the dependent variable, Viable Counts (VC), is influenced by independent variables, temperature and moisture. I finished the chapter with figure ??, a 3D scatterplot showing that all the data aligns with a flat plane rather than a single line of best fit between any pair of variables. We can use MLR to determine the equation of that flat plane and then predict VC for any combination of temperature and moisture within the measured range. Such an analysis would be extremely powerful for anyone wanting to fine-tune processes to finely control the resultant the VC. 7.2.1 Performing an MLR in R Let’s do it! Start a new script file in your project for this week and run the following code: # Read the data from a URL compost_data &lt;- read.csv(&quot;https://raw.githubusercontent.com/rtreharne/qs/refs/heads/main/data/05/compost_999.csv&quot;) # Create an MLR model lm_compost &lt;- lm(Viable.counts ~ Temperature * Moisture, data = compost_data) # Inspect the result of the model summary(lm_compost) ## ## Call: ## lm(formula = Viable.counts ~ Temperature * Moisture, data = compost_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6152 -0.3811 -0.0023 0.3978 1.7748 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 171.269754 6.665708 25.694 &lt;2e-16 *** ## Temperature -1.382293 0.116104 -11.906 &lt;2e-16 *** ## Moisture 0.886532 0.089907 9.861 &lt;2e-16 *** ## Temperature:Moisture -0.001503 0.001561 -0.963 0.336 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5806 on 496 degrees of freedom ## Multiple R-squared: 0.9942, Adjusted R-squared: 0.9941 ## F-statistic: 2.811e+04 on 3 and 496 DF, p-value: &lt; 2.2e-16 Wow. That was easy! You’ll notice that the only real difference was in the formula argument of the lm() function. You’ve incorporated both independent variables using Viable.counts ~ Temperature * Moisture. This is reminiscent of your two-way ANOVA from last week. Let’s interpret the interesting bits of the summary output: Temperature: For every 1-unit increase in Temperature, VC decreases by 1.38 units (highly significant, p &lt; 2e-16!). Moisture: For every 1-unit increase in Moisture, VC increases by 0.89 units (highly significant, p &lt; 2e-16!). Temperature:Moisture interaction: Just like with out two-way ANOVA we get a line in our summary that tells us if there is any significant interaction between our independent variables. Interestingly, in this case the interaction effect is small and not statistically significant (p = 0.36). Model Fit: Residual standard error: 0.58 (very low!) Adjusted R2: 0.9941 - The model can explain 99.41% of the variance in the dataset. Overall model significance: p &lt; 2.2e-16 In conclusion, the multi-linear regression model provides a strong fit to the data. Both Temperature and Moisture significantly impact the Viable Counts, but their interaction is not statistically significant. 7.2.2 Predicting values for Viable Counts You can now proceed to make a prediction for VC values by inputting any combination of Temperature and Moisture co-ordinates from the measured range of data. It would be a bad idea to try and predict VC for any temperatures/moisture from outside the range because you really don’t know if the behavior outside of this range will remain linear. You can use your model object lm_compost to predict VC using few lines of R code. I am going to be a bit mean here and let you figure out the best way to do this yourselves (there will be more than one way to do it!). I don’t care how you go about this - Google, ChatGPT etc - do whatever it takes. Remember to make use of this week’s formative test to check your results first before attempting the summative test. If you’ve cracked it then I encourage you to share your solutions in the Workshop channel in Microsoft Teams. "],["appendix.html", "Appendix I: Teach yourself R. LinkedIn Learning YouTube Books", " Appendix I: Teach yourself R. LinkedIn Learning Can you believe it!? As a student at the University of Liverpool you have free and full access to the LinkedIn Learning platform. Login using your UoL credentials. It’s completely choc-o-bloc with coding resources and you often get a neat little certificate every time you complete a course. Complete guide to R: Wrangling, Visualizing, and Modeling Data. Barton Poulson Data Wrangling in R. Mike Chapple R for Data Science: Analysis and Visualization. Barton Poulson Coding Exercises: R Data Science. Mark Niemann-Ross Complete Your First Project in R. Megan Silvey R for Data Science: Lunch Break Lessons. Mark Neimann-Ross YouTube Ah, YouTube. How I love you. But not shorts. Shorts are evil and rot your brain. Here are some great YouTube playlists that will introduce you to R. Introduction to R. DataDaft Statistics and Statistics with R Tutorials. MarinStatsLectures R tutorial - Learn R Programming. DataCamp Introduction to R Programming. Data Science Dojo Books Do people read books anymore? Yes they do. Here are 5 titles that are available to you in the University of Liverpool library right now. Hands-on programming with R. Garret Grolemund R in action: data analysis and graphics with R and Tidyverse. Robert Kabacoff R for Data Science. Christopher Lortie Introduction to Statistics Using R. Mustapha Akinkunmi An Introduction to Data Analysis in R: Hands-on Coding, Data Mining and Statistics from Scratch. Zamora Alfonso et. al "],["feedback.html", "Appendix II: Feedback", " Appendix II: Feedback I’m experimenting this year with “micro-feedback”. Rather than wait for you to complete (or not to complete in most cases) your end of module evaluation surveys I’m asking you to regularly rate your satisfaction with this course and provide additional comments. This will help me gain a deeper understanding how how your are interacting with the course content and to identify/fix any problems as they arise. I’ll update figure below as we go through the course. You can submit your feedback at any point here. Thanks for the feedback! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
