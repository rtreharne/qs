[["index.html", "Quantiative Skills in Biosciences I Cover", " Quantiative Skills in Biosciences I R. E. Treharne 2024-10-07 Cover "],["introduction.html", "Introduction", " Introduction This book accompanies the Quantitative Skills (QS) component of the BIOS103 - Introductory Practical Skills in Biosciences I course. If you are participating in the weekly timetabled QS workshops associated with this course then be sure to submit your weekly summative QS assignments via Canvas by the deadline specified for each workshop. All QS workshops will be delivered online via Teams. Access the meeting link from the BIOS103 Canvas course. "],["introduction-to-excel-and-r.html", "1 Introduction to Excel and R 1.1 Estimating the Volume of a Snail 1.2 Getting Ready for R 1.3 Complete your Weekly Assignments", " 1 Introduction to Excel and R If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. 1.1 Estimating the Volume of a Snail In this section, you will learn how to import data from a CSV file into Excel, perform basic calculations, create a scatterplot including a linear trendline to make predictions. The primary objective is to estimate the volume of a snail based on its mass using a provided dataset. Google Sheets Alternative If you like, you can do everything in the above video using Google Sheets instead! Here’s my alternate video just for Google Sheets. 1.1.1 Download and Import the CSV File Download the CSV File: Here is an example dataset. Download it to your local machine. If that link doesn’t you can get the dataset in your browser here. Right click and Save as. Import into Excel: Open Excel (Use the desktop version - you won’t be able to do this using the online version!). Go to Data &gt; From Text/CSV and select the downloaded CSV file. When the import wizard appears, click Load. 1.1.2 Calculate Volume \\(V\\) in Excel We will estimate the volume \\(V\\) of our snails using the formula for the volume of a sphere: \\[ V = \\frac{4}{3} \\pi r^3 \\] where \\(r\\) is the radius, which we assume is equal to half the Height L (mm) column. Add a New Column for Volume \\(V\\): In the third column, label it as Volume V (mm^3). In the first cell of this column, use the formula: = (4/3) * PI() * ((B2/2)^3) Drag the formula down to apply it to all rows. 1.1.3 Add a Linear Trendline and Equation Create a Scatter Plot: Select the Mass M (g) and Volume V (mm^3) columns. Go to the Insert tab and select Scatter Plot. Add Trendline: Click on the plot Click the green + icon that appears at the top right of the plot. Click the &gt; symbol on the Trendline option and click More options… From the trendline options menu that appears on the right, select the Linear trendline and check the box isplay Equation on chart. Right-click on a data point in the scatter plot. Interpret the Equation: The trendline equation will appear on the chart in the form of \\(y = ax + b\\), where: \\(y\\) is the volume. \\(x\\) is the mass. \\(a\\) and \\(b\\) are coefficients. For the Example dataset: a = 1341.7 \\(mm^3.g^{-1}\\) b = 1140.2 \\(mm^3\\) 1.1.4 Estimation of Volume for a Snail with Mass 10g Use the Trendline Equation: Substitute \\(x = 10\\) into the trendline equation to calculate the estimated volume \\(V\\). Express the volume in \\(cm^3\\) to one decimal place (Note: \\(1 cm^3 = 1000 mm^3\\)). For the example dataset: The estimated volume of a snail that is \\(10 g\\) is \\(14.6 cm^3\\). 1.2 Getting Ready for R Over the next couple of weeks you will continue to use Excel to load, manipulate, analyse and visualise data. Beyond this you will be using the coding language R exclusively. To prepare for this, you need to download, install and configure R and RStudio today. Chromebook Users I love a chromebook. Sadly, installing R and RStudio on one involves a bit of extra work compared to Windows and Mac. This YouTube video seems to have all the bases covered: “How to Install RStudio on a Chromebook” 1.2.1 Download and Install (Windows and Mac only) R and RStudio are actually separate things, although they are often mentioned together. R is a programming language and software environment specifically designed for statistical computing and data visualisation. Other examples of programming languages include Python, Java and Ruby. RStudio is the integrated development environment (IDE) for R. It provides a user-friendy interface that will allow you to write all your R scripts and compile them to do stuff. I’m using it to write this handbook right now! Despite their differences, you might hear the terms R and RStudio used interchangeably, as RStudio serves as the primary interface through which users interact with the R programming language. You need to download and install both R and Rstudio. Install R first from the Comprehensive R Archive Network (CRAN) Then install RStudio from the RStudio website Once both are installed, open up RStudio and get ready to create your first R Project. All University of Liverpool MWS machines already have R and RStudio installed and ready to use. 1.2.2 Creating Your First R Project in RStudio Follow these steps to set up and manage your first R project in RStudio: Open RStudio Launch RStudio from your applications menu. Start a New Project Click on the File menu at the top of the RStudio window. Select New Project… from the dropdown menu. Choose Project Type You will be prompted with three options: New Directory: Create a new project in a new directory. Existing Directory: Use an existing directory as the project’s folder. Version Control: Clone a project from a version control repository (e.g., GitHub). For your first project, select New Directory. Select Project Template Choose Empty Project. Click Next. Set Up Project Directory Directory name: Enter a name for your project folder. This will be the name of the directory created for your project. Subdirectory of: Choose the parent directory where the new project folder will be created. You can navigate to the desired location using the file browser. Click Create Project. RStudio Project Interface Once the project is created, you will see a new RStudio window or tab with the following components: Files pane: Displays the files and folders in your project directory. Script editor: Where you write and edit your R scripts. Console: Where you can directly enter and execute R commands. Environment/History: Shows your workspace objects and command history. Plots/Packages/Help/Viewer: Various tabs for viewing plots, managing packages, accessing help documentation, and viewing other outputs. Create and Save an R Script Click File &gt; New File &gt; R Script. Write some R code in the script editor. For example: To run your print command, click on the line and click the Run button at the top right of your script editor window or press Ctrl + ENTER (Cmd + Enter on Mac). You will see the following output in your console window: ## [1] &quot;Hello World!&quot; And there it is! You’ve just successfully compiled your first line of R. Congratulations! 1.2.3 Something More Complicated As you delve deeper into R programming, you’ll find that your scripts become more sophisticated than the “Hello World” example above. In the following example, I’ll walk you through a script to create a random number generator. Here’s the script in its entirety: # create a variable called &quot;seed&quot; seed &lt;- 999 # set the seed set.seed(seed) # generate a random number random_number &lt;- runif(1) # print the random number print(random_number) Cut and paste these lines into the script file we were working on earlier (overwrite the “Hello World” example). Now, you can run each line in turn as before (using Ctrl + ENTER) or you can run everything by clicking the Source button. You should see the following number appear in your console: Let’s break down each line of the script to understand its purpose and functionality. Creating a Variable Called “seed” # create a variable called &quot;seed&quot; seed &lt;- 123 seed &lt;- 123: Here, we are using the &lt;- operator to assign the value 123 to the variable named seed. In R, variables are used to store data that can be reused or manipulated later in the script. The number 123 is arbitrary in this case, but we use it to illustrate how to set a seed for random number generation. Setting the Seed # set the seed set.seed(seed) The set.seed() function initializes the R environment’s build in random number generator with the value stored in seed. Setting a seed is essential for reproducibility, meaning that if someone else runs this code with the same seed, they will get the same random number output in their console. This is particularly useful in simulations and randomised experiments where consistent results are needed. Generating a Random Number # generate a random number random_number &lt;- runif(1) random_number &lt;- runif(1): This line generates a single random number between 0 and 1. The function runif() generates random numbers from a uniform distribution, which means that each number within the specified range has an equal probability of being selected. The 1 inside the parentheses specifies that only one random number should be generated. The resulting number is then assigned to the variable called random_number. Printing the Random Number # print the random number print(random_number) print(random_number): The print() function outputs the value stored in random_number to the console. This is useful for verifying the output of your code and ensuring that the operations have been executed correctly. Summary This example script demonstrates a more complex task than the basic “Hello World” script. It introduces key concepts like variable assignment with &lt;-, setting a seed for reproducibility using set.seed(), generating random numbers with runif(), and printing results using print(). As you continue learning R, these foundational concepts will become increasingly important, enabling you to build more advanced and meaningful analyses. Remember, comments (#) are your friends! They help explain what each part of your code does, making it easier for you and others to understand and maintain your scripts. Be liberal with your comments. You’ll thank yourself later (trust me). Give me feedback I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 1.3 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["summarising-data-and-anova-in-excel.html", "2 Summarising Data and ANOVA in Excel 2.1 Summarising Data 2.2 Analysis of Variance (ANOVA) 2.3 Complete your Weekly Assignments", " 2 Summarising Data and ANOVA in Excel If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. In this section we will only be using Excel. No R today! Specifically, you will be developing two important skills: Summarising Data. Constructing and testing hypotheses. The ultimate aim is to gain insight and learn something new about the world from the data that we have painstaking measured in our well designed lab experiments. This is a cornerstone of what being a scientist is all about. 2.1 Summarising Data Raw data is beautiful, but messy. Showing another person your raw data and expecting them to immediately understand it, no matter how proud you are of the toil expended to generate the data, is an unrealistic expectation. You need to boil your data down into something that another person can grasp instantaneously. Let’s take a look at a Zebrafish dataset from an experiment that is uncannily similar to the one you performed in your lab practical this week. 2.1.1 Download and Import the CSV File Download the CSV File: Here is an example dataset. Download it to your local machine. Import into Excel: Open Excel (Use the desktop version - you won’t be able to do this using the online version!). Go to Data &gt; From Text/CSV and select the downloaded CSV file. When the import wizard appears, click Load. You should now see something like that in Figure 2.1. There are 3 columns: ID - A unique number to identify a measurement. conc_pc - The ethanol concentration (%) that the each embryo was treated with. length_micron - The measured lengths, in \\(\\mu m\\) of the embryos. We call this format, in which each row corresponds to a single measurement, a long format. Figure 2.1: You should see this (or something like it) after you have imported your date into Excel. 2.1.2 Generating a Summary Table Excel Alternative If you are having trouble accessing the Desktop version of Excel then here is an alternative video. “Summary Table with Google Sheets” Identify your Groups Click anywhere in your table. Select the Data menu and click the Advanced icon in the Sort &amp; Filter section. This will bring up a window called “Advanced Filter”. Select the Copy to another location action. Your list range should already be set to $B:$B, but if not make it so. Set the Copy to cell to $E:$1 Make sure the Unique records only check box is selected and click OK. You should now see a complete list of your alcohol concentration groups in a column with a header conc_pc. Make this into a new table by clicking on any of the concentration values, and then Insert &gt; New table &gt; OK. Nice. Now you’re ready to start building out your summary table horizontally. Let’s start with calculating the mean Zebrafish length for each group. Right now, your spreadsheet should look roughly the same as the screenshot in figure 2.2 Figure 2.2: Constructing a summary table Calculating a Mean Column Create a new column in your summary table by typing the word Mean in cell F1. Calculate the mean of the Zebrafish lengths for the control group (0% alcohol concentration) by entering the following formula into cell F2. =AVERAGE(IF($B1:$B$161=$E2,$C1:$C$161)) A Deeper Explanation The formula =AVERAGE(IF($B$2:$B$161=$E2,$C$2:$C$161)) is an array formula that calculates the average length of Zebrafish for a specific group based on the concentration of alcohol. It’s a bit of a beast isn’t it? Let’s break it down. $B$2:$B$161: The $ symbols before both the column letter B and the row numbers 2 and 161 lock the entire range. This means that when you copy the formula to other cells, this range will not change; it will always refer to cells B2 to B161. $E2: The $ before the column letter E locks the column, but since there’s no $ before the row number 2, the row number can change if the formula is dragged down across rows. This cell is used to compare each value in the range $B$2:$B$161 to the specific concentration value in the corresponding row in column E. $C$2:$C$161: Similar to the range for column B, this locks the range of cells in column C from which the values will be averaged, conditional on the IF statement. AVERAGE(IF(…)): The IF function checks each row in the range $B$2:$B$161 to see if it matches the value in the corresponding row in column E. If it matches, the corresponding value in column $C$2:$C$161 is included in the average calculation. The AVERAGE function then calculates the mean of these filtered values. This approach is particularly useful when you want to calculate conditional averages across a dataset, ensuring that the correct cells are referenced even when copying the formula to different parts of the spreadsheet. Calculating More Columns Create four more columns with headers: Std. Dev. Median Min Max Drag the cell F2 to G2. Change the word AVERAGE in the formula in G2 to STDEV. This will calculate the standard deviation for the group and the remaining cells in the column should also auto complete. Do the same for the median, min and max columns. Be sure to use the corresponding function. A Deeper Explanation When analysing data, it’s important to understand the basic statistical measures that summarise the data’s distribution. Here are some key terms: Mean: The mean, often referred to as the average, is the sum of all values in a dataset divided by the number of values. It provides a central value for the data. However, the mean can be influenced by outliers (extremely high or low values). Standard Deviation: The standard deviation measures the amount of variation or dispersion in a dataset. It is calculated as the square root of the variance, where variance is the average of the squared differences between each data point and the mean. A low standard deviation indicates that the data points tend to be close to the mean, while a high standard deviation indicates more spread out data. Median: The median is the middle value in a dataset when the values are arranged in ascending or descending order. If the dataset has an odd number of values, the median is the central value. If the dataset has an even number of values, the median is the average of the two central values. The median is less affected by outliers compared to the mean. Min: The minimum (min) value is the smallest value in the dataset. It provides a measure of the lower bound of the data. Max: The maximum (max) value is the largest value in the dataset. It provides a measure of the upper bound of the data. These measures are fundamental for understanding the distribution of data. The mean and median give you central tendencies, while the standard deviation tells you how spread out the data is. The minimum and maximum values provide the range within which all the data points fall. 2.1.3 Presenting Your Summary Table At some point you may wish to include your Excel summary table in a Word document. There’s a lot of wiggle room on how you choose to format your table but there are a few unbreakable rules: The table MUST have a caption. The caption should be placed ABOVE the table (not below as for a figure or graph). The caption should be numbered accordingly. For example, if this is the first table in your document the figure caption should start “Figure 1: …”. The caption should be descriptive and unambiguous. The reader should be able to quickly interpret what is going on without having to read the body of the text. Any symbols or variables or units should be defined in the caption. The data should be formatted to a sensible number of decimal places (i.e. if you’re measurements are made to 1 decimal place, your summary values should not be quoted to more than this). Follow these rules and you can’t go wrong. Figure 2.3 shows how my summary table looks when copied and pasted into Word. I like to make my tables span the entire width of my document using the Auto-fit to window command. I also like to center my columns. These are personal preferences, but you can’t deny they look great! Figure 2.3: Formatting a summary table in Microsoft Word. 2.2 Analysis of Variance (ANOVA) You’re about to learn a critical skill that is important for becoming a scientist: formulating hypotheses and testing them. This is a cornerstone of scientific inquiry. You’ve already summarised your data using descriptive statistics. Now, we’ll move on to another branch of statistics called inferential statistics. This involves using an appropriate statistical test to determine whether specific hypotheses that we construct should be accepted or rejected. Knowing which statistical test to use depends on the data and context. It takes time and lots of practice to become proficient at this, and it’s completely normal to forget which test you need or how to interpret the result. We’ll stick with out Zebrafish dataset and perform an Analysis of Variance (ANOVA) to determine whether there is something we can learn from our data. We’ll formulate a hypothesis and use Excel to perform the ANOVA. Then we’ll interpret the results. But before we dive into this, let’s create a boxplot to visually inspect the data and see if we can generate some gut intuition as to what might be going on. 2.2.1 Grouped Boxplots in Excel You might not have seen a grouped boxplot before. That’s OK. I’m confident that you’ll have a good intuition for what they show. However, for more information the key components of a boxplot read the text in the Anatomy of a Boxplot section. Let’s dive straight in for now though. Create the Boxplot Excel Alternative If you are having trouble accessing the Desktop version of Excel then here is an alternative video. “Boxplots with Excel Online” Insert a Boxplot: Select your data. Go to the Insert tab, click on Insert Statistical Chart, and choose Box and Whisker. Select Data: Your plot will look a bit weird. That’s because we need to configure the groupings properly. Click the Select data button. Remove the conc_pc series from the left-hand list. Click the Edit button on the (currently empty) right-hand list. Boxplots in the wrong order? Sort the conc_pc column from smallest to largest. Re-scale y-axis: It’s best to re-scale the y-axis to maximise the space used by the boxplots. This will make any effect easier to see. Double-click on the numbers in the y-axis. Set the Bounds &gt; Minimum: to 1000. Add Axis Labels: Click the green + icon in the top-right of your graph. Check the Axes titles box. Double-click on each label in turn and update with appropriate labels: X-axis: “Alcohol Conc. (%)” Y-axis: “Embryo Length (\\(\\mu m\\)) Note: To use the \\(\\mu\\) symbol click Insert &gt; Symbols &gt; Symbol. Find the symbol in the list and click Insert. Get Rid of Chart Title: If you’re going to be presenting this figure in a report or poster then it should not have a title above the axes. Instead you should include a figure caption below the plot. The same unbreakable rules for your caption are the same as those described above for table captions. Just make sure your caption is below the figure instead of above. Export Your Figure: Right click on your figure anywhere outside the plot area and you should see the option to Save as picture. Save it somewhere sensible as a .png file and then insert it into a Word document with a sensible caption. Figure 2.4: Distribution of Zebrafish embryo lengths organised by Alcohol treatments. Interpret the Boxplot Figure 2.4 shows our finished boxplot. Alcohol Concentration 0%: The median embryo length is approximately 2500 µm, with the mean slightly above the median. The data is relatively spread out, as shown by the wide box and long whiskers. There are no outliers in this group. Alcohol Concentration 1.5%: The median length is slightly lower than the 0% group, but the mean is still fairly close. The box and whiskers are narrower than in the 0% group, indicating less variability in embryo length. One outlier is present above the whisker, indicating a particularly large embryo in this concentration or possibly, a random measurement error. Alcohol Concentration 2%: The median and mean have both decreased, showing a reduction in embryo length as alcohol concentration increases. The box is narrower, indicating less variability, but there are several outliers both above and below the whiskers, suggesting that while most embryos were of a similar size, a few were much larger or smaller. Alcohol Concentration 2.5%: The median and mean lengths have further decreased, indicating a continued negative effect of alcohol concentration on embryo length. The box is of similar size to the 2% group, but with longer whiskers, indicating more variability in the data. There is one outlier below the whisker, indicating a particularly small embryo in this concentration. In summary, As alcohol concentration increases, the median and mean embryo lengths decrease, indicating a negative correlation between alcohol concentration and embryo length. Variability in embryo length decreases slightly up to 2% concentration but then increases again at 2.5%, as evidenced by the longer whiskers. The presence of outliers, particularly at higher concentrations, suggests that while most embryos are affected similarly, some experience more extreme changes in size. However, visual patterns alone cannot confirm the existence of a true relationship between alcohol concentration and embryo length. To rigorously explore whether the observed trends are statistically significant or merely due to chance, we must construct testable hypotheses. This process will allow us to formalise our observations and set the stage for appropriate statistical analysis. Anatomy of a Boxplot A boxplot is a standardised way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. The box in the boxplot represents the interquartile range (IQR), which is the range between Q1 and Q3. The central line within the box indicates the median, which is the middle value of the dataset. Sometimes, an “X” symbol is also included within the box, representing the mean of the dataset. However, it is important to note that the mean is not always shown in a boxplot. The “whiskers” of the boxplot extend from the box to the smallest and largest values within 1.5 times the IQR from the first and third quartiles, respectively. These whiskers help to indicate the spread of the majority of the data. Outliers are data points that fall outside the range defined by the whiskers. These are typically plotted as individual points beyond the ends of the whiskers, highlighting data points that are unusually high or low compared to the rest of the dataset. To determine whether a data point is an outlier, you compare it to the thresholds defined by the IQR: Any data point below Q1 - 1.5 * IQR is considered a lower outlier. Any data point above Q3 + 1.5 * IQR is considered an upper outlier. Summary: Box: Represents the interquartile range (IQR), the middle 50% of the data. Central Line: Indicates the median value. X (if shown): Indicates the mean value. Whiskers: Extend to the smallest and largest values within 1.5 times the IQR from the quartiles. Outliers: Data points that lie outside the whiskers, typically displayed as individual points. 2.2.2 Constructing Testable Hypotheses Given the patterns observed in the boxplot, where higher alcohol concentrations appear to be associated with shorter embryo lengths, it is essential to move from visual interpretation to a more rigorous statistical approach. This involves constructing and testing hypotheses to determine whether the observed trends are statistically significant. Formulating the Hypotheses In the context of your data, the primary goal is to determine whether different alcohol concentrations have a statistically significant effect on embryo length. To do this, we formulate a null hypothesis (\\(H_{0}\\)) and an alternative hypothesis (\\(H_{1}\\)): Null Hypothesis (\\(H_{0}\\)): There is no statistically significant difference in the mean embryo lengths between the different alcohol concentration groups. Any observed differences are attributed to random variation rather than an effect of alcohol concentration. \\[ H_0: \\bar{x}_0 = \\bar{x}_{1.5} = \\bar{x}_2 = \\bar{x}_{2.5} \\] Here, \\(\\bar{x}_0\\), \\(\\bar{x}_{1.5}\\), \\(\\bar{x}_2\\), and \\(\\bar{x}_{2.5}\\) represent the mean embryo lengths at 0%, 1.5%, 2%, and 2.5% alcohol concentrations, respectively. Alternative Hypothesis (\\(H_{1}\\)): At least one of the mean embryo lengths differs significantly from the others, suggesting that alcohol concentration has a measurable impact on embryo length. \\[ H_1: \\text{At least one } \\bar{x} \\text{ is different} \\] The Origin of Hypothesis Testing Hypothesis testing emerged in the early 20th century through the work of statisticians like Ronald A. Fisher, who applied these methods to agricultural experiments, leading to significant advancements in statistical methodology. However, Fisher’s legacy is also marred by his support for eugenics, reflecting the darker intersections of early statistical science with discriminatory ideologies. While hypothesis testing remains central to scientific research, the use of p-values has faced criticism. P-values, often misinterpreted, simply measure data compatibility with the null hypothesis, not the truth of the hypothesis itself. The conventional threshold (p &lt; 0.05) can lead to arbitrary decisions, and practices like p-hacking undermine the validity of results. Alternatives and complements to p-values include confidence intervals (which offer a range of likely values for parameters), Bayesian methods (which incorporate prior knowledge into probability assessments), and effect sizes (which quantify the magnitude of an effect). Understanding these tools and their limitations enables researchers to draw more nuanced and reliable conclusions from their data. Getting Ready to Test A One-way ANOVA is a statistical test used to determine whether there are significant differences between the means of three or more independent (unrelated) groups. In this case, the groups are the different levels of alcohol concentration (0%, 1.5%, 2%, and 2.5%). The term “one-way” refers to the fact that we are analysing the effect of a single factor (alcohol concentration) on the dependent variable (embryo length). If we were interested in analysing the impact of an additional dependent variables (e.g. incubation temperature), as well as any interaction effects with alcohol concentration, we would likely want to perform a “two-way” ANOVA. We are using ANOVA in this context because: Multiple Group Comparisons: We have more than two groups (four alcohol concentration levels), and ANOVA is designed to handle comparisons across multiple groups simultaneously. This is more efficient and statistically sound than performing multiple t-tests, which would increase the likelihood of Type I errors (false positives). Assessing Variability: ANOVA compares the variability within each group (i.e., how much embryo lengths vary within each alcohol concentration) to the variability between groups (i.e., how much the group means differ from each other). This helps us determine if the observed differences in means are greater than what we would expect by chance alone. Assumptions of ANOVA For ANOVA to be valid, certain assumptions must be met: Independence of Observations: The data points in each group are independent of each other. In other words, each data point corresponds to a unique embryo. Normality: The distribution of the residuals (differences between observed and predicted values) should be approximately normal. Homogeneity of Variances: The variances within each group should be roughly equal. In this case, I have engineered the dataset so that these assumptions are met. However, in practice, when working with real data, it is crucial to check whether these assumptions hold before applying ANOVA. If the assumptions are violated, the results of the ANOVA may not be reliable, and alternative methods may be necessary. I’ll show you how to check that the assumptions for a statistical test are met in Chapter 5 and the available alternative tests for cases where the assumptions are not met. 2.2.3 Performing a One-Way Anova in Excel Excel Alternative If you are having trouble accessing the Desktop version of Excel then here is an alternative video. “ANOVA with Google Sheets” Please be reminded that the guidance in the video above, and in the text below, is only relevant if you are using the desktop version of Excel on a Windows machine. It might work on a Mac - there’s no guarantee, and it won’t be relevant if you are using Google Sheets. If you can’t do the following with your personal device then please use one of the University machines. Step 1: Install and Activate the Data Analysis ToolPak To perform a One-Way ANOVA in Excel, you need to install and activate the Analysis ToolPak add-on. Open Excel and go to File &gt; Options. In the Options menu, select Add-ins. At the bottom, next to “Manage”, ensure Excel Add-ins is selected and click Go. In the Add-Ins box, check the Analysis ToolPak option and click OK. You should now see a Data Analysis button in the Data ribbon. Step 2: Create a Pivot Table to Reshape Data Before performing the ANOVA, you need to reshape your long data into a wide format using a pivot table. Here’s how to do it: Select your dataset (including headers). Go to the Insert tab and click on PivotTable. In the Create PivotTable dialog box, select where you want the PivotTable to be placed (e.g., New Worksheet). In the PivotTable Fields pane: Drag conc_pc to the Columns area. Drag id to the Rows area. Drag length_micron to the Values area. The resulting pivot table will have the concentration levels as columns and the measurements as rows, which is the required format for One-Way ANOVA. Step 3: Perform the One-Way ANOVA Click on the Data ribbon and select Data Analysis. In the Data Analysis dialog, select ANOVA: Single Factor and click OK. In the ANOVA: Single Factor dialog box: Input Range: Select the range of your reshaped data (including the labels). Grouped By: Choose Columns (since each group is in a separate column). Labels in First Row: Check this option if you included labels. Alpha: Set this to 0.05 (the default significance level). Output Options: Choose where you want to display the results (e.g., New Worksheet Ply). You can name the new sheet “Results”. Click OK to run the analysis. You should see a new sheet that looks identical to that shown in 2.5. Figure 2.5: Results tables of one-way ANOVA in Excel Step 4: Interpret the Results Excel will generate a new worksheet with the ANOVA results. The output includes two tables: Summary Table: Count: Number of observations in each group. Sum: Sum of all values in each group. Average: Mean value of each group. Variance: Variability within each group. ANOVA Table: Source of Variation: Between Groups: Variability between the groups. Within Groups: Variability within each group. SS (Sum of Squares): Measure of the total variation. df (Degrees of Freedom): Calculated as the number of groups minus 1 for between groups, and total observations minus the number of groups for within groups. MS (Mean Square): SS divided by df. F (F-Statistic): Ratio of MS between groups to MS within groups. P-Value: Indicates if the results are statistically significant. F crit: Critical value of F for the given alpha level. Step 5: Make a Conclusion Compare the P-Value to the alpha level (0.05): If P-Value ≤ 0.05, reject the null hypothesis and conclude that there is a significant difference between the groups. If P-Value &gt; 0.05, fail to reject the null hypothesis and conclude that there is no significant difference between the groups. In your ANOVA result, a p-value of 2.48E-08 means that the probability of observing the differences between your group means by random chance is exceedingly low (just 0.0000000248). Since this p-value is far below the common threshold of 0.05, we can reject the null hypothesis (\\(H_0\\)), and conclude that there are statistically significant differences between the groups. While the one-way ANOVA tells us that there are significant differences between the groups, it does not specify which groups differ from each other. To determine where these differences lie, additional testing, known as post-hoc testing, is required. Post-hoc tests, allow us to compare the group means directly and identify which specific groups are significantly different. Although post-hoc testing requires a bit more work, it can significantly enhance our understanding of the data and provide deeper insights into the relationships between groups. Understanding Exponent Notation In statistical analysis, particularly when working with software like Excel or R, you might values expressed in scientific notation, often using the letter “E” followed by a number. For example, the p-value 2.48E-08 appears in your ANOVA results. What Does 2.48E-08 Mean? The notation 2.48E-08 is Excel’s way of representing the number 2.48 × 10⁻⁸. Here’s how to break it down: 2.48: This is the base number. E-08: This indicates that the base number (2.48) should be multiplied by 10 raised to the power of -8. So, 2.48E-08 is mathematically equivalent to: \\[ 2.48 \\times 10^{-8} = 0.0000000248 \\] This value is extremely small, which is typical for p-values when the test results are highly significant. Why Use Scientific Notation? Scientific notation is used to conveniently express very large or very small numbers that would otherwise be cumbersome to write out in full. In the case of p-values, this format is particularly useful because significant results often involve very small numbers. Instead of writing 0.0000000248, which can be error-prone and hard to read, Excel uses 2.48E-08 to convey the same information succinctly. 2.2.4 Performing Post-hoc Tests in Excel Following a significant ANOVA, we need to perform additional tests to determine where the differences between the groups lie. These are called Post-hoc tests. Step 1: Create a new table to list group comparisons In cell A19, underneath your ANOVA result table, Create a new column label called Groups. List all the possible ways two groups can be compared to each other (there are six ways in total): 0% v 1.5% 0% v 2.0% 0% v 2.5% 1.5% v 2.0% 1.5% v 2.5% 2.0% v 2.5% Create additional column headers P-value and Significant? in cells B19 and C19 respectively. Step 2: Perform T-tests for each group comparison Starting in cell B20 type: =TTEST(wide!B$4:B$164, wide!C$4:C$164, 2, 2) Copy the cell down for the remaining rows and update the columns in the formula to correspond with the respective groups that are being compared. What is a T-test? Unlike ANOVA, which compares multiple groups simultaneously to see if there are any significant differences in the means of the groups, a T-test can only compare two groups at a time. The assumptions for a T-test are similar to those for ANOVA: the data should be normally distributed, the samples should be independent, and the variances of the two groups should be equal if using a two-sample T-test assuming equal variances. Like ANOVA, the key output of a T-test is the p-value. In Excel you perform a T-test like this: =T.TEST(list1, list2, tails, type) where: list1: Corresponds to the list of values in your first group. list2: Corresponds to the list of values in your second group. tails: Requires a value of 1 or 2 indicating whether the test is one-tailed or two-tailed respectively. A one-tailed T-test tests for a difference in a specific direction (greater or less), while a two-tailed T-test tests for any difference regardless of direction. type: Requires a value of 1, 2, or 3 indicating: 1: A paired t-test (i.e., groups are not independent). 2: An independent t-test with equal variances between groups. 3: An independent t-test with unequal variances between groups (also known as a Welch test). T-tests are powerful for pairwise comparisons but need to be used in conjunction with corrections like Bonferroni when multiple T-tests are conducted, as performing multiple tests increases the risk of Type I errors (false positives). Let’s create a new table underneath my ANOVA table in Excel, say starting in cell A19 Step 3: Calculate Bonferroni Corrected Alpha Level - To calculate the Bonferroni corrected alpha level divide the initial threshold value of 0.05 by the number of t-tests you are performing, i.e. 6. - Use the corrected value by comparing it to the p-value of each of the t-tests to determine if the difference in the means of groups is significant. Step 4: Extend Your Conclusions The post-hoc tests indicate that there are significant differences in embryo lengths were found between the following pairs of alcohol concentrations: 0% vs. 2% 0% vs. 2.5% 1.5% vs. 2% 1.5% vs. 2.5% These findings suggest that increases in alcohol concentration from 0% to 2%, and from 1.5% to 2.5%, lead to significant changes in embryo length. Feedback Please. I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 2.3 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["calibration-curves-and-linear-regression-in-excel.html", "3 Calibration Curves and Linear Regression in Excel 3.1 Calibration Curves 3.2 Linear Regression 3.3 Complete your Weekly Assignments {chapter-3-assignments}", " 3 Calibration Curves and Linear Regression in Excel If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. In this section we’ll be re-visiting linear trendlines in Excel in the context of calibration curves and extending our knowledge by going deeper into the world of linear regression. We will be working on two important skills: Manipulating data (transposing) Making predictions from linear models. 3.1 Calibration Curves A calibration curve is a plot of a measurable quantity (in our case absorbance, as determined by spectrophotometry) against the concentration of known standards. This relationship, typically linear, allows for the determination of the concentration of unknown samples by interpolation. The Beer-Lambert Law provides the foundation for generating calibration curves in spectrophotometry. It describes the relationship between absorbance (A) and the concentration (C) of a substance in solution. The law is given by the equation: \\[ A = m \\cdot C \\] Where: A is the absorbance (a dimensionless quantity, i.e. no units!). m is the slope (related to molar absorptivity and path length). C is the protein concentration (in mg/mL). There are a few key considerations to make when using this law to determine the concentrations of unknown solutions: Linear range: the calibration curve is only valid within a certain concentration range. At very high concentrations, the relationship may no longer be linear (due to factors like light scattering). Extrapolating concentrations from our calibration curves that are beyond those of our known standards is very, very naughty. Don’t do it! Reproducibility: It is critical that the same instrument (including scan settings and wavelength) be use for both the standards and unknowns. Let’s take a look at an example dataset from an experiment designed to generate a calibration curve for hemoglobin. Assume that starting from a stock solution of concentration 1.5 mg/mL a set of seven protein solutions have been created using a 1:1 serial dilution and that the absorbance of each solution has been measure at a wavelength of 560nm. Download the CSV File: Here is an example dataset. Download it to your local machine. Import into Excel: Open Excel (Use the desktop version - you won’t be able to do this using the online version!). Go to Data &gt; From Text/CSV and select the downloaded CSV file. When the import wizard appears, do not click Load. There’s something not right about this dataset. It’s sideways! The columns of data run horizontally instead of vertically. We don’t like this, it makes it much harder to plot figures and perform any analysis. We need to transform the data somehow and flip it to vertical instead of horizontal. We need to transpose the data. Transpose the data: On the import wizard click the Transform Data button. This will open up a new window called Power Query Editor. Click the Transform tab and then click the Transpose button. Click the Use First Row as Headers button. Return to the Home tab and click Close &amp; Load to import your transposed data. Generate a Calibration Curve: Click anywhere on your data table. Click the Insert tab and select the Scatter chart from the charts option. Get rid of the title. Label the x-axis: “Protein Conc. (mg/mL)”. Label the y-axis: “Absorbance (Arb. units)”. Click the green plus (chart elements) icon at the top-right of the graph and select More options from Trendline. Add a linear trendline and check the box to “Display Equation on Chart”. If you’re including your figure in a report it’s better practice to include details of the \\(m\\) and \\(b\\) values that you extract in your figure caption like in figure 3.1. Use the Calibration Curve: Now that you’ve extracted the m and b values from your calibration curve’s linear trendline you can use it to find the concentration of an unknown sample by measuring its absorbance. Our calibration curve for the dataset gives us the equation: \\[ A = 0.3827 \\cdot C - 0.0024 \\] If an unknown protein solution has an absorbance of 0.50, you can rearrange the equation to solve for concentration \\(C\\): \\[ 0.50 = 0.3827 \\cdot C - 0.0024 \\] Solving for \\(C\\): \\[ C = \\frac{0.50 + 0.0024}{0.3827} = 1.3 \\, \\text{mg/mL} \\] Thus, the concentration of the unknown protein solution is 1.3 mg/mL. Note that I’ve expressed my answer to 2 significant figures to match the minimum level of precision available to me during the calculation (i.e. 2 s.f for both 0.5 and 0.0024). Figure 3.1: Calibration curve for Hemoglobin determined from a standard set generated from a 1:1 serial dilution from a starting 1.5 mg/mL solution. Values of m=0.3827 and b=0.0024 were extracted from the linear relationship A = m * C + b, where b is the systematic error associated with the measurement (detector noise). 3.2 Linear Regression A linear trendline is visually helpful, but what what Excel is actually doing behind the scenes is something more powerful: linear regression. Linear regression is a statistical method used to quantify how much the variation in a dependent variable can be attributed to changes in an independent variable. It helps us understand how one variable predicts or influences another. Additionally, it provides insight into how much of the variation is due to error or other unaccounted factors, and whether we need to explore other relationships or variables that may better explain the observed patterns. Linear regression models are valuable because they offer an estimation of the relationship between variables, and by extension, help in predicting future outcomes based on known values of independent variables. Why “Linear”? In nature, over relatively small ranges of dependent and independent variables, the relationship between them can often be approximated as linear. This means that as one variable increases or decreases, the other responds in a predictable and proportional manner. While not always the case, assuming linearity can be useful for many applications, especially for the scope of this course. Independent and Dependent Variables: - The independent variable is the one you manipulate or change to see its effect (e.g., hours of sunlight). - The dependent variable is the one being measured or observed (e.g., sunflower growth). These are sometimes referred to as explanatory and response variables, respectively. Linear Regression as a Hypothesis Test Performing a linear regression is also a test of a hypothesis. This time, our null hypothesis is that there is no linear relationship between the dependent and independent variables. In other words, any observed association between the two is just due to random chance. More formally: Null Hypothesis (H₀): There is no linear relationship between the independent variable and the dependent variable. The slope of the regression line is equal to zero. H₀: β₁ = 0 (where β₁ represents the slope of the regression line) Alternative Hypothesis (H₁): There is a linear relationship between the independent variable and the dependent variable. The slope of the regression line is not equal to zero. H₁: β₁ ≠ 0 3.2.1 The Linear Regression Equation The linear regression equation provides a way to express the relationship between the independent variable (predictor) and the dependent variable (outcome) as a straight line: \\[ Y = β₀ + β₁X + ε \\] Y: The dependent variable (outcome) we are trying to predict. X: The independent variable (predictor) we use to make predictions. β₀: The intercept, or the value of Y when X is 0. This represents the starting point of the relationship. β₁: The slope, or the change in Y for every one-unit increase in X. It tells us how steep the relationship is. ε: The error term, which accounts for the variance in Y that cannot be explained by X. 3.2.2 Performing a Linear Regression in Excel Let’s work through an example using a full linear regression with Excel’s Analysis ToolPak. Suppose we’re investigating the growth per day of sunflowers (dependent variable) based on the number of hours of direct sunlight they receive each day (independent variable). In this case, we want to determine if there is a linear relationship between hours of sunlight and sunflower growth. By performing the regression analysis, we’ll be able to assess if and how sunlight impacts sunflower growth, and how strong that relationship is. Let’s get started with the analysis in Excel! 1. Download the data: Here is an example sunflower dataset. Download it to your local machine. 2. Import the data: Open up a new Excel workbook. Go to the Data tab and select the Import from csv/txt icon. Once the wizards has loaded click Load. 3. Perform Regression: Click anywhere on the sheet, select the Data tab again and select the Data Analysis button in the “Analysis” area of the tools ribbon. If you can’t see the button then you probably need to configure the Analysis ToolPak Add-In. Select “Regression” and click “OK”. Select the “Input Y Range” - This is your dependent variable, i.e. how much your sunflowers grew per day (column B). Include the header. Select the “Input X Range” - This is your independent variable, i.e. number of hours of direct sunlight per day (column A). Include the header. Tick the **Labels” box. Select the New Worksheet Ply output option Check all the remaining boxes for Residuals and Normal Probability sections. Click OK. Hopefully, you’ll see something like that shown in figure 3.2. Figure 3.2: This is what you should see after performing a regression on your example sunflower dataset using Excel’s Analysis ToolPak Add-In. 3.2.3 Interpreting Your Linear Regression Wow! Your linear regression has provided so much information—it can feel like an avalanche at first. But don’t worry, not all of it is important for our purposes. Let’s focus on what is interesting and useful. 1. Summary Output Table: Multiple R: This is the correlation coefficient between your observed and predicted values. It ranges from -1 to 1. A value closer to 1 or -1 indicates a strong relationship, whereas a value close to 0 means a weak relationship. In our case, a value of 0.65 indicates a reasonably strong correlation between growth and sunlight. R Square (R²): This tells us how much of the variance in the dependent variable is explained by the independent variable(s). In our case, an R² of 0.42 means that 42% of the variation in the dependent variable is explained by the model. Conversely, it means that approx 58%, i.e. the majority, of the variance is explained by other unknown variables. In this context, these unknown variables could include things like daily average temperature, rainfall, humidity etc. Adjusted R Square: This adjusts the R² value for the number of predictors in your model. It’s more useful when dealing with multiple independent variables as it accounts for any unnecessary complexity added by too many variables. Standard Error: This measures the average distance that the observed values fall from the regression line. A smaller standard error means a better fit. Note that the units of your standard error are the same as for your dependent variable, i.e. mm. 2. ANOVA Table The ANOVA table tells us whether the overall regression model is significant. Degrees of Freedom (df): This refers to the number of independent pieces of information that went into calculating the estimates. It’s a balance between the number of observations and the number of predictors in the model. SS Regression (Sum of Squares due to Regression): This represents the variance in the dependent variable explained by the model. SS Residual (Sum of Squares of Residuals): This represents the variance that the model doesn’t explain. Significance F: This is the p-value for the overall regression model. In our case, A p-value &lt; 0.05 means that the model is statistically significant, and we reject the null hypothesis that there is no relationship between the dependent and independent variables. 3. Coefficient Table This table is where you’ll find the key parameters for your linear regression model. Intercept (β₀): This is the expected value of the dependent variable when the independent variable is zero. In other words, it’s your y-intercept value. Gradient (β₁): This is the slope of your line, showing how much the dependent variable changes with each unit increase in the independent variable. P-values: These tell us whether the coefficients (intercept and gradient) are statistically significant. If the p-value for the gradient is less than 0.05, we can say that the independent variable has a significant impact on the dependent variable. 4. Residual Plots and Additional Tables Residual Plot: This plot shows the differences between the observed and predicted values (residuals). Ideally, these residuals should be randomly scattered around 0, indicating a good model fit. If this is not the case then it might not be appropriate for you to be performing a linear regression. Normal Probability Plot: This checks if the residuals follow a normal distribution. If the points follow a straight line, the residuals are normally distributed. Again, if this is not the case then you should seek an alternative analysis. Residual Output: This provides detailed information about each residual (the error for each observation). Probability Output: Provides the probabilities of observing these residuals given the model fit. 3.2.4 Making Predictions with Our Model With the insights from our linear regression analysis, we can now use our model to make predictions using the equation \\[ \\hat{Y} = \\beta_0 + \\beta_1 \\hat{X} \\] where: - \\(\\hat{Y}\\) is the predicted value of the dependent variable. - \\(\\beta_0\\) is the intercept, representing the expected value of \\(Y\\) when \\(X\\) is zero. - \\(\\beta_1\\) is the gradient (or slope), indicating how much \\(Y\\) changes with a one-unit change in \\(X\\). - \\(\\hat{X}\\) is the value of the independent variable for which we want to make a prediction. For example, to predict the daily growth of our sunflowers when they’ve received six hours of sunlight: \\[ \\hat{Y} = 2.449 + ( 0.898 \\times 6 )= 7.836 \\] So, the predicted value of \\(\\hat{Y}\\) for \\(\\hat{X} = 6\\) is \\(7.836\\) \\(mm\\). But wait! Couldn’t I have achieved this by simply adding a trend line to a plot of \\(X\\) vs. \\(Y\\), as discussed in section @ref{section:calibration-curves}? Yes, you could have extracted the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) from the equation of the line, and even added the \\(R^2\\) value to the plot. So why go through all this additional work? The full regression analysis allows us to determine the standard error (\\(SE\\)) of the model, which enables us to calculate the uncertainty associated with a predicted value \\(\\hat{Y}\\). This is something we couldn’t do with a simple trend line! This uncertainty, often referred to as the margin of error (\\(ME\\)), provides the range within which future individual observations are expected to fall. To calculate \\(ME\\): \\[ ME = t \\times SE \\] where \\(t\\) is the t-statistic which, in this context, can be drawn from a Student’s t-distribution and accounts for the uncertainty in estimating the population parameters from a sample. In excel you can calculate \\(t\\) using: =T.INV.2T(alpha, df) alpha is the significance level, in this case 0.05. df is the degrees of freedom which can be calculated as \\(df = n - k - 1\\) where \\(n\\) is the number of data points (observables) and \\(k\\) is the number of independent variables. In our case, \\(n=24\\) and \\(k=1\\) so \\(df=24-1-1=22\\). 3.2.5 Extending to Multiple Variables Right now, we’re focusing on simple linear regression (one dependent and one independent variable). However, what if we wanted to examine the effect of multiple independent variables (e.g., sunlight and temperature on sunflower growth)? This is called multiple regression and we’d consider the following linear regression equation instead: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon \\] Where: \\(X_1, X_2, ..., X_n\\): The independent variables (e.g., sunlight, temperature, etc.). \\(\\beta_0\\): As before, this is the intercept, representing the expected value of \\(Y\\) when all independent variables are 0. \\(\\beta_1, \\beta_2, ..., \\beta_n\\): The coefficients of the independent variables, representing the change in \\(Y\\) for a one-unit change in \\(X\\), assuming all other variables remain constant. Multiple regression allows us to assess the combined effect of several factors, which is crucial when studying complex systems where multiple variables might influence the outcome. Unfortunately, Excel has its limits for this. But don’t worry—we’ll tackle multiple regression using R in Week 5! 3.3 Complete your Weekly Assignments {chapter-3-assignments} In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["introduction-to-r-part-i.html", "4 Introduction to R: Part I 4.1 Reading and Inspecting Data 4.2 Subsetting 4.3 Summarising Data in R 4.4 Complete your Weekly Assignments", " 4 Introduction to R: Part I If you are currently participating in a timetabled BIOS103 QS workshop, please ensure that you cover all of this section’s content and complete this week’s formative and summative assessments in the BIOS103 Canvas Course. We are going to diverge from what you have been doing in the labs this week in order to focus on introducing you to R. Our learning objectives are as follows: Start a new R Project in R Studio Read a dataset to a variable Inspect a dataset Subset a dataset by slicing and filtering Create summary tables of descriptive statistics Sort a table You should have completed section 1.2 already and have R and R Studio installed on your personal machine. If you are unable to install R and RStudio then please use one of the University computers. OMG. Why, are you making me learn to code? In my experience, at least half of you will love learning to code. The rest of you will hate my guts. I’m OK with that. That’s because I truly believe that in the near future you will see that even a basic understanding of coding gives you a huge advantage in your studies and future careers. However, if you really need me to give you explicit reasons as to why I’m making you learn to code then here are six: 1. Data Literacy All the biosciences rely heavily on data analysis. Learning to code will equip you the skills to efficiently manipulate and interpret complex datasets. R, in particular, is well-suited for handling bioscientific data, from genomic sequences to clinical trials and environmental modelling. 2. Reproducibility Coding promotes transparency and reproducibility in scientific analyses. Unlike manual methods, where calculations or procedures may be difficult to replicate, scripts provide a clear, step-by-step record that can be easily shared and re-run. This is vital for ensuring the integrity of scientific research. 3. Automation Bioscientists often work with large datasets, and coding enables the automation of repetitive tasks such as data cleaning, statistical analyses, and reporting. This not only saves time but also minimises human error, allowing students and researchers to focus on interpreting their results. 4. Career Readiness Coding has become an indispensable skill in the biosciences. Whether working in research, biotechnology, or environmental consultancy, understanding how to work with data in R (or similar tools) will give you a competitive edge when applying for jobs or pursuing further academic research. 5. Critical Thinking Writing code fosters critical thinking and problem-solving skills. While learning to code you will have to break down complex problems, troubleshoot errors, and develop logical workflows. These skills are not only useful for coding but are also vital for scientific thinking and research. 6. Integration with Lab Skills Coding complements traditional lab skills. In the era of bioinformatics and systems biology, being able to analyse experimental data computationally can provide additional insights that are often not apparent from lab experiments alone. Have you accepted your fate then? Good. Now lets get something else straight: I am NOT going to teach you to code. You are going to do that yourself, over many months (and probably years). This book represents a mere “dipping of the toes” into the world of R coding and is by no means comprehensive. In fact, it is completely and utterly incomplete. You will need to fill in many of the blanks yourself as you encounter them. How you go about this is up to you, but checkout the Appendix for good places to start. 4.1 Reading and Inspecting Data Let’s dive in then! This week we’re going to be introducing you to some fundamental data handling concepts in R using the fantastic Pantheria dataset of extant and recently extinct mammals. Open RStudio and start a new project Download the data from here (or right click, save as from here). Copy the downloaded file into your RStudio project directory (folder). You should see the file appear in your Files window in the bottom-right corner of RStudio. Create a new R script file and save it with a sensible filename (e.g. pantheria_summary.R) Read the data to a variable called pantheria_data: In your new script file, write the following line of code on line 1: pantheria_data &lt;- read.csv(&quot;pantheria_999.csv&quot;) Run the line by clicking anywhere on the line and then clicking the run button (in the top-right corner of your script window) or by pressing Ctrl+Enter on your keyboard. You should see a new line appear in your Environment window in the top-right of your RStudio like in figure 4.1. If you don’t see it then the here are the most common reasons as to why: You haven’t used quotation marks around your data filename You’ve mis-spelled your filename You haven’t put your data file in the correct folder You didn’t create an R project first. Figure 4.1: Details of your data variable should appear in your environment window after running line 1. Figure 4.2: By clicking on your data variable in your environment window you can inspect the data in RStudio What is a variable in R? A variable in R is a symbolic name representing a value stored in memory. Variables are created by assigning values using &lt;-. Naming Convention I prefer using lowercase words for variable names. If multiple words are needed, I separate them with an underscore “_“, avoiding spaces. For example: my_variable. Best Practices Descriptive Names: Choose clear, meaningful names. Consistency: Use the same naming style throughout your code. Avoid Reserved Words: Don’t use R’s reserved keywords (if, else, for, etc.) Uniqueness: Ensure variable names are unique within your environment. These practices enhance code readability and maintainability. Well done if you can see your data variable! You can click on your variable (in the environment window) and inspect your data directly. You should see something like that shown in figure 4.2. Wow! Look at all that data. Yummy. Now the world is your oyster, as they say. From here you can go in any number of directions and use R to summarise, visualise and analyse your data in order to gain novel insights. Let’s start small by asking the very simple question: “What order of mammal occurs the most frequently (has the most number of rows) in the dataset?”. To answer this question, I need to go through my data row by row and count how many times a row corresponding to each order (e.g. Dermoptera, Chiroptera, Rodentia etc …) appears. Let’s extend my R script as follows: pantheria_data &lt;- read.csv(&quot;pantheria_999.csv&quot;) orders &lt;- table(data$Order) print(orders) Let’s break these additional lines down, step-by-step: Firstly, you’ll notice that I’ve left lines 2 and 5 blank. You don’t need to do this, but I like to because it gives my code a bit of breathing space and is often easier to identify a problem later. In line 3, I’m using the table() function to count the number of times a unique value in my order column appears. By putting data$Order inside the table function’s brackets we are “passing” it all of the values in the Order column as a big list. I’m then assigning the output of the table function to a new variable called orders so that I can refer to it later. Lastly in line 5, I’m using a print function to output the contents of the variable orders to my console as shown in figure 4.3 Figure 4.3: When using print(orders) the following should appear in my RStudio console. What is a function in R? A function in R is a set of instructions that performs a specific task or calculation, taking inputs (arguments) and returning an output (result). So far in this course, we have used three pre-defined functions: read.csv() to read data from a CSV file, table() to summarise categorical data, and print() to display output in the console. While these functions are built into R, we can also create our own functions when we need to perform customised tasks—more on this later in the course. You can see by inspecting your console output that the order Rodentia is the most frequently occurring order with 549 rows in the data. But what if there were lots more orders? It might not be so straightforward. Let’s get our R script to pick out the most frequently occurring order for us: pantheria_data &lt;- read.csv(&quot;pantheria_999.csv&quot;) orders &lt;- table(data$Order) most_common_order &lt;- names(which.max(orders)) print(most_common_order) I’ve inserted the line most_common_order &lt;- names(which.max(orders)) at line 5. Let’s break it down: which.max(orders): Function: which.max() is a built-in R function that returns the index of the first maximum value in its input. Input: In this case, the input is orders, which is the table we created earlier containing the counts of each order in the dataset. Output: The function identifies the index position of the maximum count (i.e., the highest frequency) in the orders table. In our case “Rodentia” is at position 24 in the table. names(...): Function: The names() function retrieves the names (or labels) associated with the elements of an object. For example if I used names(orders) the function would simply return a list of all the unique orders. Usage: By wrapping which.max(orders) inside names(), we are saying, “give me the name of the order that corresponds to the maximum frequency index returned by which.max().” Output: In our case the name “Rodentia” is returned. most_common_order: I’ve assigned the result of my names(which.max(orders)) to a new variable using &lt;-. Finally, I’ve updated my print statement to `print(most_common_order)’. This should now print out “Rodentia” in my console when I re-run my script. Phew! That took a bit of explaining. Make sure you understand what just happened. The wrapping of a function in a function is a common practice in R and things can quickly become obfuscated. It’s good practice to comment your code so that you (or someone else) can quickly make sense of what is going on. Here is my final code, with comments! # Read the CSV file and store its contents in &#39;data&#39;. data &lt;- read.csv(&quot;pantheria_999.csv&quot;) # Create a frequency table of the &#39;Order&#39; column. orders &lt;- table(data$Order) # Get the name of the most common order. most_common_order &lt;- names(which.max(orders)) # Print the most common order to the console. print(most_common_order) 4.2 Subsetting In data analysis, subsetting refers to the process of extracting a portion of a dataset based on specific criteria. This allows you to focus on the most relevant information, making analysis more efficient and tailored to your needs. There are two key methods for subsetting data in R: Slicing and Filtering. 4.2.1 Slicing Slicing is a crucial operation in data analysis, enabling you to extract specific rows, columns or both from an existing data frame. This technique helps streamline workflows, especially when working with large datasets, by narrowing down data to focus on relevant sections. Slicing Example Let’s add a new line to our earlier script file: slice &lt;- data[1:1000, 1:10] This code creates a new variable called slice and assigns to it the first 1,000 rows and the first 10 columns of the dataset data. Run the line and you should see a new dataframe called slice appear in your Environment window (top-right). What if I’d wanted to select ALL the rows but still slice off the first 10 columns? Well, that would look like this: slice &lt;- data[, 1:10] And what if I don’t want to use index values to select the columns (i.e., 1:10) and I want to select them by name? For example let’s say I’ve inspected the column headers and I specifically want to select the following columns: Order AdultBodyMass_g BasalMetRate_mLO2hr I can do that using this code: slice &lt;- data[, c(&quot;Order&quot;, &quot;AdultBodyMass_g&quot;, &quot;BasalMetRate_mLO2hr&quot;)] Here I’ve defined a vector using the c() function containing the verbatim names of columns I want and used this instead of specifying a range of column indices. 4.2.2 Filtering Unlike slicing, which selects a subset of rows and columns based on their position, filtering involves evaluating each row against a set of logical conditions and including only those that meet the criteria. Filtering Example In the above slicing example we created a dataframe variable called slice which contains three columns. If you click on the variable in the environment window to select it you’ll see something like that shown in 4.4. Figure 4.4: You’ve sliced your data but you’ll need to be able to filter in order to remove the -999 values. Hmmm, that’s weird. What’s with all the -999 values? In this dataset the value -999 has been used to indicate where a value wasn’t measured. I can remove these -999 values in my slice dataframe like this: filtered &lt;- slice[slice$AdultBodyMass_g &gt;=0 &amp; slice$BasalMetRate_mlO2hr &gt;=0, ] Let’s break this down: filtered &lt;-: This assigns the result of the filtering operation to a new variable called filtered. This variable will contain only the rows from the slice data frame that meet certain conditions. slice[slice$AdultBodyMass_g &gt;= 0 &amp; slice$BasalMetRate_mlO2hr &gt;= 0, ]: This part performs the actual filtering operation on the slice data frame. slice$AdultBodyMass_g: This accesses the AdultBodyMass_g column of the slice data frame. The $ operator is used to refer to a specific column. slice$BasalMetRate_mlO2hr: Similarly, this accesses the BasalMetRate_mlO2hr column of the slice data frame. slice$AdultBodyMass_g &gt;= 0: This condition checks each value in the AdultBodyMass_g column to see if it is greater than or equal to 0, generating a logical vector (TRUE or FALSE) for each row. slice$BasalMetRate_mlO2hr &gt;= 0: This condition checks each value in the BasalMetRate_mlO2hr column to see if it is greater than or equal to 0, producing another logical vector. &amp;: This is the logical AND operator, which combines the two logical vectors created by the previous conditions. The result is a new logical vector that is TRUE only for rows where both conditions are TRUE. slice[… , ]: The entire expression inside the square brackets is used to subset the slice data frame. The rows for which the combined condition is TRUE are selected, and all columns (indicated by the empty space after the comma) are returned. In summary, the code creates a new data frame, filtered, which contains only the rows from the slice data frame where both the AdultBodyMass_g and BasalMetRate_mlO2hr values are greater than or equal to 0. 4.2.3 So what? Well, being as you’ve gone to all that trouble to slice and filter your data we should probably do something spectacular with it! Try adding the following code to your script to and running it. plot(slice$AdultBodyMass_g, slice$BasalMetRate_mLO2hr, log=&quot;xy&quot;, xlab = &quot;Adult Body Mass (g)&quot;, ylab = &quot;Basal Metabolic Rate (mLO2/hr)&quot;, main = &quot;&quot;) You should see something that looks like what is shown in figure 4.5. Figure 4.5: An all species log-log scatterplot of mammal adult body mass (g) vs basal metabolic rate (mLO\\(_2\\)/hr) Notice that I’ve used the argument log=\"xy\" within my plot() function to make both the x and y axes scale logarithmically. This means that instead of the axes increasing in equal increments (e.g., 1, 2, 3), they increase by factors of ten (e.g., 10, 100, 1000). This is particularly useful when the relationship between two variables spans several orders of magnitude, as is often the case in biological data. When both axes are scaled logarithmically, relationships that might look curved on a linear scale can appear as straight lines, making it easier to identify patterns. In this case, the plot reveals a linear relationship between the two variables (e.g., adult body mass and basal metabolic rate) when viewed on a log-log scale. This linearity indicates that as one variable increases by a certain percentage, the other variable increases by a consistent percentage, rather than by a fixed amount. This relationship is described as allometric scaling and is remarkably common in nature. What’s fascinating is that this log-log linearity holds across all mammals! From the smallest mouse to the largest whale, body mass and metabolic rate follow a consistent scaling relationship when plotted logarithmically. This insight allows scientists to understand fundamental principles about how biological processes like metabolism are related to size in the animal kingdom. 4.3 Summarising Data in R Remember how in chapter 2 we generated a summary table or descriptive statistics using Excel? We can do it in R too! As an example let’s answer the following question: What are the top 10 families with the highest number of neonate body mass observations, along with their summary statistics (mean, median, max, min, and standard deviation)? The following script will do the job, as you can see by the accompanying output. # Read the CSV and filter out invalid rows data &lt;- read.csv(&quot;pantheria_999.csv&quot;) data_filtered &lt;- data[data$NeonateBodyMass_g != -999, ] # Calculate summary statistics for each Family summary_table &lt;- aggregate(NeonateBodyMass_g ~ Family, data_filtered, function(x) c(Mean = mean(x), Median = median(x), Max = max(x), Min = min(x), SD = sd(x), Count = length(x))) # Convert the list columns to individual columns summary_table &lt;- do.call(data.frame, summary_table) # Rename the columns colnames(summary_table) &lt;- c(&quot;Family&quot;, &quot;Mean&quot;, &quot;Median&quot;, &quot;Max&quot;, &quot;Min&quot;, &quot;Std. Dev.&quot;, &quot;Count&quot;) # Format all numeric values to 1 decimal place (except Count) summary_table[, 2:6] &lt;- round(summary_table[, 2:6], 1) # Sort by Count in decreasing order and return top 10 most frequently occurring families in data summary_table &lt;- summary_table[order(-summary_table$Count), ][1:10, ] # Write to CSV and print write.csv(summary_table, &quot;summary_table.csv&quot;, row.names = FALSE) print(summary_table) ## Family Mean Median Max Min Std. Dev. Count ## 9 Bovidae 8291.2 4819.6 39843.1 500.0 8521.8 36 ## 17 Cercopithecidae 465.9 450.0 890.0 240.7 156.7 19 ## 69 Mustelidae 149.7 30.0 1894.4 2.0 429.0 19 ## 18 Cervidae 4252.0 3082.2 13500.0 400.0 4027.9 18 ## 13 Canidae 167.6 102.1 412.3 28.0 127.6 14 ## 47 Heteromyidae 3.1 3.0 7.7 1.0 1.9 14 ## 80 Phocidae 16490.5 15122.7 39393.0 3050.0 10434.1 14 ## 93 Sciuridae 10.3 6.1 33.0 3.3 10.0 13 ## 41 Felidae 186.4 161.6 409.9 72.0 102.4 12 ## 56 Leporidae 74.4 90.0 123.0 25.9 39.0 11 See if you can reproduce the output by creating a new script file in your project, copying and pasting the code above and clicking the Source button in the top right of your script editor window. What Next? I don’t need you to understand every line in the script. In fact there are some lines (6 -7 in particular) that are doing some seriously funky stuff that took me years to learn and understand fully. Nonetheless, ask yourselves what would you need to tweak to get the code to generate a summary table for another variable (e.g. AdultBodyMass_g) or group the results by Order instead of Family. Often, when learning to code, it can feel overwhelming trying to understand every single detail of a script before running it. However, there’s value in a “run it and see what happens” approach, especially when you’re starting out. The beauty of coding is that you don’t need to fully grasp every element in order to get results. In fact, many experienced coders begin with scripts they might not completely understand. The key is knowing just enough to use the script to answer your question. Once you see the output and observe how the code works, you can begin to tweak and modify it to suit a different dataset or to refine your results. The process of experimenting with the code helps deepen your understanding over time. You’ll gradually learn how each part of the script contributes to the output, and that’s where real learning happens. Turbo charge your learning with Chat-GPT I have serious mis-givings about asking generative AI to generate code out of thin air. In my experience, GAI likes to show off and often over-complicate things. This can seriously confuse students who are just starting to learn to code. However, using it to understand existing code? Yes! Do it! What a fantastic way to learn. I use it every day when it comes to code. I cut and paste the script above into Chat-GPT and give it the following prompt: “Explain in detail”. You can see the result here. Isn’t that amazing? Feedback Please. I really value your feedback on these materials for quantitative skills. Please rate them below and then leave some feedback. It’s completely anonymous and will help me improve things if necessary. Say what you like, I have a thick skin - but feel free to leave positive comments as well as negative ones. Thank you. 4.4 Complete your Weekly Assignments In the BIOS103 Canvas course you will find this week’s formative and summative QS assignments. You should aim to complete both of these before the end of the online workshop that corresponds to this section’s content. The assignments are identical in all but the following details: You can attempt the formative assignment as many times as you like. It will not contribute to your overall score for this course. You will receive immediate feedback after submitting formative assignments. Make sure you practice this assignment until you’re confident that you can get the correct answer on your own. You can attempt the summative assignment only once. It will be identical to the formative assignment but will use different values and datasets. This assignment will contribute to your overall score for this course. Failure to complete a summative test before the stated deadline will result in a zero score. You will not receive immediate feedback after submitting summative assignments. Typically, your scores will be posted within 7 days. In ALL cases, when you click the button to “begin” a test you will have two hours to complete and submit the questions. If the test times out it will automatically submit. "],["appendix.html", "Appendix: Teach yourself R. LinkedIn Learning YouTube Books", " Appendix: Teach yourself R. LinkedIn Learning Can you believe it!? As a student at the University of Liverpool you have free and full access to the LinkedIn Learning platform. Login using your UoL credentials. It’s completely choc-o-bloc with coding resources and you often get a neat little certificate every time you complete a course. Complete guide to R: Wrangling, Visualizing, and Modeling Data. Barton Poulson Data Wrangling in R. Mike Chapple R for Data Science: Analysis and Visualization. Barton Poulson Coding Exercises: R Data Science. Mark Niemann-Ross Complete Your First Project in R. Megan Silvey R for Data Science: Lunch Break Lessons. Mark Neimann-Ross YouTube Ah, YouTube. How I love you. But not shorts. Shorts are evil and rot your brain. Here are some great YouTube playlists that will introduce you to R. Introduction to R. DataDaft Statistics and Statistics with R Tutorials. MarinStatsLectures R tutorial - Learn R Programming. DataCamp Introduction to R Programming. Data Science Dojo Books Do people read books anymore? Yes they do. Here are 5 titles that are available to you in the University of Liverpool library right now. Hands-on programming with R. Garret Grolemund R in action: data analysis and graphics with R and Tidyverse. Robert Kabacoff R for Data Science. Christopher Lortie Introduction to Statistics Using R. Mustapha Akinkunmi An Introduction to Data Analysis in R: Hands-on Coding, Data Mining and Statistics from Scratch. Zamora Alfonso et. al "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
